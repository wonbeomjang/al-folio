---
layout: post
title: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
date: 2023-03-29 00:00:00 +0900
description: optimize transformer on gpu device
categories: [transformer, hardware-optimization, paper]
giscus_comments: true
related_posts: true
---

# Introduction

í˜„ì¬ NLPì™€ Vision ë¶„ì•¼ì—ì„œ transformerëŠ” í™œë°œíˆ ì‚¬ìš©ë˜ê³  ìˆë‹¤.  
í•˜ì§€ë§Œ transformerëŠ” ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì¡ì•„ë¨¹ëŠ” ëª¨ë“ˆì´ì—ˆê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ sparse-approximation, low-rank approximation ë“±ì„ ì œì•ˆí–ˆë‹¤.  
í•˜ì§€ë§Œ ì´ë“¤ì€ ì´ë¡ ê³¼ ë‹¬ë¦¬ computational speedë¥¼ ì¦ê°€ì‹œì¼œì£¼ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ë‹¤.  
ì €ìëŠ” GPUì—ì„œ ë¹ ë¥¸ SRAMìœ¼ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” IO-aware ì•Œê³ ë¦¬ì¦˜ì„ ì œì‹œí–ˆë‹¤.

## Hardware performance

### GPU Memory Hierarchy

<p align="center">
    <img src="/assets/post/image/legacy/fastattention-gpu-hierchy.png" width="50%">
</p>

GPUëŠ” CPUì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë©”ëª¨ë¦¬ ê³„ì¸µì„ ê°€ì§„ë‹¤. DRAMì´ ê°€ì¥ ëŠë¦¬ê³  ìš©ëŸ‰ì´ í¬ë©°, SRAMì´ ê°€ì¥ ë¹ ë¥´ê³  ìš©ëŸ‰ì´ ì‘ë‹¤.  
GPUëŠ” ë³‘ë ¬ ì—°ì‚° ì‹œ ë°ì´í„°ë¥¼ HBMì—ì„œ ê°€ì ¸ì˜¨ í›„ SRAMì— ì˜¬ë ¤ë†“ê³  ì—°ì‚°ì„ í•œë‹¤. ì´í›„ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ì½ì–´ë“¤ì´ë©´ SRAMì— ìˆëŠ” ì •ë³´ëŠ” ë‹¤ì‹œ HBMì— ì €ì¥ëœë‹¤.

### Performance characteristics

í¼í¬ë¨¼ìŠ¤ë¥¼ ê³ ë ¤í•  ë•Œ ì—°ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì ‘ê·¼ì˜ ê´€ì ìœ¼ë¡œ ë‘ ê°€ì§€ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

1. Compute-bound: ì—°ì‚°ëŸ‰ì´ ë©”ëª¨ë¦¬ ì ‘ê·¼ë³´ë‹¤ ë§ì€ ê²½ìš°ì´ë‹¤. ex) MatMul
2. Memory-bound: ë©”ëª¨ë¦¬ ì ‘ê·¼ì´ ì—°ì‚°ëŸ‰ë³´ë‹¤ ë§ì€ ê²½ìš°ì´ë‹¤. ex) softmax, batchnorm

### Kernel fusion

Memory-bound ì—°ì‚°ì„ ê°€ì†í•˜ëŠ” ë° ë§ì´ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì€ kernel fusionì´ë‹¤.  
ë§Œì•½ ê°™ì€ inputì— ëŒ€í•´ ì—¬ëŸ¬ ì—°ì‚°ì„ í•œë‹¤ê³  í•˜ë©´, ì»´íŒŒì¼ëŸ¬ëŠ” ìë™ìœ¼ë¡œ ë§ì€ elementwise operationì„ fusioní•œë‹¤.

## Standard Attention Implementation

Sequence length $$N$$ê³¼ head dimension $$d$$ì— ëŒ€í•˜ì—¬ attentionì€ input sequence $$Q,K,V \in \mathbb{R}^{N \times d}$$ë¥¼ ì´ìš©í•˜ì—¬  
$$O \in \mathbb{R}^{M \times d}$$ë¥¼ êµ¬í•œë‹¤. ê·¸ì— ëŒ€í•œ ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
S=QK^\top \in \mathbb{R}^{N \times N}, P=softmax(S) \in \mathbb{R}^{N \times N}, O = PV \in \mathbb{R}^{N \times d}
$$

ì´ë•Œ softmaxëŠ” row-wise operationì´ë‹¤. ë³´í†µì˜ attentionì€ $$O(N^2)$$ì˜ memory costë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ëŒ€ë‹¤ìˆ˜ì˜ ê²½ìš°ì—ëŠ” $$N \gg d$$ë¥¼ ë§Œì¡±í•œë‹¤(GPT-2, N=1024 and d=64).

<p align="center">
    <img src="/assets/post/image/legacy/standard-attention-algorithm.png" width="80%">
</p>

# FlashAttention

FlashAttentionì€ **Tiling**ê³¼ **Recomputation**ì„ ì‚¬ìš©í•˜ì—¬ Attentionì„ ê°€ì†í™”í•œë‹¤.

### Tiling

<p align="center">
    <img src="/assets/post/image/legacy/fastattention-tiling.png" width="50%">
</p>

ê¸°ì¡´ì˜ softmax ì—°ì‚°ì€ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ê±°ì¹œë‹¤.

$$
m(x):=\underset{i}{max}(x_i), f(x):=[e^{x_1-m(x)} ... e^{x_B-m(x)}],
$$

$$
l(x):=\sum_i f(x)_i, softmax(x):= \frac{f(x)}{l(x)}
$$

vector $$x^{(1)}, x^{(2)} \in \mathbb{R}^B$$ì¼ ë•Œ vectorì˜ concatenation $$x=[x^{(1)} x^{(2)}]$$ì— ëŒ€í•´ softmaxëŠ” ë‹¤ìŒê³¼ ê°™ì´ decompositioní•  ìˆ˜ ìˆë‹¤.

$$
m(x)=m([x^{(1)} x{(2)}])=max(m(x^{(1)})),m(x^{(2)}),
$$

$$
f(x):=[e^{m(x^{(1)})-m(x)}f(x^{(1)}) ... e^{x^{(2)}-m(x)}f(x^{(2)})],
$$

$$
l(x)=l([x^{(1)} x{(2)}])=e^{m(x^{(1)})-m(x)}l(x^{(1)}) + e^{x^{(2)}-m(x)}l(x^{(2)}),
$$

$$
softmax(x):= \frac{f(x)}{l(x)}
$$

ì¦‰, softmaxë¥¼ block ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

### Recomputation

ì €ìëŠ” backward ë•Œ $$O(N^2)$$ì˜ memoryë¥¼ ì €ì¥í•˜ì§€ ì•Šê¸° ìœ„í•´ softmax normalization statistics $$(m,l)$$ì„ ì €ì¥í•œ í›„ backward ë•Œ ë‹¤ì‹œ êµ¬ì„±í•œë‹¤.  
ì´ë¡œ ì¸í•´ FLOPsëŠ” ì¦ê°€í•˜ì§€ë§Œ HBMì—ì„œ ë°ì´í„°ë¥¼ ì½ëŠ” íšŸìˆ˜ê°€ ì¤„ì–´ë“¤ì–´ ì†ë„ê°€ í–¥ìƒëœë‹¤.

### Kernel Fusion

<p align="center">
    <img src="/assets/post/image/legacy/fastattention-kernel-fusion.png" width="50%">
</p>

Tilingì„ í†µí•´ í•œ ë²ˆì˜ HBM loadì—ì„œ matrix multiply, softmax, optionally masking and dropout, matrix multiplyë¥¼ í•œ í›„ HBMì— ì €ì¥í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.  
ì´ëŠ” ë°˜ë³µì ì¸ IO operationì„ ì¤„ì—¬ì¤€ë‹¤.

<p align="center">
    <img src="/assets/post/image/legacy/fastattention-algorithm.png" width="80%">
</p>

> **Theorem 1**. Algorithm 1 returns $$O=softmax(QL^\top)V$$ with $$O(N^2d)$$ FLOPs and requires additional memory beyond inputs and output.

## Analysis: IO Complexity of FlashAttention

<p align="center">
    <img src="/assets/post/image/legacy/fastattention-coomplexity.png" width="80%">
</p>

FlashAttentionì€ standardë³´ë‹¤ GFLOPsëŠ” ë§ì§€ë§Œ, HBM read and writeê°€ ì ì–´ runtimeì´ ê°œì„ ë˜ì—ˆë‹¤.

> **Theorem 2.** Let $$N$$ be the sequence length, $$d$$ be the head dimension, and $$M$$ be the size of SRAM with $$d \leq M \leq Nd$$. Standard attention (Algorithm 0) requires $$\Theta(Nd+N^2)$$ HBM accesses, while FlashAttention (Algorithm 1) requires $$\Theta(N^2d^2M^{-1})$$ HBM accesses.

> **Proposition 3.** Let $$N$$ be the sequence length, $$d$$ be the head dimension, and $$M$$ be the size of SRAM with $$d \leq M \leq Nd$$. There does not exist an algorithm to compute exact attention with $$\Theta(N^2d^2M^{-1})$$ HBM accesses for all $$M$$ in the range $$[d, Nd]$$.

# Extension

Block-sparse attentionì„ ì‘ìš©í•˜ì—¬ block-sparse flashattentionì„ ë§Œë“¤ê¸°ë„ í–ˆë‹¤.

$$
S=QK^\top \in \mathbb{R}^{N \times N}, P=softmax(S \odot \mathbb{1}_{\tilde{M}}) \in \mathbb{R}^{N \times N}, O=PV \in \mathbb{R}^{N \times d}
$$

> **Proposition 4.** Let $$N$$ be the sequence length, $$d$$ be the head dimension, and $$M$$ be the size of SRAM with $$d \leq M \leq Nd$$. Block-sparse FlashAttention (Algorithm 5) requires $$\Theta(N^2d^2M^{-1})$$ HBM accesses where ğ‘  is the fraction of nonzero blocks in the block-sparsity mask.

## Experiment

FlashAttentionì€ tilingì„ í†µí•´ ì†ë„ê°€ ë¹ ë¥´ê³ , recomputationì„ í†µí•´ ë©”ëª¨ë¦¬ê°€ ì¤„ì–´ë“¤ì—ˆë‹¤.  
ì´ë¥¼ ì´ìš©í•˜ì—¬ sequence lengthë¥¼ ëŠ˜ë¦´ ìˆ˜ ìˆì—ˆê³ , ì´ëŠ” ì¶”ê°€ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì™”ë‹¤.

### BERT

<p align="center">
    <img src="/assets/post/image/legacy/fastattention-bert-performance.png" width="80%">
</p>

BERT í•™ìŠµ ì‹œ MLPerf 1.1 ê¸°ì¤€ í•™ìŠµ ì‹œê°„ì´ 15% ê°œì„ ë˜ì—ˆë‹¤.

### GPT-2

GPT-2ëŠ” Huggingface, Megatron-LMê³¼ ë¹„êµí–ˆëŠ”ë° ê°ê° 3ë°°, 1.7ë°°ì˜ speed upì´ ë°œìƒí–ˆë‹¤.

<p align="center">
    <img src="/assets/post/image/legacy/fastattention-gpt-2-performace.png" width="80%">
</p>

### Long-range Arena

LRAì—ì„œë„ ê¸°ì¡´ ëŒ€ë¹„ 2.4ë°°ì˜ speed upì„ ë³´ì˜€ìœ¼ë©°, ë‹¤ë¥¸ attention methodë³´ë‹¤ ì„±ëŠ¥ë„ ì¢‹ì•˜ë‹¤.

<p align="center">
    <img src="/assets/post/image/legacy/fastattention-long-reange-arena-performace.png" width="80%">
</p>

## Better Models with Longer Sequences

### Language Modeling with Long Context.

Recomputingìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì¤„ì–´ë“¤ë©´ì„œ ë” ê¸´ input sequenceë¥¼ ë‹¤ë£° ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. ì´ë¥¼ í†µí•´ ì¶”ê°€ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì™”ë‹¤.

<p align="center">
    <img src="/assets/post/image/legacy/fastattention-bert-with-long-sequence.png" width="80%">
</p>

### Long Document Classification

<p align="center">
    <img src="/assets/post/image/legacy/fastattention-long-document-classification.png" width="80%">
</p>

### Path-X and Path-256

<p align="center">
    <img src="/assets/post/image/legacy/fastattention-path-x.png" width="80%">
</p>

Path-Xì™€ Path-256ì€ long contextë¡œ ê¸°ì¡´ì˜ ëª¨ë¸ë“¤ì€ randomí•œ ê²°ê³¼ì™€ ë¹„ìŠ·í•˜ê²Œ ë‚˜ì™”ë‹¤.  
FlashAttentionì€ í•´ë‹¹ ë°ì´í„°ì…‹ì— random ì´ìƒì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¨ ì²« ë²ˆì§¸ ëª¨ë¸ì´ë‹¤.

## Benchmarking Attention

<p align="center">
    <img src="/assets/post/image/legacy/fastattention-banchmarking.png" width="80%">
</p>

Attention ê³„ì—´(Attention, FlashAttention)ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ $$O(N^2)$$ì´ì§€ë§Œ, approximate attention(sparse attention)ì€ $$O(n)$$ì´ë‹¤.  
ë”°ë¼ì„œ sequence lengthë¥¼ í‚¤ìš°ë‹¤ ë³´ë©´ approximate attentionì´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ, ì„±ëŠ¥ì—ì„œëŠ” ìš°ìˆ˜í•˜ë‹¤.

# Limitation

FlashAttentionì€ CUDA kernelì„ ì‚¬ìš©í•´ì•¼ í•˜ë¯€ë¡œ ì—”ì§€ë‹ˆì–´ë§ì´ í•„ìš”í•˜ë‹¤.  
ê·¸ë¦¬ê³  GPUë§ˆë‹¤ ì»´íŒŒì¼ì´ í•„ìš”í•˜ë©° í™•ì¥ì„±ì— ë¬¸ì œê°€ ìˆë‹¤.  
ë˜í•œ í˜„ì¬ëŠ” single GPUë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§Œë“¤ì–´ì¡Œìœ¼ë¯€ë¡œ, multi-GPUë¥¼ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ë„ ì œì‘í•´ì•¼ í•œë‹¤.

---

**ì¶”ê°€ê¸€...**  
ì•„ì•—... í¬ìŠ¤íŠ¸ë¥¼ ì˜¬ë¦¬ê³  4ê°œì›” ë§Œì— URLì´ í‹€ë ¸ë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ë‹¤... í•˜ì§€ë§Œ ì–´ì©” ìˆ˜ ì—†ë‹¤... ê·¸ëƒ¥ ê°„ë‹¤...  
fastattentiondl ì•„ë‹ˆë¼ flashattentionìœ¼ë¡œ í•´ì•¼ í•˜ëŠ”ë°....
