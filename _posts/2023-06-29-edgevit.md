---
layout: post
title:  "EdgeViT"
date:   2023-06-29 00:00:00 +0900
description: CVPR2023 award 후보
categories: [backbone, paper, vit]
tags: [backbone, paper, cvpr, vit, efficient-architecture]

---  


# Intorduction
 ViT는 global한 representation을 학습하면서 imagenet banchmark에서 압도적인 성능을 내고있다. 하지만 self-attention이라는 연산의 비용이 커서 inference speed나 power efficiency가 떨어진다. 이를 해결하기 위한 기존의 연구는 크게 다음과 같았다.
 
 1. Spatial Resolution에서 hierarchical architecture을 만들어 연산량을 줄인다.
 2. Locally-grouped self-attention mechanism을 사용한다.
 3. Key, value를 pooling하여 subsampling한다.

 하지만 이러한 방법은 time cost가 중요한 mobile이나 edge platform에서 충분하지 못하다. 따라서 저자는 다음의 요소를 고려하여 EdgeViT를 설계했다.

 **1. Inference efficiency**
   EdgeViT는 on-device에서 사용할 정도로 가볍고 에너지를 적게 사용해야한다. 기존에는 이를 측정하기위해 FLOPS를 주로 사용했지만 이는 latency과 energy consumption을 제대로 반영하지 못한다. 따라서 FLOPS는 추정치로만 참고하고 실제 mobile device에서 밴치마크를 진행한다.
  
 **2. Model size**
   현대 스마트폰은 RAM이 32GB일정도로 메모리량이 충분하다. 따라서 절대적인 모델크기를 고려하는 것은 현대 mobile device에는 적합하지 않아서 이는 크게 고려하지 않는다.
 
 **3. Implementation Friendliness**
  현실적으로 implementation을 하기위해서는 ONNX, TensorRT, TorchScript등 기존의 framework와의 호환성을 고려해야한다.

 이를 고려하여 저자는 local-global-local bottleneck을 제안했고 이는 energy efficient하고 inference speed도 빠르다.

# Local-Global-Local-BottleNeck

<p align="center">
    <img src="/assets/post/image/edgevit/Fig3.png" width="80%">
</p>
 
 ViT의 성능이 좋은 이유는 global한 representation을 학습하기 유리하기 때문이다. Multi-head self-attention이 이를 가능하게 만들었으니 이를 모사하는 연산을 만드는 것을 필수적이다. 따라서 저자는 factorization을 통해 해당 연산을 3가지로 쪼개 모사했다.

## Local Aggregation
 연산을 가볍게 만들기 위해서는 절대적인 연산량을 줄여야 한다. 다행이도 이미지에서는 주변 픽셀은 비슷하다는 inductive bias가 존재한다. 따라서 전체 token에 대하여 attention을 계산하지 않고 주변 픽셀의 정보를 aggregation함으로써 scope를 줄일 수 있다. 따라서 attention 계산에 앞서서 locally proximate tokens의 signals을 integrate한다. 이는 기존 depth-wise separable convolution을 사용한다.

## Global Sparse Attention
 이제 attention에 대한 scope을 줄였으니 local window를 나타내는 representaion인 delegate token간 attention을 계산하자. 이 때 $$r \times r$$ local window의 center값을 나타내는 token을 뽑아서 log-range relation을 계산하기 위해 self-attention을 계산한다. 이때 $$r$$ 은 sampling rate을 나타내게 된다.

## Local Propagation
 이제 global contextual information을 계산했으니 local window에 전파해야한다. 이는 간단하게 transpose convolution으로 구성했다.

위의 3가지 연산을 통해 local-global-local bottleneck을 구성한다. 그리고 이는 다음과 같은 식으로 연결된다.
$$X=LocalAgg(Norm(X_{in}))+X_{in}$$
$$Y=FFN(Norm(X))+X$$
$$Z=LocalProp(GlobalSpaarseAttn(Norm(Y)))+Y$$
$$X_{out}=FFN(Norm(Z))+Z$$
 이 때 FFN은 fully conntected layer 2개로 구성되어있으며 Normalization은 Layer Normalization을 사용한다.



## Model Archtecture
``
<p align="center">
    <img src="/assets/post/image/edgevit/Fig2.png" width="80%">
</p>

전체적인 모델구조는 위와 같다. Spatial resolution을 기준으로 hierachical 구조를 가지고 있다. Downsampling은 1번째만 제외하고 2x2 convolotion을 stride 2로 구성한다. 첫 번째 downsamping은 4x4 convolution은 stride 4로 연산한다. Patch embedding은 요즘 성능이 좋은 relative positional embedding\[[paper](https://arxiv.org/abs/1803.02155)]을 사용한다.

<p align="center">
    <img src="/assets/post/image/edgevit/table1.png" width="80%">
</p>

또한 scalarbility를 위해 3개의 모델 구조로 만든다.

# Experiment
밴치마크를 위해서 삼성스마트폰을 사용했으며  CPU는 Snapdragon 888을 사용했다. 또한 TorchScript lite를 사용하여 50 step을 기준으로 pytorch에서 제공하는 android benchmarking app을 사용하여 측정했다. 또한 전력을 측정하기 위해서 Monsoon High Voltage Power Monitor울 Snapdragon 888 Hardware Development Kit (HDK8350)과 연결하여 측정했으며, NPU는 범용성이 떨어져서 측정하지 않았다고 한다. 
## ImageNet

<p align="center">
    <img src="/assets/post/image/edgevit/table2.png" width="80%">
</p>

가벼운 ViT 모델 중에서는 좋은 성능을 보여준다. 하지만 CNN 계열과 비교했을 때는 다소 아쉬운 성능을 보인다.

<p align="center">
    <img src="/assets/post/image/edgevit/table3.png" width="80%">
</p>

실험하면서 정확도가 낮을수록 전력이 줄어든다는 것을 발견했고, EdgeViT는 전력대비 정확도를 측정하면 좋은 결과를 낸다
