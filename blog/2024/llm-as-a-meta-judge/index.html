<!DOCTYPE html> <html lang="kr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> META-REWARDING LANGUAGE MODELS: Self-Improving Alignment with LLM-as-a-Meta-Judge 설명 | Wonbeom Jang </title> <meta name="author" content="Wonbeom Jang"> <meta name="description" content="개발을 좋아하는 딥러닝 리서쳐 장원범입니다. "> <meta name="keywords" content="computer-vision, machine-learning, software-engineer, software-engineering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.wonbeomjang.kr/blog/2024/llm-as-a-meta-judge/"> </head> <body class="fixed-top-nav "> <header> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1352545434285249" crossorigin="anonymous"></script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wonbeom </span> Jang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">META-REWARDING LANGUAGE MODELS: Self-Improving Alignment with LLM-as-a-Meta-Judge 설명</h1> <p class="post-meta"> September 19, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/paper"> <i class="fa-solid fa-hashtag fa-sm"></i> paper</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>     ·   <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> paper</a>   <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> llm</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>InstructGPT의 성공 이후, LLM의 instruction following 능력은 매우 중요한 요소로 자리 잡았다.<br> 이를 개선하기 위해 SFT, preference optimization(RLHF, DPO, PPO, KPO 등)과 같은 방법들이 사용되었으나, 이러한 방식들은 많은 시간과 비용이 소요된다는 한계가 있다.</p> <p>이를 해결하기 위해 Self-Reward 방법론이 제시되었다. 이 접근법에서는 하나의 LLM이 Actor와 Judge 두 가지 역할을 수행하며 자체적으로 preference optimization을 수행한다.</p> <ul> <li> <strong>Actor</strong>: 주어진 instruction에 대한 response를 생성.</li> <li> <strong>Judge</strong>: Actor가 생성한 response를 평가하며, LLM-as-a-Judge 방식을 활용해 reward가 되는 preference pair를 생성.<br> 하지만 기존 방식은 Actor가 좋은 response를 생성하는 데만 초점이 맞춰져 있어 Judge의 성능에는 관심을 두지 않는다는 한계가 있다.</li> </ul> <p>이 문제를 해결하기 위해 저자는 <strong>LLM-as-a-Meta-Judge</strong>를 제안했다.<br> 이 방법론의 핵심은 LLM이 Actor와 Judge 역할뿐만 아니라 Meta-Judge 역할까지 수행하도록 하여 Judge 능력에 대한 추가적인 reward를 제공하는 것이다.</p> <hr> <h1 id="meta-rewarding">Meta-Rewarding</h1> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/fig1.png" width="80%"></p> <p>Meta-Rewarding은 세 가지 주요 구성 요소로 이루어진다:</p> <ul> <li> <strong>Actor</strong>: 주어진 instruction에 대해 다수의 response를 생성.</li> <li> <strong>Judge</strong>: LLM-as-a-Judge 프롬프트를 통해 각 response를 평가하고 score를 생성.<br> 이 score는 Actor를 학습시키는 preference pair로 사용된다.</li> <li> <strong>Meta-Judge</strong>: 여러 Judge를 평가해 가장 적합한 Judge를 선택.<br> 여기서는 LLM-as-a-Meta-Judge 프롬프트를 활용해 결과를 생성하고, 이를 Judge 학습용 preference pair로 사용한다.</li> </ul> <hr> <h2 id="actor-preference-dataset-creation">Actor Preference Dataset Creation</h2> <h3 id="1-sample-responses-from-actor">1. Sample Responses from Actor</h3> <p>Iteration \(t\)에서 현재 모델 \(M_t\)를 사용하여 \(K\)개의 response를 생성.</p> \[\{y_1, ..., y_{K}\}\] <h3 id="2-aggregate-multiple-judgements">2. Aggregate Multiple Judgements</h3> <p>각 response \(y_k\)에 대해 N개의 서로 다른 Judge를 생성하며, 5점 척도로 평가.<br> 만약 parsing이 불가능한 경우 해당 데이터를 제외(drop).</p> \[\{j_k^1, ..., j_k^N\}\] <h3 id="3-preference-data-selection-with-length-control">3. Preference Data Selection with Length-Control</h3> <ul> <li>최고 점수 \(S_{\text{max}}\)의 response \(y_c\)와 최저 점수 \(S_{\text{min}}\)의 response \(y_r\)를 선택.</li> <li>길이 조정을 통해 response quality를 일정 수준 이상 유지.</li> <li>점수 범위 내 비슷한 quality는 제외(drop).</li> </ul> <hr> <h2 id="judge-preference-data-creation">Judge Preference Data Creation</h2> <h3 id="1-responses-selection">1. Responses Selection</h3> <ul> <li>모든 데이터를 사용하는 것은 비효율적이므로, judge confidence가 낮은 데이터를 우선 선택.</li> <li>instruction에 대한 response score의 분산(variance)이 가장 높은 데이터를 활용.</li> </ul> <h3 id="2-pairwise-meta-judge-evaluation">2. Pairwise Meta-Judge Evaluation</h3> <ul> <li>\(\{j^1, ..., j^N\}\)에서 두 개의 judgement를 선택해 \((j^m, j^n)\) 구성.</li> <li>두 judge 순서를 바꿔 평가하여 position bias를 해결.</li> <li>평가 결과가 같으면 accept, 다르면 reject.</li> </ul> <p>Position별 가중치 계산:</p> \[\omega_{1} = \frac{\text{win}_{\text{2nd}}}{\text{win}_{\text{1nd}} + \text{win}_{\text{2nd}}}, \text{ } \omega_{2} = \frac{\text{win}_{\text{1nd}}}{\text{win}_{\text{1nd}} + \text{win}_{\text{2nd}}}\] <p>Meta-Judge 결과로 battle result 계산:</p> \[r_{mn} = \begin{cases} 1 &amp; \text{if the meta-judge prefers } j_m \\ -1 &amp; \text{if the meta-judge prefers } j_n \\ 0 &amp; \text{if tie or parse error.} \end{cases}\] \[B_{mn} = \omega_1 \mathbb{1}[r^{mn} = 1] + \omega_2 \mathbb{1}[r^{nm} = -1]\] <h3 id="3-elo-score-and-pair-selection">3. Elo Score and Pair Selection</h3> <ul> <li>Elo score를 통해 judge의 reward 계산.</li> </ul> <hr> <h1 id="experiments">Experiments</h1> <h2 id="experiment-set-up">Experiment Set-up</h2> <p>Iteration마다 학습 방법을 달리하여 Meta-Rewarding 효과를 평가.</p> <ul> <li> <strong>Iter 1</strong>: SFT 모델에서 시작해 DPO를 통해 Actor와 Judge preference pair를 학습하여 \(M_1\) 생성.</li> <li> <strong>Iter 2</strong>: \(M_1\)을 기반으로 Actor와 Judge preference pair를 학습하여 \(M_2\) 생성.</li> <li> <strong>Iter 3</strong>: \(M_2\)에서 Actor preference pair만 학습하여 \(M_3\) 생성.</li> <li> <strong>Iter 4</strong>: \(M_3\)에서 Actor preference pair만 학습하여 \(M_4\) 생성.</li> </ul> <hr> <h2 id="instruction-following-evaluation">Instruction Following Evaluation</h2> <h3 id="meta-rewarding-iterations-significantly-improve-the-win-rate">Meta-Rewarding iterations significantly improve the win rate</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/fig3.png" width="80%"></p> <p>Meta-Rewarding은 특히 Length Control 조건에서 높은 성능을 보임.</p> <h3 id="the-meta-judge-and-length-control-mechanism-are-important">The meta-judge and length-control mechanism are important</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table1.png" width="80%"></p> <p>Table 1에 따르면, iteration이 진행됨에 따라 평균 길이가 증가하지 않음을 확인.</p> <h3 id="meta-rewarding-improves-nearly-all-instruction-categories">Meta-Rewarding improves nearly all instruction categories</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/fig4.png" width="80%"></p> <p>거의 모든 카테고리에서 성능 향상 확인.</p> <h3 id="meta-rewarding-enhances-responses-to-complex-and-hard-questions">Meta-Rewarding enhances responses to complex and hard questions</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table2.png" width="80%"></p> <p>복잡한 질문(arena-hard)에 대해서도 높은 성능 보임.</p> <h3 id="meta-rewarding-does-not-sacrifice-multi-turn-ability">Meta-Rewarding does not sacrifice multi-turn ability</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table6.png" width="80%"></p> <p>Single-turn 데이터만으로 학습했음에도 multi-turn 성능 유지.</p> <hr> <h2 id="reward-modeling-evaluation">Reward Modeling Evaluation</h2> <h3 id="the-model-improves-in-judging-after-judge-training">The model improves in judging after judge training</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table3.png" width="80%"></p> <p>Meta-Rewarding은 GPT-4와의 judge 상관관계를 개선.</p> <h3 id="meta-rewarding-improves-human-judge-correlation">Meta-Rewarding improves human judge correlation</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table7.png" width="80%"></p> <p>사람과의 judge 상관관계 역시 개선.</p> <hr> <h2 id="ablations-and-analysis">Ablations and Analysis</h2> <h3 id="length-control-mechanism">Length-Control Mechanism</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table4.png" width="80%"></p> <p>Length Control이 없으면 verbosity 증가.</p> <h3 id="meta-judge-biases">Meta-Judge Biases</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table5.png" width="80%"></p> <p>높은 점수를 준 judge를 선호하는 경향 발견.</p> <h3 id="judge-scoring-shift">Judge Scoring Shift</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/fig5.png" width="80%"></p> <p>Score 분포가 학습 중 5점으로 집중됨(score-bias).</p> <hr> <h3 id="limitations">Limitations</h3> <ul> <li>Judge 모델이 적은 quality 차이를 tie로 판단하는 경향 있음.</li> <li>Meta-Judge에서 bias가 존재.</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/byol/">Bootstrap your own latent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/pretraining-data-dection-for-large-language-models/">Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method 설명</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/fado/">Fairness-aware Data Valuation for Supervised Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/tinyvit/">TinyViT</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/NASNet/">[AutoML] NASNet</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"wonbeomjang/wonbeomjang.github.io","data-repo-id":"R_kgDOIsrE7Q","data-category":"Comments","data-category-id":"DIC_kwDOIsrE7c4CaZnD","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"ko",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Wonbeom Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 24, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-NH0GKRG1BP"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-NH0GKRG1BP");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>