<!DOCTYPE html> <html lang="kr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> META-REWARDING LANGUAGE MODELS: Self-Improving Alignment with LLM-as-a-Meta-Judge 설명 | Wonbeom Jang </title> <meta name="author" content="Wonbeom Jang"> <meta name="description" content="개발을 좋아하는 딥러닝 리서쳐 장원범입니다. "> <meta name="keywords" content="computer-vision, machine-learning, software-engineer, software-engineering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.wonbeomjang.kr/blog/2024/llm-as-a-meta-judge/"> </head> <body class="fixed-top-nav "> <header> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1352545434285249" crossorigin="anonymous"></script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wonbeom </span> Jang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">META-REWARDING LANGUAGE MODELS: Self-Improving Alignment with LLM-as-a-Meta-Judge 설명</h1> <p class="post-meta"> September 19, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/paper"> <i class="fa-solid fa-hashtag fa-sm"></i> paper</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>     ·   <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> paper</a>   <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> llm</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>InstructGPT가 성공한 이후로 LLM의 instruction following 능력은 중요하다는 것을 알게 되었다. 따라서 SFT나 preference optimization(RLHF, DPO, PPO, KPO, …)을 통해 human alignment를 높이려고 했다. 하지만 이와 같은 방법들은 많은 시간과 돈이 소요된다는 단점이 있다.</p> <p>따라서 Self-Reward라는 방법이 제시되었다. 이 방법론은 하나의 LLM이 Actor, Judge 두 가지 역할을 수행하면서 자체적으로 preference optimization을 수행한다.</p> <ul> <li>Actor: Specific instruction에 대한 response를 생성한다.</li> <li>Judge: Actor가 생성한 response를 LLM-as-a-Judge 방식으로 수행하여 reward가 되는 preference pair를 생성한다.<br> 하지만 이와 같은 방법도 Actor가 좋은 response를 생성하는 데만 관심이 있고, judge의 성능에는 관심이 없다는 것에 대한 단점이 있다.</li> </ul> <p>따라서 저자는 Judge의 성능을 높이기 위해서 LLM-as-a-Meta-Judge를 제안했다. 핵심 아이디어는 하나의 LLM이 Actor, Judge뿐만 아니라 Meta-Judge 역할도 수행한다는 것이다. 이를 통해 모델의 judge 능력에 대한 reward를 줄 수 있다.</p> <h1 id="meta-rewarding">Meta-Rewarding</h1> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/fig1.png" width="80%"></p> <p>Meta Rewarding은 response를 생성하는 actor, response를 평가하는 judge, 그리고 judge를 평가하는 meta-judge로 구성된다.</p> <p><strong>Actor</strong><br> Actor는 각각의 instruction에 대하여 다수의 response를 생성한다.</p> <p><strong>Judge</strong><br> Judge는 LLM-as-a-Judge 프롬프트로 각 response에 대해 score가 포함된 judge를 생성한다. 여기서 생성된 score는 actor를 학습시키기 위한 preference pair가 된다.</p> <p><strong>Meta-Judge</strong><br> 하나의 response에 대하여 여러 가지의 judge를 뽑아 어떤 judge가 좋은지 판단한다. LLM-as-a-Meta-Judge prompt가 사용되고, 여기서 생성된 결과는 judge를 학습시키기 위한 preference pair가 된다.</p> <h2 id="actor-preference-dataset-creation">Actor Preference Dataset Creation</h2> <h3 id="1-sample-responses-from-actor">1. Sample Responses from Actor</h3> <p>Iteration \(t\)일 때 현재 모델 \(M_t\)를 이용하여 \(K\)개의 response를 생성한다.</p> \[\{y_1,...,y_{K}\}\] <h3 id="2-aggregate-multiple-judgements">2. Aggregate Multiple Judgements</h3> <p>각 response \(y_k\)에 대하여 N개의 서로 다른 judge를 생성한다. 5점 scale로 평가하되 parsing이 되지 않으면 drop한다.</p> \[\{j_k^1,...,j_k^N\}\] <h3 id="3-preference-data-selection-with-length-control">3. Preference Data Selection with Length-Control</h3> <p>가장 높은 점수 \(S_{\text{max}}\)를 가진 \(y_c\), 가장 낮은 점수 \(S_{\text{min}}\)을 가진 \(y_r\)를 선택한다. 단, 해당 response를 그대로 쓰지 않고, length control을 통해 길이 조정을 한다.</p> \[[(1 - \rho) S_{\text{max}} + \rho S_{\text{min}}, S_{\text{max}}]\] <p>위 식 안에 점수가 들어가면 비슷한 quality로 판단하여 drop한다. 그리고 최대한 짧은 답변을 고르려고 노력했다.</p> <h2 id="judge-preference-data-creation">Judge Preference Data Creation</h2> <h3 id="1-responses-selection">1. Responses Selection</h3> <p>모든 데이터를 활용하는 것은 비효율적이다. 따라서 학습 효율을 위해 judge confidence가 가장 낮은 데이터에 집중한다. 따라서 하나의 instruction에 대해 response score의 variance가 가장 높은 데이터부터 시작한다.</p> <h2 id="2-pairwise-meta-judge-evaluation">2. Pairwise Meta-Judge Evaluation</h2> <p>\(\{j^1, ..., j^N\}\)에서 두 가지 judgement를 뽑아 \((j^m, j^n)\)을 구성하고 LLM-as-a-Meta-Judge를 수행한다. 이때 position bias를 해결하기 위해 두 judge의 순서를 바꿔서 다시 수행한다. 그리고 만약 결과가 같으면 accept하고, 결과가 다르면 reject한다. 또한 first position과 second position의 가중치를 계산하여 보정했다.</p> \[\omega_{1} = \frac{\text{win}_{\text{2nd}}}{\text{win}_{\text{1nd}} + \text{win}_{\text{2nd}}}, \text{ } \omega_{2} = \frac{\text{win}_{\text{1nd}}}{\text{win}_{\text{1nd}} + \text{win}_{\text{2nd}}}\] <p>그리고 각 judge 결과를 이용하여 battle result를 만든다.</p> \[r_{mn} = \begin{cases} 1 &amp; \text{if the meta-judge prefers } j_m \\ -1 &amp; \text{if the meta-judge prefers } j_n \\ 0 &amp; \text{if tie or parse error.} \end{cases}\] \[B_{mn} = \omega_1 \mathbb{1}[r^{mn} = 1] + \omega_2 \mathbb{1}[r^{nm} = -1]\] <h2 id="3-elo-score-and-pair-selection">3. Elo Score and Pair Selection</h2> <p>이후에 Elo score를 계산하여 reward를 구한다.</p> \[\arg\max_{\varepsilon} \sum_{m,n} B_{mn} \log \left( \frac{e^{\varepsilon_m - \varepsilon_n}}{1 + e^{\varepsilon_m - \varepsilon_n}} \right).\] <p>이때도 judge의 length가 너무 길어지면 reject한다.</p> <h1 id="experiments">Experiments</h1> <h2 id="experiment-set-up">Experiment Set-up</h2> <p>각 Iteration마다 학습 방법을 바꾼다.</p> <blockquote> <p>Iter 1 Obtain \(M_1\) by training using DPO (initialized from the SFT model) on both actor and judge preference pairs generated by the SFT model.<br> Iter 2 Obtain \(M_2\) by training \(M_1\) using DPO on actor and judge preference pairs generated by \(M_1\).<br> Iter 3 Obtain \(M_3\) by training \(M_2\) using DPO exclusively on actor preference pairs generated by \(M_2\).<br> Iter 4 Obtain \(M_4\) by training \(M_3\) using DPO exclusively on actor preference pairs generated by \(M_3\).</p> </blockquote> <h2 id="instruction-following-evaluation">Instruction Following Evaluation</h2> <h3 id="meta-rewarding-iterations-significantly-improves-the-win-rate">Meta-Rewarding iterations significantly improves the win rate</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/fig3.png" width="80%"></p> <p>저자는 Length Control win rate에서 좋은 성능을 보인다고 했다.</p> <h3 id="the-meta-judge-and-length-control-mechanism-are-important">The meta-judge and length-control mechanism are important.</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table1.png" width="80%"></p> <p>Table 1에서 볼 수 있듯, average lengths는 iteration에 따라 증가하지 않는다는 것을 보이고 있다.</p> <h3 id="meta-rewarding-improves-nearly-all-instruction-categories">Meta-Rewarding improves nearly all instruction categories.</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/fig4.png" width="80%"></p> <p>Fig4에서 볼 수 있듯 거의 모든 카테고리에서 성능 향상이 일어났다.</p> <h3 id="meta-rewarding-improves-answering-of-complex-and-hard-questions">Meta-Rewarding improves answering of complex and hard questions.</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table2.png" width="80%"></p> <p>Arena-hard에서도 좋은 성능을 보였다.</p> <h3 id="meta-rewarding-does-not-sacrifice-multi-turn-ability-despite-training-only-on-single-turn">Meta-Rewarding does not sacrifice multi-turn ability despite training only on single-turn</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table6.png" width="80%"></p> <p>Single-turn으로만 학습했음에도 불구하고 multi-turn의 성능을 떨어뜨리지 않았다.</p> <h2 id="reward-modeling-evaluation">Reward Modeling Evaluation</h2> <h3 id="the-model-improves-in-judging-after-performing-judge-training">The model improves in judging after performing judge training</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table3.png" width="80%"></p> <p>Meta-Rewarding 방법은 GPT-4와의 judge 상관관계를 높여주는 것으로 나왔다.</p> <h3 id="meta-rewarding-training-improve-judging-correlation-with-human">Meta-Rewarding training improve judging correlation with Human</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table7.png" width="80%"></p> <p>Meta-Rewarding 방법은 사람과의 judge 상관관계를 높여주는 것으로 나왔다.</p> <h2 id="ablations-and-analysis">Ablations and Analysis</h2> <h3 id="length-control-mechanism">Length-Control Mechanism</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table4.png" width="80%"></p> <p>Length control을 안 썼을 때 verbosity가 발생하는 것을 보였다.</p> <h3 id="meta-judge-biases">Meta-Judge Biases</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/table5.png" width="80%"></p> <p>Meta-Rewarding 방법은 높은 점수를 준 judge를 선호하는 것으로 나왔다.</p> <h3 id="judge-scoring-shift">Judge Scoring Shift</h3> <p align="center"><img src="/assets/post/image/llm-as-a-meta-judge/fig5.png" width="80%"></p> <p>위의 문제를 보기 위해 Gaussian kernel density estimation을 이용해서 score의 분포를 보았다. 이는 score-bias로 학습하는 동안 score 분포를 5점에 가까운 분포로 바꾸게 되었다.</p> <h3 id="limitations">Limitations</h3> <p>Judge 모델이 판단할 때 적은 quality 차이는 tie를 주는 경향이 있으므로 평균낼 때 주의해야 한다.<br> 또한 meta-judge에서 bias가 있다.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/byol/">Bootstrap your own latent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/meta-pseudo-label/">Meta Pseudo Labels</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/tinyvit/">TinyViT</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/fado/">Fairness-aware Data Valuation for Supervised Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/multimodal-vs-unimodal/">What Makes Multi-modal Learning Better than Single (Provably)</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"wonbeomjang/wonbeomjang.github.io","data-repo-id":"R_kgDOIsrE7Q","data-category":"Comments","data-category-id":"DIC_kwDOIsrE7c4CaZnD","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"ko",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Wonbeom Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 20, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>