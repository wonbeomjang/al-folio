<!DOCTYPE html> <html lang="kr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method 설명 | Wonbeom Jang </title> <meta name="author" content="Wonbeom Jang"> <meta name="description" content="개발을 좋아하는 딥러닝 리서쳐 장원범입니다. "> <meta name="keywords" content="computer-vision, machine-learning, software-engineer, software-engineering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.wonbeomjang.kr/blog/2024/pretraining-data-dection-for-large-language-models/"> </head> <body class="fixed-top-nav "> <header> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1352545434285249" crossorigin="anonymous"></script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wonbeom </span> Jang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method 설명</h1> <p class="post-meta"> December 11, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/paper"> <i class="fa-solid fa-hashtag fa-sm"></i> paper</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>     ·   <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> paper</a>   <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> llm</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>많은 LLM(대규모 언어 모델) 개발자들은 사용된 학습 코퍼스를 비공개로 처리합니다. 이는 저작권과 윤리적 문제와 같은 이유 때문입니다. 이러한 상황에서 저자는 블랙박스 LLM과 텍스트가 주어졌을 때, 해당 텍스트가 학습 데이터에 포함되어 있는지 확인할 수 있는 방법론을 제시합니다.</p> <h2 id="아이디어">아이디어</h2> <p>이 연구는 <strong>Divergence-from-randomness</strong>에서 영감을 받았습니다. 특정 단어의 <strong>문서 내 사용 빈도(Within-document term-frequency)</strong>와 <strong>전체 문서 컬렉션 내 사용 빈도(frequency of a word within the collection)</strong> 간 차이를 측정함으로써 해당 단어가 문서에서 얼마나 중요한 정보를 담고 있는지 알 수 있다는 개념입니다. 이를 기반으로 다음과 같은 측정 방법이 제안되었습니다:</p> <ol> <li> <p><strong>Within-document term-frequency</strong></p> <ul> <li>LLM이 예측한 토큰의 확률로 계산됩니다.</li> <li>이는 토큰 확률 분포(Token probability distribution)를 의미합니다.</li> </ul> </li> <li> <p><strong>Frequency of a word within the collection</strong></p> <ul> <li>코퍼스에서 해당 토큰의 평균 등장 빈도를 나타냅니다.</li> <li>이는 토큰 빈도 분포(Token frequency distribution)로 정의됩니다.</li> </ul> </li> </ol> <p>토큰 확률 분포와 토큰 빈도 분포 간의 <strong>Divergence</strong>가 높다면, 해당 텍스트가 모델의 학습 코퍼스에 포함되었을 가능성을 나타냅니다.</p> <hr> <h2 id="방법론">방법론</h2> <h3 id="문제-정의">문제 정의</h3> <p>텍스트 \(x\), LLM \(\mathcal{M}\), 정보가 없는 학습 코퍼스 \(D\), 학습 데이터 검출 과제 \(\mathcal{A}\)에 대해 다음을 정의합니다:</p> \[\mathcal{A}(x,\mathcal{M})\rightarrow\{0,1\}\] <ol> <li> <p><strong>Token Probability Distribution Computation</strong></p> <ul> <li>LLM \(\mathcal{M}\)에 텍스트 \(x\)를 질의하여 각 토큰 확률을 계산합니다.</li> </ul> </li> <li> <p><strong>Token Frequency Distribution Computation</strong></p> <ul> <li>접근 가능한 대규모 참조 코퍼스 \(\mathcal{D}^\prime\)를 사용하여 토큰 빈도를 추정합니다.</li> </ul> </li> <li> <p><strong>Score Calculation via Comparison</strong></p> <ul> <li>두 분포를 비교하여 각 토큰의 확률을 조정(calibration)하고, 이를 기반으로 학습 데이터 여부를 판단할 점수를 계산합니다.</li> </ul> </li> <li> <p><strong>Binary Decision</strong></p> <ul> <li>점수에 임계값을 적용하여 \(x\)가 모델 \(\mathcal{M}\)의 학습 코퍼스에 포함되어 있는지 예측합니다.</li> </ul> </li> </ol> <hr> <h3 id="세부-절차">세부 절차</h3> <h4 id="token-probability-distribution-computation">Token Probability Distribution Computation</h4> <p>시작 토큰 \(x_0\)를 포함하여 텍스트 \(x\)는 다음과 같이 정의됩니다:</p> \[x^\prime=x_0x_1x_2...x_n\] <p>\(\mathcal{M}\)에 \(x\)를 질의하여 다음을 계산합니다:</p> \[\{p(x_i|x_{&lt; i};\mathcal{M}): 0 &lt; i \le n\}\] <h4 id="frequency-of-a-word-within-the-collection">Frequency of a Word within the Collection</h4> <p>참조 코퍼스 \(\mathcal{D}^\prime\)에서 특정 토큰 \(x_i\)의 빈도는 다음과 같이 계산됩니다:</p> \[p(x_i, \mathcal{D}^\prime) = \frac{\text{count}(x_i)}{N^\prime}\] <p>만약 \(x_i\)가 코퍼스에 존재하지 않는 경우, 라플라스 스무딩(Laplace Smoothing)을 적용합니다:</p> \[p(x_i; D^\prime) = \frac{\text{count}(x_i) + 1}{N^\prime + |V|}\] <table> <tbody> <tr> <td>여기서 $$</td> <td>V</td> <td>$$는 어휘(vocabulary) 크기입니다.</td> </tr> </tbody> </table> <h4 id="score-calculation-through-compression">Score Calculation through Compression</h4> <p>토큰 확률 \(p(x_i;\mathcal{M})\)와 참조 코퍼스 확률 \(p(x_i;D^\prime)\) 간의 크로스 엔트로피(Cross-Entropy)는 다음과 같이 계산됩니다:</p> \[\alpha_i = -p(x_i; \mathcal{M}) \cdot \log p(x_i; D^\prime).\] <p>특정 토큰이 우세한 영향을 미치지 않도록 상한선을 정의합니다:</p> \[\alpha_i = \begin{cases} \alpha_i, &amp; \text{if } \alpha_i &lt; a \\ a, &amp; \text{if } \alpha_i \geq a \end{cases}\] <p>텍스트 \(x\)에서 여러 토큰 \(x_i\)가 존재할 때, 평균을 계산하여 최종 점수를 구합니다:</p> \[\beta = \frac{1}{|\text{FOS}(x)|} \sum_{x_j \in \text{FOS}(x)} \alpha_j\] <h4 id="binary-decision">Binary Decision</h4> <p>최종적으로 점수 \(\beta\)에 임계값 \(\tau\)를 적용하여 학습 코퍼스 포함 여부를 판단합니다:</p> \[\text{Decision}(x, \mathcal{M}) = \begin{cases} 0 \quad (x \notin \mathcal{D}), &amp; \text{if } \beta &lt; \tau, \\ 1 \quad (x \in \mathcal{D}), &amp; \text{if } \beta \geq \tau. \end{cases}\] <hr> <h2 id="experimental-results">Experimental Results</h2> <h3 id="main-result">Main Result</h3> <p>Wiki 데이터를 기반으로 한 실험 결과는 아래와 같습니다:</p> <p align="center"><img src="/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%201.png" width="80%"></p> <hr> <h3 id="ablation-studies">Ablation Studies</h3> <p>다양한 설정에서 실험한 결과는 다음과 같습니다:</p> <p align="center"><img src="/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%202.png" width="80%"></p> <p align="center"><img src="/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%203.png" width="80%"></p> <h4 id="baselines">Baselines</h4> <ul> <li> <strong>CLD</strong>: Baseline</li> <li> <strong>+LUP</strong>: Upper Bound 추가</li> <li> <strong>+SFO</strong>: 동적 Threshold 적용</li> </ul> <h4 id="reference-corpus">Reference Corpus</h4> <p>참조 코퍼스로 무엇을 사용하더라도 결과에는 큰 차이가 없음을 보여줍니다:</p> <p align="center"><img src="/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%204.png" width="80%"></p> <h4 id="upper-bound">Upper Bound</h4> <p>Upper Bound는 각 토큰에 대해 다르게 적용해야 합니다:</p> <p align="center"><img src="/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%205.png" width="80%"></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/SENet/">학부생이 본 SENet</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/llm-as-a-meta-judge/">META-REWARDING LANGUAGE MODELS: Self-Improving Alignment with LLM-as-a-Meta-Judge 설명</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/byol/">Bootstrap your own latent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/multimodal-vs-unimodal/">What Makes Multi-modal Learning Better than Single (Provably)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EA%B2%BD%EB%9F%89%ED%99%94-EffientNet/">[네트워크 경량화] EfficientNet</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"wonbeomjang/wonbeomjang.github.io","data-repo-id":"R_kgDOIsrE7Q","data-category":"Comments","data-category-id":"DIC_kwDOIsrE7c4CaZnD","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"ko",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Wonbeom Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 19, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>