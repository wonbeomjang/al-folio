<!DOCTYPE html> <html lang="kr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method 설명 | Wonbeom Jang </title> <meta name="author" content="Wonbeom Jang"> <meta name="description" content="개발을 좋아하는 딥러닝 리서쳐 장원범입니다. "> <meta name="keywords" content="computer-vision, machine-learning, software-engineer, software-engineering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.wonbeomjang.kr/blog/2024/pretraining-data-dection-for-large-language-models/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wonbeom </span> Jang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method 설명</h1> <p class="post-meta"> December 17, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/paper"> <i class="fa-solid fa-hashtag fa-sm"></i> paper</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>     ·   <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> paper</a>   <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> llm</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <ul> <li>많은 model 개발자들은 LLM을 개발할 때 사용한 corpus를 비공개로 처리한다. <ul> <li>저작권, 윤리적 문제가 존재하기 때문이다.</li> </ul> </li> <li>저자는 black-box LLM과 text가 주어졌을 때 해당 text가 training data에 포함되어있는지 확인하는 방법론을 제시한다.</li> </ul> <h2 id="아이디어">아이디어</h2> <ul> <li>Divergence-from-randomness에서 영감을 받는다.</li> <li> <strong><em>특정 단어가 한 문서 내에서 사용되는 빈도(Within-document term-frequency)</em></strong>와 <strong><em>그 단어가 전체 문서 컬렉션 내에서 사용되는 빈도(frequency of a word within the collection)</em></strong>가 얼마나 차이가 나는지를 측정하면, 그 단어가 해당 문서에서 얼마나 중요한 정보를 담고 있는지 알 수 있다. <ul> <li>Within-document term-frequency <ul> <li>LLM predicted token의 probability</li> <li>Token probability distribution</li> </ul> </li> <li>Frequency of a word within the collection <ul> <li>Corpus에서 해당 token의 빈도수</li> <li>Token frequency distribution</li> </ul> </li> </ul> </li> <li>Token probability distribution와 Token frequency distribution의 divergence가 높으면 해당 text가 모델의 training corpus에 있다는 의미다.</li> </ul> <h2 id="방법론">방법론</h2> <ol> <li> <strong>Within-document term-frequency</strong> <ul> <li>특정 text에서 probability distribution을 계산한다.</li> </ul> </li> <li> <strong>Frequency within the collection</strong> <ul> <li>전체 corpus에서 해당 token이 평균적으로 얼마나 자주 등장하는지를 나타낸다.</li> </ul> </li> <li> <strong>Divergence</strong> <ul> <li>Within-document term-frequency와 Frequency within the collection을 비교한다.</li> </ul> </li> </ol> <h1 id="problem-statement">Problem Statement</h1> <ul> <li>Text \(x\), LLM \(\mathcal{M}\), 정보가 없는 pretraining corpus \(D\), pretraining data detection task \(A\)에 대해서 <ul> <li> \[\mathcal{A}(x,\mathcal{M})\rightarrow\{0,1\}\] </li> </ul> </li> </ul> <p align="center"><img src="/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image.png" width="80%"></p> <ol> <li>Token probability distribution computation <ul> <li>\(\mathcal{M}\)에 텍스트 \(x\)를 query하여 각 token probability를 계산한다.</li> </ul> </li> <li>Token probability distribution computation <ul> <li>접근 가능한 대규모 참조 말뭉치 \(\mathcal{D}^\prime\)를 사용하여 토큰 빈도를 추정한다.</li> </ul> </li> <li>Score calculation via <em>comparison</em> <ul> <li>두 분포를 비교하여 각 token의 probability을 calibration하고, calibration된 probability를 기반으로 pretraining data인지 판별하는 점수를 계산한다.</li> </ul> </li> <li>Binary decision <ul> <li>Score에 threshold를 적용하여 \(x\)가 모델 \(\mathcal{M}\)의 pretraining corpus에 있는지 예측한다.</li> </ul> </li> </ol> <h2 id="32-token-probability-distribution-computation">3.2 Token Probability Distribution Computation</h2> <ul> <li>\(x_0\): start-of-sentence token</li> </ul> \[x^\prime=x_0x_1x_2...x_n\] <ul> <li>\(\mathcal{M}\)에 \(x\)를 query한다.</li> </ul> \[\{p(x_i|x_{\lt i};\mathcal{M}):0\lt i \le n \}\] <h2 id="33-frequency-of-a-word-within-the-collection">3.3 Frequency of a word within the collection</h2> <ul> <li>다음의 term으로 계산한다.</li> </ul> \[p(x_i,\mathcal{D}^\prime)=\frac{\text{count}(x_i)}{N^\prime}\] <ul> <li> <table> <tbody> <tr> <td>\(x_i\)가 없는 경우를 위해 Laplace smoothing을 추가한다. $$</td> <td>V</td> <td>$$는 vocabulary size다.</td> </tr> </tbody> </table> </li> </ul> \[p(x_i;D^\prime)=\frac{\text{count}(x_i)+1}{N^\prime+|V|}\] <h2 id="34-score-calculation-through-compression">3.4 Score Calculation through Compression</h2> <ul> <li>\(p(x_i;\mathcal{M})\)과 \(p(x_i;D^\prime)\)의 cross-entropy를 계산한다.</li> </ul> \[\alpha_i = -p(x_i; \mathcal{M}) \cdot \log p(x_i; D^\prime).\] <ul> <li>특정 token이 우세한 영향을 미치지 않도록 upper bound를 정의한다.</li> </ul> \[\begin{equation} \alpha_i = \begin{cases} \alpha_i, &amp; \text{if } \alpha_i &lt; a \\ a, &amp; \text{if } \alpha_i \geq a \end{cases} \end{equation}\] <ul> <li>Text \(x\)에 대해서 token \(x_i\)가 여러 개 존재할 수 있다. <ul> <li>이럴 때는 첫 번째 토큰의 결과를 가져온다.</li> </ul> </li> </ul> \[\beta=\frac{1}{|\text{FOS}(x)|}\sum_{x_j \in \text{FOS(x)}}\alpha_j\] <p align="center"><img src="/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%201.png" width="80%"></p> <h2 id="35-binary-decision">3.5 Binary Decision</h2> <ul> <li>Threshold로 pretraining corpus \(D\)에 있는지 결정</li> </ul> \[\text{Decision}(x, \mathcal{M}) = \begin{cases} 0 \quad (x \notin \mathcal{D}), &amp; \text{if } \beta &lt; \tau, \\ 1 \quad (x \in \mathcal{D}), &amp; \text{if } \beta \geq \tau. \end{cases}\] <p align="center"><img src="/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%202.png" width="80%"></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/byol/">Bootstrap your own latent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/llm-as-a-meta-judge/">META-REWARDING LANGUAGE MODELS: Self-Improving Alignment with LLM-as-a-Meta-Judge 설명</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/multimodal-vs-unimodal/">What Makes Multi-modal Learning Better than Single (Provably)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/tinyvit/">TinyViT</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/convolutional-character-network/">Convolutional Character Network</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"wonbeomjang/wonbeomjang.github.io","data-repo-id":"R_kgDOIsrE7Q","data-category":"Comments","data-category-id":"DIC_kwDOIsrE7c4CaZnD","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"ko",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Wonbeom Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 19, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>