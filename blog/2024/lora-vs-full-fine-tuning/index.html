<!DOCTYPE html> <html lang="kr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LoRA vs Full Fine-tuning: An Illusion of Equivalence | Wonbeom Jang </title> <meta name="author" content="Wonbeom Jang"> <meta name="description" content="개발을 좋아하는 딥러닝 리서쳐 장원범입니다. "> <meta name="keywords" content="computer-vision, machine-learning, software-engineer, software-engineering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.wonbeomjang.kr/blog/2024/lora-vs-full-fine-tuning/"> </head> <body class="fixed-top-nav "> <header> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1352545434285249" crossorigin="anonymous"></script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wonbeom </span> Jang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">LoRA vs Full Fine-tuning: An Illusion of Equivalence</h1> <p class="post-meta"> December 28, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/paper"> <i class="fa-solid fa-hashtag fa-sm"></i> paper</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>     ·   <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> paper</a>   <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> llm</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>Pre-trained 모델을 downstream task에 finetuning하는 것은 computation-, data-efficient한 방법이다. 하지만 full-finetuning은 시간과 비용적으로 부담이 크다. 이를 해결하기 위해 LoRA와 같은 PEFT 방법이 제시되었다. 그러나 LoRA로 full-finetuning과 동일한 성능을 내도록 학습했을 때, 두 방법이 실제로 동일하게 작동하는지는 명확하지 않다. 저자는 이러한 의문을 실험을 통해 분석하며 다음과 같은 결론을 얻었다.</p> <ol> <li>LoRA는 intruder dimensions을 도입하며 full-finetuning과 구조적으로 다른 학습을 진행한다.</li> <li>LoRA로 fine-tuned된 모델은 intruder dimensions을 만들며 pre-training distribution에서 더 많이 벗어나고, continual pre-training에서도 덜 robust하다.</li> <li>Low-rank LoRA가 target task에서 잘 작동하더라도 higher-rank parameterization이 더 좋은 결과를 낸다.</li> </ol> <h1 id="model-differences-between-lora-and-full-fine-tuning">Model Differences Between LoRA and Full Fine-Tuning</h1> <p>저자는 Sharma et al. (2024)의 Singular Value Decomposition (SVD)을 활용한 pruning에서 영감을 얻었다. 이를 통해 LoRA fine-tuned 모델과 full fine-tuned 모델의 weight matrices의 singular vector와 pre-trained weight의 singular vector의 cosine similarity를 비교했다.</p> <p align="center"><img src="/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image.png" width="80%"></p> <p>Fig. 2(b)에서 볼 수 있듯 LoRA와 full finetuning의 singular vector는 다른 양상을 보인다. LoRA fine-tuned singular vector는 full fine-tuned singular vector에 비해 pre-trained singular vector와 cosine similarity가 낮았다. 그리고 LoRA rank가 높을수록 singular vector가 pre-trained singular vector의 cosine similarity가 높았다.</p> <p>저자는 이렇게 cosine similarity가 낮은 singular vector를 intruder dimension이라고 명명한다.</p> \[\text{Definition 1: A singular vector } y_i \text{ from the fine-tuned weight matrix } W_{\text{tuned}} \text{ is an intruder dimension if and only if } \text{max}_i(\cos(y_j,x_i)) &lt; \epsilon, \text{ where } \epsilon \text{ is a similarity threshold and } x_i \text{ is a singular vector in } W_0.\] <p>Full fine-tuning에서는 pre-trained singular vector와 높은 cosine similarity를 가지는 singular vector가 비슷한 singular value를 가지고 있다. 이는 full fine-tuning이 pre-trained singular vector와 singular value를 활용해 small update를 진행한다는 것을 보여준다. 반면, LoRA는 새로운 singular vector를 도입해 large norm으로 update를 한다.</p> <p align="center"><img src="/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%201.png" width="80%"></p> <p>Fig. 3에서 empty column은 intruder dimensions을 나타내며, 이는 full fine-tuning과의 차이를 보여준다.</p> <h2 id="setup">Setup</h2> <p>저자는 RoBERTa-base로 실험을 진행했다.</p> <h3 id="1-lora-finetuned-model은-high-ranking-intruder-dimensions을-가지지만-fully-fine-tuned-model은-그렇지-않다">1. LoRA finetuned model은 high-ranking intruder dimensions을 가지지만 fully fine-tuned model은 그렇지 않다.</h3> <p align="center"><img src="/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%202.png" width="80%"></p> <p>위 알고리즘에 따라, top-k highest ranking singular vector에 대해 모든 pre-trained singular vector와 maximum cosine similarity를 측정했을 때 threshold \(\epsilon\) 이하면 intruder dimension으로 분류한다. 저자는 LoRA로 학습한 모델들이 작은 \(\epsilon\) 에 대해 \(r \leq 16\)일 때 지속적으로 intruder dimension을 가진다는 것을 확인했다. 또한 rank가 올라갈수록 intruder dimension이 줄어드는 것을 확인했다.</p> <h3 id="2-lora-fine-tuned-model이-full-fine-tuned-model보다-학습량이-적은-작업에서도-intruder-dimensions이-존재한다">2. LoRA fine-tuned model이 full fine-tuned model보다 학습량이 적은 작업에서도 intruder dimensions이 존재한다.</h3> <p align="center"><img src="/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%203.png" width="80%"></p> <p>수학, 코딩과 같은 학습량이 적은 데이터로 fine-tuning할 경우에도 intruder dimensions이 발생한다. Magicoder 모델과 같은 code model에서도 intruder dimension이 나타나는데, 이는 pre-training domain과 code domain의 차이에서 비롯된 것으로 판단된다. 이 경우에도 LoRA fine-tuned model이 intruder dimensions을 더 많이 가진다.</p> <h3 id="3-full-fine-tuning-updates는-lora-update보다-higher-effective-rank를-가진다-full-rank일지라도">3. Full fine-tuning updates는 LoRA update보다 higher effective rank를 가진다. (Full rank일지라도)</h3> <p align="center"><img src="/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%204.png" width="80%"></p> <p>Full fine-tuning은 LoRA tuning보다 더 높은 effective rank를 가진다. 예를 들어 \(r=768\)인 RoBERTa도 평균적으로 effective rank를 300으로 업데이트한다. 이는 LoRA가 full capacity \(r\)을 사용하지 못하고 업데이트를 진행한다는 것을 의미한다. 따라서 LoRA와 full fine-tuning의 차이는 coding과 같은 어려운 작업에서 더 두드러진다.</p> <h3 id="4-intruder-dimension은-high-and-low-singular-values-모두에-존재한다">4. Intruder dimension은 high and low singular values 모두에 존재한다.</h3> <p align="center"><img src="/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%205.png" width="80%"></p> <p>Fig. 11a에서 볼 수 있듯, singular value의 비율과 관계없이 full fine-tuning보다 항상 더 많은 intruder dimensions을 가진다.</p> <h3 id="5-scaling-alpha를-lora의-rank에-따라-조절하면-intruder-dimensions이-줄어들고-effective-rank이-늘어난다">5. Scaling \(\alpha\)를 LoRA의 rank에 따라 조절하면 intruder dimensions이 줄어들고 effective rank이 늘어난다.</h3> <p>많은 논문에서 \(\alpha=2r\)로 설정하여 학습을 진행한다. 저자는 \(\alpha\)의 영향을 확인하기 위해 \(\alpha=2r\)과 \(\alpha=8\)로 설정해 비교 실험을 진행했다. 고정된 \(\alpha\) 값에서는 모든 rank에서 LoRA가 intruder dimensions을 보였으며, \(\alpha=2r\)과 비교했을 때 훨씬 적은 effective rank를 가졌다.</p> <h3 id="6-intruder-dimensions의-수는-fine-tuning-dataset의-크기에-비례하여-늘어난다">6. Intruder dimensions의 수는 fine-tuning dataset의 크기에 비례하여 늘어난다.</h3> <p align="center"><img src="/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%206.png" width="80%"></p> <p>Fig. 12에서 \(r=8\)인 경우 하나 이상의 데이터셋을 학습시킬 때 intruder dimensions이 추가로 발생한다. 반면 \(r=1\)인 경우 intruder dimensions 수가 비슷한데, 이는 \(r=1\)일 때 모델의 표현력 한계 때문으로 추정된다.</p> <h3 id="conjecture-intruder-dimensions은-norm과-stability에-큰-영향을-끼친다">Conjecture: Intruder dimensions은 norm과 stability에 큰 영향을 끼친다.</h3> <p>Pre-trained singular vector와 다르게, LoRA는 intruder dimensions을 추가하면서 smaller dataset에 fine-tuning하므로 pre-trained vectors보다 큰 영향을 미친다. 반면 full fine-tuning은 pre-trained 모델의 spectral property를 유지하며 효과적으로 적응한다. 이를 통해 LoRA 모델은 fine-tuning task 이외의 분야에서 부정적 영향을 미치고, full fine-tuning 모델은 이러한 악영향이 적음을 확인할 수 있다.</p> <h1 id="behavioral-differences-between-lora-and-full-fine-tuning">Behavioral Differences Between LoRA and Full Fine-Tuning</h1> <h3 id="1-lower-rank에서-lora는-continual-learning에-robust하지-않고-이전-작업을-더-많이-잊어버린다">1. Lower rank에서 LoRA는 continual learning에 robust하지 않고 이전 작업을 더 많이 잊어버린다.</h3> <p align="center"><img src="/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%207.png" width="80%"></p> <p>Fig. 8에서 볼 수 있듯 LoRA는 target task에 대해 full fine-tuning과 비슷한 성능을 내지만 rank가 작을 경우 continual pre-training 성능이 낮다. Rank를 올리면 forgetting이 줄어들며, full fine-tuning이 가장 낮은 forgetting 비율을 가진다.</p> <h3 id="2-같은-test-accuracy로-fine-tuning했을-때-pre-training-pseudo-loss가-u-shaped-curve를-그리는-것을-확인할-수-있다">2. 같은 test accuracy로 fine-tuning했을 때 pre-training pseudo loss가 U-shaped curve를 그리는 것을 확인할 수 있다.</h3> <p align="center"><img src="/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%208.png" width="80%"></p> <p>이는 downstream task에 대해 optimal한 rank가 존재한다는 것을 보여준다. Rank가 낮을 경우 intruder dimensions의 영향으로 forgetting이 발생하며, rank가 높을 경우 overparameterization으로 인해 target task에 대해 overfitting이 일어난다.</p> <h3 id="3-alpha를-적절하게-설정하면-model-performance에-긍정적인-영향을-미친다">3. \(\alpha\)를 적절하게 설정하면 model performance에 긍정적인 영향을 미친다.</h3> <p>LoRA는 rank와 관계없이 forgetting 현상이 발생한다. 그러나 \(\alpha=8\)이 \(\alpha=2r\)보다 intruder dimensions이 더 많음에도 불구하고 높은 \(\alpha\)는 continual pre-training에서 성능 향상에 기여한다.</p> <h1 id="왜-intruder-dimensions이-존재할까">왜 Intruder Dimensions이 존재할까?</h1> <h3 id="1-pre-trained-matrix에-random-vector를-더하면-intruder-dimensions이-생긴다">1. Pre-trained matrix에 random vector를 더하면 intruder dimensions이 생긴다.</h3> <p>이를 확인하기 위해 pre-trained weights \(W \in \mathbb{R}^{n \times n}\), randomly sampled vector \(v \in \mathbb{R}^n\), \(W\)의 singular value보다 큰 \(\lambda\)에 대해 \(\text{SVD}(W + \lambda vv^{T})\)와 \(\text{SVD}(W)\)를 비교했다. 이를 통해 random vector 추가가 intruder dimensions 생성에 영향을 미친다는 것을 확인했다.</p> <h3 id="2-update-rule에서의-차이점">2. Update rule에서의 차이점</h3> <p align="center"><img src="/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%209.png" width="80%"></p> <p>LoRA는 더 큰 learning rate를 사용하며 low-rank space에서 gradient projection을 진행한다. 이러한 방식은 full fine-tuning과 차이를 보이며 intruder dimensions 생성에 영향을 준다.</p> <h3 id="3-product-parameterization-of-lora">3. Product parameterization of LoRA</h3> <p>Matrices 곱은 spectral differences를 증가시키며 lower effective rank를 초래한다.</p> <p align="center"><img src="/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%2010.png" width="80%"></p> <p>따라서 LoRA adaptor에서 B만 학습시키는 것이 intruder dimensions이 더 적다는 것을 알 수 있다.</p> <h1 id="conclusion">Conclusion</h1> <p>저자는 LoRA와 full fine-tuning이 weight metrics의 spectral properties에서 큰 차이가 있음을 확인했다. LoRA 모델은 intruder dimensions을 자주 가지며, pre-trained singular vectors와 거의 orthogonal하다. 반면, full fine-tuning은 pre-trained spectral properties를 유지하며 효과적으로 적응한다. LoRA는 일부 task에서는 효과적일 수 있지만, pre-training domain과 다른 domain에 대해 더 많은 한계를 가지며, 이러한 한계는 continual learning에서도 명확히 드러난다.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/byol/">Bootstrap your own latent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/tinyvit/">TinyViT</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/integral-neural-network/">Integral Neural Network</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EA%B2%BD%EB%9F%89%ED%99%94-EffientNet/">[네트워크 경량화] EfficientNet</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/meta-pseudo-label/">Meta Pseudo Labels</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"wonbeomjang/wonbeomjang.github.io","data-repo-id":"R_kgDOIsrE7Q","data-category":"Comments","data-category-id":"DIC_kwDOIsrE7c4CaZnD","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"ko",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Wonbeom Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 30, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-NH0GKRG1BP"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-NH0GKRG1BP");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>