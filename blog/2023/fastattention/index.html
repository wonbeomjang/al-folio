<!DOCTYPE html> <html lang="kr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness | Wonbeom Jang </title> <meta name="author" content="Wonbeom Jang"> <meta name="description" content="optimize transformer on gpu device"> <meta name="keywords" content="computer-vision, machine-learning, software-engineer, software-engineering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.wonbeomjang.kr/blog/2023/fastattention/"> </head> <body class="fixed-top-nav "> <header> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1352545434285249" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-NH0GKRG1BP"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-NH0GKRG1BP");</script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">WonbeomÂ </span> Jang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</h1> <p class="post-meta"> March 28, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a> Â  Â· Â  <a href="/blog/category/transformer"> <i class="fa-solid fa-tag fa-sm"></i> transformer</a> Â  <a href="/blog/category/hardware-optimization"> <i class="fa-solid fa-tag fa-sm"></i> hardware-optimization</a> Â  <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> paper</a> Â  </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>í˜„ì¬ NLPì™€ Vision ë¶„ì•¼ì—ì„œ transformerëŠ” í™œë°œíˆ ì‚¬ìš©ë˜ê³  ìˆë‹¤.<br> í•˜ì§€ë§Œ transformerëŠ” ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì¡ì•„ë¨¹ëŠ” ëª¨ë“ˆì´ì—ˆê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ sparse-approximation, low-rank approximation ë“±ì„ ì œì•ˆí–ˆë‹¤.<br> í•˜ì§€ë§Œ ì´ë“¤ì€ ì´ë¡ ê³¼ ë‹¬ë¦¬ computational speedë¥¼ ì¦ê°€ì‹œì¼œì£¼ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ë‹¤.<br> ì €ìëŠ” GPUì—ì„œ ë¹ ë¥¸ SRAMìœ¼ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” IO-aware ì•Œê³ ë¦¬ì¦˜ì„ ì œì‹œí–ˆë‹¤.</p> <h2 id="hardware-performance">Hardware performance</h2> <h3 id="gpu-memory-hierarchy">GPU Memory Hierarchy</h3> <p align="center"> <img src="/assets/post/image/legacy/fastattention-gpu-hierchy.png" width="50%"> </p> <p>GPUëŠ” CPUì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë©”ëª¨ë¦¬ ê³„ì¸µì„ ê°€ì§„ë‹¤. DRAMì´ ê°€ì¥ ëŠë¦¬ê³  ìš©ëŸ‰ì´ í¬ë©°, SRAMì´ ê°€ì¥ ë¹ ë¥´ê³  ìš©ëŸ‰ì´ ì‘ë‹¤.<br> GPUëŠ” ë³‘ë ¬ ì—°ì‚° ì‹œ ë°ì´í„°ë¥¼ HBMì—ì„œ ê°€ì ¸ì˜¨ í›„ SRAMì— ì˜¬ë ¤ë†“ê³  ì—°ì‚°ì„ í•œë‹¤. ì´í›„ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ì½ì–´ë“¤ì´ë©´ SRAMì— ìˆëŠ” ì •ë³´ëŠ” ë‹¤ì‹œ HBMì— ì €ì¥ëœë‹¤.</p> <h3 id="performance-characteristics">Performance characteristics</h3> <p>í¼í¬ë¨¼ìŠ¤ë¥¼ ê³ ë ¤í•  ë•Œ ì—°ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì ‘ê·¼ì˜ ê´€ì ìœ¼ë¡œ ë‘ ê°€ì§€ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.</p> <ol> <li>Compute-bound: ì—°ì‚°ëŸ‰ì´ ë©”ëª¨ë¦¬ ì ‘ê·¼ë³´ë‹¤ ë§ì€ ê²½ìš°ì´ë‹¤. ex) MatMul</li> <li>Memory-bound: ë©”ëª¨ë¦¬ ì ‘ê·¼ì´ ì—°ì‚°ëŸ‰ë³´ë‹¤ ë§ì€ ê²½ìš°ì´ë‹¤. ex) softmax, batchnorm</li> </ol> <h3 id="kernel-fusion">Kernel fusion</h3> <p>Memory-bound ì—°ì‚°ì„ ê°€ì†í•˜ëŠ” ë° ë§ì´ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì€ kernel fusionì´ë‹¤.<br> ë§Œì•½ ê°™ì€ inputì— ëŒ€í•´ ì—¬ëŸ¬ ì—°ì‚°ì„ í•œë‹¤ê³  í•˜ë©´, ì»´íŒŒì¼ëŸ¬ëŠ” ìë™ìœ¼ë¡œ ë§ì€ elementwise operationì„ fusioní•œë‹¤.</p> <h2 id="standard-attention-implementation">Standard Attention Implementation</h2> <p>Sequence length \(N\)ê³¼ head dimension \(d\)ì— ëŒ€í•˜ì—¬ attentionì€ input sequence \(Q,K,V \in \mathbb{R}^{N \times d}\)ë¥¼ ì´ìš©í•˜ì—¬<br> \(O \in \mathbb{R}^{M \times d}\)ë¥¼ êµ¬í•œë‹¤. ê·¸ì— ëŒ€í•œ ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p> \[S=QK^\top \in \mathbb{R}^{N \times N}, P=softmax(S) \in \mathbb{R}^{N \times N}, O = PV \in \mathbb{R}^{N \times d}\] <p>ì´ë•Œ softmaxëŠ” row-wise operationì´ë‹¤. ë³´í†µì˜ attentionì€ \(O(N^2)\)ì˜ memory costë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ëŒ€ë‹¤ìˆ˜ì˜ ê²½ìš°ì—ëŠ” \(N \gg d\)ë¥¼ ë§Œì¡±í•œë‹¤(GPT-2, N=1024 and d=64).</p> <p align="center"> <img src="/assets/post/image/legacy/standard-attention-algorithm.png" width="80%"> </p> <h1 id="flashattention">FlashAttention</h1> <p>FlashAttentionì€ <strong>Tiling</strong>ê³¼ <strong>Recomputation</strong>ì„ ì‚¬ìš©í•˜ì—¬ Attentionì„ ê°€ì†í™”í•œë‹¤.</p> <h3 id="tiling">Tiling</h3> <p align="center"> <img src="/assets/post/image/legacy/fastattention-tiling.png" width="50%"> </p> <p>ê¸°ì¡´ì˜ softmax ì—°ì‚°ì€ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ê±°ì¹œë‹¤.</p> \[m(x):=\underset{i}{max}(x_i), f(x):=[e^{x_1-m(x)} ... e^{x_B-m(x)}],\] \[l(x):=\sum_i f(x)_i, softmax(x):= \frac{f(x)}{l(x)}\] <p>vector \(x^{(1)}, x^{(2)} \in \mathbb{R}^B\)ì¼ ë•Œ vectorì˜ concatenation \(x=[x^{(1)} x^{(2)}]\)ì— ëŒ€í•´ softmaxëŠ” ë‹¤ìŒê³¼ ê°™ì´ decompositioní•  ìˆ˜ ìˆë‹¤.</p> \[m(x)=m([x^{(1)} x{(2)}])=max(m(x^{(1)})),m(x^{(2)}),\] \[f(x):=[e^{m(x^{(1)})-m(x)}f(x^{(1)}) ... e^{x^{(2)}-m(x)}f(x^{(2)})],\] \[l(x)=l([x^{(1)} x{(2)}])=e^{m(x^{(1)})-m(x)}l(x^{(1)}) + e^{x^{(2)}-m(x)}l(x^{(2)}),\] \[softmax(x):= \frac{f(x)}{l(x)}\] <p>ì¦‰, softmaxë¥¼ block ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.</p> <h3 id="recomputation">Recomputation</h3> <p>ì €ìëŠ” backward ë•Œ \(O(N^2)\)ì˜ memoryë¥¼ ì €ì¥í•˜ì§€ ì•Šê¸° ìœ„í•´ softmax normalization statistics \((m,l)\)ì„ ì €ì¥í•œ í›„ backward ë•Œ ë‹¤ì‹œ êµ¬ì„±í•œë‹¤.<br> ì´ë¡œ ì¸í•´ FLOPsëŠ” ì¦ê°€í•˜ì§€ë§Œ HBMì—ì„œ ë°ì´í„°ë¥¼ ì½ëŠ” íšŸìˆ˜ê°€ ì¤„ì–´ë“¤ì–´ ì†ë„ê°€ í–¥ìƒëœë‹¤.</p> <h3 id="kernel-fusion-1">Kernel Fusion</h3> <p align="center"> <img src="/assets/post/image/legacy/fastattention-kernel-fusion.png" width="50%"> </p> <p>Tilingì„ í†µí•´ í•œ ë²ˆì˜ HBM loadì—ì„œ matrix multiply, softmax, optionally masking and dropout, matrix multiplyë¥¼ í•œ í›„ HBMì— ì €ì¥í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.<br> ì´ëŠ” ë°˜ë³µì ì¸ IO operationì„ ì¤„ì—¬ì¤€ë‹¤.</p> <p align="center"> <img src="/assets/post/image/legacy/fastattention-algorithm.png" width="80%"> </p> <blockquote> <p><strong>Theorem 1</strong>. Algorithm 1 returns \(O=softmax(QL^\top)V\) with \(O(N^2d)\) FLOPs and requires additional memory beyond inputs and output.</p> </blockquote> <h2 id="analysis-io-complexity-of-flashattention">Analysis: IO Complexity of FlashAttention</h2> <p align="center"> <img src="/assets/post/image/legacy/fastattention-coomplexity.png" width="80%"> </p> <p>FlashAttentionì€ standardë³´ë‹¤ GFLOPsëŠ” ë§ì§€ë§Œ, HBM read and writeê°€ ì ì–´ runtimeì´ ê°œì„ ë˜ì—ˆë‹¤.</p> <blockquote> <p><strong>Theorem 2.</strong> Let \(N\) be the sequence length, \(d\) be the head dimension, and \(M\) be the size of SRAM with \(d \leq M \leq Nd\). Standard attention (Algorithm 0) requires \(\Theta(Nd+N^2)\) HBM accesses, while FlashAttention (Algorithm 1) requires \(\Theta(N^2d^2M^{-1})\) HBM accesses.</p> </blockquote> <blockquote> <p><strong>Proposition 3.</strong> Let \(N\) be the sequence length, \(d\) be the head dimension, and \(M\) be the size of SRAM with \(d \leq M \leq Nd\). There does not exist an algorithm to compute exact attention with \(\Theta(N^2d^2M^{-1})\) HBM accesses for all \(M\) in the range \([d, Nd]\).</p> </blockquote> <h1 id="extension">Extension</h1> <p>Block-sparse attentionì„ ì‘ìš©í•˜ì—¬ block-sparse flashattentionì„ ë§Œë“¤ê¸°ë„ í–ˆë‹¤.</p> \[S=QK^\top \in \mathbb{R}^{N \times N}, P=softmax(S \odot \mathbb{1}_{\tilde{M}}) \in \mathbb{R}^{N \times N}, O=PV \in \mathbb{R}^{N \times d}\] <blockquote> <p><strong>Proposition 4.</strong> Let \(N\) be the sequence length, \(d\) be the head dimension, and \(M\) be the size of SRAM with \(d \leq M \leq Nd\). Block-sparse FlashAttention (Algorithm 5) requires \(\Theta(N^2d^2M^{-1})\) HBM accesses where ğ‘  is the fraction of nonzero blocks in the block-sparsity mask.</p> </blockquote> <h2 id="experiment">Experiment</h2> <p>FlashAttentionì€ tilingì„ í†µí•´ ì†ë„ê°€ ë¹ ë¥´ê³ , recomputationì„ í†µí•´ ë©”ëª¨ë¦¬ê°€ ì¤„ì–´ë“¤ì—ˆë‹¤.<br> ì´ë¥¼ ì´ìš©í•˜ì—¬ sequence lengthë¥¼ ëŠ˜ë¦´ ìˆ˜ ìˆì—ˆê³ , ì´ëŠ” ì¶”ê°€ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì™”ë‹¤.</p> <h3 id="bert">BERT</h3> <p align="center"> <img src="/assets/post/image/legacy/fastattention-bert-performance.png" width="80%"> </p> <p>BERT í•™ìŠµ ì‹œ MLPerf 1.1 ê¸°ì¤€ í•™ìŠµ ì‹œê°„ì´ 15% ê°œì„ ë˜ì—ˆë‹¤.</p> <h3 id="gpt-2">GPT-2</h3> <p>GPT-2ëŠ” Huggingface, Megatron-LMê³¼ ë¹„êµí–ˆëŠ”ë° ê°ê° 3ë°°, 1.7ë°°ì˜ speed upì´ ë°œìƒí–ˆë‹¤.</p> <p align="center"> <img src="/assets/post/image/legacy/fastattention-gpt-2-performace.png" width="80%"> </p> <h3 id="long-range-arena">Long-range Arena</h3> <p>LRAì—ì„œë„ ê¸°ì¡´ ëŒ€ë¹„ 2.4ë°°ì˜ speed upì„ ë³´ì˜€ìœ¼ë©°, ë‹¤ë¥¸ attention methodë³´ë‹¤ ì„±ëŠ¥ë„ ì¢‹ì•˜ë‹¤.</p> <p align="center"> <img src="/assets/post/image/legacy/fastattention-long-reange-arena-performace.png" width="80%"> </p> <h2 id="better-models-with-longer-sequences">Better Models with Longer Sequences</h2> <h3 id="language-modeling-with-long-context">Language Modeling with Long Context.</h3> <p>Recomputingìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì¤„ì–´ë“¤ë©´ì„œ ë” ê¸´ input sequenceë¥¼ ë‹¤ë£° ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. ì´ë¥¼ í†µí•´ ì¶”ê°€ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì™”ë‹¤.</p> <p align="center"> <img src="/assets/post/image/legacy/fastattention-bert-with-long-sequence.png" width="80%"> </p> <h3 id="long-document-classification">Long Document Classification</h3> <p align="center"> <img src="/assets/post/image/legacy/fastattention-long-document-classification.png" width="80%"> </p> <h3 id="path-x-and-path-256">Path-X and Path-256</h3> <p align="center"> <img src="/assets/post/image/legacy/fastattention-path-x.png" width="80%"> </p> <p>Path-Xì™€ Path-256ì€ long contextë¡œ ê¸°ì¡´ì˜ ëª¨ë¸ë“¤ì€ randomí•œ ê²°ê³¼ì™€ ë¹„ìŠ·í•˜ê²Œ ë‚˜ì™”ë‹¤.<br> FlashAttentionì€ í•´ë‹¹ ë°ì´í„°ì…‹ì— random ì´ìƒì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¨ ì²« ë²ˆì§¸ ëª¨ë¸ì´ë‹¤.</p> <h2 id="benchmarking-attention">Benchmarking Attention</h2> <p align="center"> <img src="/assets/post/image/legacy/fastattention-banchmarking.png" width="80%"> </p> <p>Attention ê³„ì—´(Attention, FlashAttention)ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ \(O(N^2)\)ì´ì§€ë§Œ, approximate attention(sparse attention)ì€ \(O(n)\)ì´ë‹¤.<br> ë”°ë¼ì„œ sequence lengthë¥¼ í‚¤ìš°ë‹¤ ë³´ë©´ approximate attentionì´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ, ì„±ëŠ¥ì—ì„œëŠ” ìš°ìˆ˜í•˜ë‹¤.</p> <h1 id="limitation">Limitation</h1> <p>FlashAttentionì€ CUDA kernelì„ ì‚¬ìš©í•´ì•¼ í•˜ë¯€ë¡œ ì—”ì§€ë‹ˆì–´ë§ì´ í•„ìš”í•˜ë‹¤.<br> ê·¸ë¦¬ê³  GPUë§ˆë‹¤ ì»´íŒŒì¼ì´ í•„ìš”í•˜ë©° í™•ì¥ì„±ì— ë¬¸ì œê°€ ìˆë‹¤.<br> ë˜í•œ í˜„ì¬ëŠ” single GPUë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§Œë“¤ì–´ì¡Œìœ¼ë¯€ë¡œ, multi-GPUë¥¼ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ë„ ì œì‘í•´ì•¼ í•œë‹¤.</p> <hr> <p><strong>ì¶”ê°€ê¸€â€¦</strong><br> ì•„ì•—â€¦ í¬ìŠ¤íŠ¸ë¥¼ ì˜¬ë¦¬ê³  4ê°œì›” ë§Œì— URLì´ í‹€ë ¸ë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ë‹¤â€¦ í•˜ì§€ë§Œ ì–´ì©” ìˆ˜ ì—†ë‹¤â€¦ ê·¸ëƒ¥ ê°„ë‹¤â€¦<br> fastattentiondl ì•„ë‹ˆë¼ flashattentionìœ¼ë¡œ í•´ì•¼ í•˜ëŠ”ë°â€¦.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/flashattention-2/">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/mobile-vit/">MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/tinyvit/">TinyViT</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/multimodal-vs-unimodal/">What Makes Multi-modal Learning Better than Single (Provably)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/mobileone/">MobileOne: An Improved One millisecond Mobile Backbone</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"wonbeomjang/wonbeomjang.github.io","data-repo-id":"R_kgDOIsrE7Q","data-category":"Comments","data-category-id":"DIC_kwDOIsrE7c4CaZnD","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"ko",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2024 Wonbeom Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 20, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>