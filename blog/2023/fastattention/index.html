<!DOCTYPE html> <html lang="kr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1352545434285249" crossorigin="anonymous"></script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness | Wonbeom Jang</title> <meta name="author" content="Wonbeom Jang"> <meta name="description" content="optimize transformer on gpu device"> <meta name="keywords" content="computer-vision, machine-learning, software-engineer, software-engineering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.wonbeomjang.kr/blog/2023/fastattention/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Wonbeom </span>Jang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</h1> <p class="post-meta">March 28, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/transformer"> <i class="fas fa-tag fa-sm"></i> transformer</a>   <a href="/blog/category/hardware-optimization"> <i class="fas fa-tag fa-sm"></i> hardware-optimization</a>   <a href="/blog/category/paper"> <i class="fas fa-tag fa-sm"></i> paper</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>현재 NLP와 Vision분야에서 transformer는 활발히 사용되고 있다. 하지만 transformer는 메로리를 많이 잡아먹는 모듈이였고 이를 해결하기 위해 sparse-approximation, low-rank approximation등을 제안하였다. 하지만 이들은 이론과 달리 computational speed를 증가시켜주지 못하는 경우가 많았다. 저자는 GPU에서 빠른 SRAM으로 연산을 수행할 수 있는 IO-aware algorithm을 제시하였다.</p> <h2 id="hardware-performace">Hardware performace</h2> <h3 id="gpu-memory-hierchy">GPU Memory Hierchy</h3> <p align="center"> <img src="/assets/post/image/legacy/fastattention-gpu-hierchy.png" width="50%"> </p> <p>GPU는 CPU와 마찬가지로 메모리 계층을 가진다. DRAM이 가장 느리고 용량이 크며 SRAM이 가장 빠르고 용량이 작다. GPU는 병렬연산시 데이터를 HBM에서 가져온 후 SRAM에 올려놓고 연산을 한다. 이후 다른 데이터를 읽어드리면 SRAM에 있는 정보는 다시 HBM에 저장된다.</p> <h3 id="performance-characteristics">Performance characteristics</h3> <p>퍼포먼스를 고려할 때 연산량과 메모리접근의 관점으로 두 가지를 나눌 수 있다.</p> <ol> <li>Compute-bound: 연산량이 메모리접근보다 많은 경우이다. ex) MatMul</li> <li>Memory-bound: 메모리접근이 연산량보다 많은 경우이다. ex) softmax, batchnorm</li> </ol> <h3 id="kernel-fusion">Kernel fusion</h3> <p>Memory-bound연산을 가속시키는데 많이 사용되는 방법은 kernel fusion이다. 만약 같은 input에 대해 여러 연산을 한다고하면 컴파일러는 자동적으로 많은 elementwise operation을 fusion한다.</p> <h2 id="standard-attention-implementation">Standard Attention Implementation</h2> <p>Sequnce length \(N\)과 head dimension \(d\)에 대하여 attention는 input sequence \(Q,K,V \in \mathbb{R}^{N \times d}\) 를 이용하여 \(O \in \mathbb{R}^{M \times d}\)를 구한다. 그에대한 식은 다음과 같다.</p> \[S=QK^\top \in \mathbb{R}^{N \times N}, P=softmax(S) \in \mathbb{R}^{N \times N}, O = PV \in \mathbb{R}^{N \times d}\] <p>이 때 softmax는 row-wise operatio이다. 보통의 attention은 \(O(N^2)\)의 memory cost를 사용하는데 대다수의 경우에는 \(N \gg d\)를 만족한다(GPT-2, N=1024 and d=64).</p> <p align="center"> <img src="/assets/post/image/legacy/standard-attention-algorithm.png" width="80%"> </p> <h1 id="flash-attention">Flash Attention</h1> <p>FlashAttention은 <strong>Tiling</strong>과 <strong>Recomputation</strong>을 사용하여 Attention을 가속화한다.</p> <h3 id="tiling">Tiling</h3> <p align="center"> <img src="/assets/post/image/legacy/fastattention-tiling.png" width="50%"> </p> <p>기존의 softmax연산은 다음과 같은 과정을 거친다.</p> \[m(x):=\underset{i}{max}(x_i), f(x):=[e^{x_1-m(x)} ... e^{x_B-m(x)}],\] \[l(x):=\sum_i f(x)_i, softmax(x):= \frac{f(x)}{l(x)}\] <p>vector \(x^{(1)}, x^{(2)} \in \mathbb{R}^B\)일 때 vector의 concatenation \(x=[x^{(1)} x^{(2)}]\) 대해 softmax는 다음과 같이 decomposition을 할 수 있다.</p> \[m(x)=m([x^{(1)} x{(2)}])=max(m(x^{(1)})),m(x^{(2)}),\] \[f(x):=[e^{m(x^{(1)})-m(x)}f(x^{(1)}) ... e^{x^{(2)}-m(x)}f(x^{(2)})],\] \[l(x)=l([x^{(1)} x{(2)}])=e^{m(x^{(1)})-m(x)}l(x^{(1)}) + e^{x^{(2)}-m(x)}l(x^{(2)}),\] \[softmax(x):= \frac{f(x)}{l(x)}\] <p>즉, softmax를 block단위를 쪼개서 계산할 수 있다는 것이다.</p> <h3 id="recomputation">Recomputation</h3> <p>저자는 baward때 \(O(N^2)\)의 memory를 저장하지 않기 위해서 softmax normalization statistics \((m,l)\)을 저장한 수 backward때 다시 구성한다. 이로인해 FLOPs는 증가하지만 HBM에서 데이터를 읽는 회수가 줄어들어 속도가 향상된다.</p> <h3 id="kernel-fusion-1">Kernel Fusion</h3> <p align="center"> <img src="/assets/post/image/legacy/fastattention-kernel-fusion.png" width="50%"> </p> <p>Tiling을 통해 한 번의 HBM load에서 matrix multiply, softmax, optionally masking and dropout, matrix multiply을 한 후 HBM에 저장할 수 있게 되었다. 이는 반복적인 IO operaion을 줄여준다.</p> <p align="center"> <img src="/assets/post/image/legacy/fastattention-algorithm.png" width="80%"> </p> <blockquote> <p><strong>Theorem 1</strong>. Algorithm 1 returns \(O=softmax(QL^\top)V\)with \(O(N^2d)\) FLOPs and requires additional memory beyond inputs and output</p> </blockquote> <h2 id="analysis-io-complexity-of-flashattention">Analysis: IO Complexity of FlashAttention</h2> <p align="center"> <img src="/assets/post/image/legacy/fastattention-coomplexity.png" width="80%"> </p> <p>Flash attention은 standard보다 GFLOPs는 많지만 HBM read and write가 적어 runtime이 개선되었다.</p> <blockquote> <p><strong>Theorem 2.</strong> Let \(N\) be the sequence length, \(d\) be the head dimension, and \(M\) be size of SRAM with \(d \leq M \leq Nd\). Standard attention (Algorithm 0) requires \(\Theta(Nd+N^2)\) HBM accesses, while FlashAttention (Algorithm 1) requires \(\Theta(N^2d^2M^{-1})\) HBM accesses.</p> </blockquote> <blockquote> <p><strong>Proposition 3.</strong> Let \(N\) be the sequence length, \(d\) be the head dimension, and \(M\) be size of SRAM with \(d \leq M \leq Nd\). There does not exist an algorithm to compute exact attention with \(\Theta(N^2d^2M^{-1})\) HBM accesses for all \(M\) in the range \([d, Nd]\).</p> </blockquote> <h1 id="extension">Extension</h1> <p>Block-sparse attention을 응용하여 block-sparse flashattention을 만들기도 했다.</p> \[S=QK^\top \in \mathbb{R}^{N \times N}, P=softmax(S \odot \mathbb{1}_{\tilde{M}}) \in \mathbb{R}^{N \times N}, O=PV \in \mathbb{R}^{N \times d}\] <blockquote> <p><strong>Proposition 4.</strong> Let \(N\) be the sequence length, \(d\) be the head dimension, and \(M\) be size of SRAM with \(d \leq M \leq Nd\). Block-sparse FlashAttention (Algorithm 5) requires \(\Theta(N^2d^2M^{-1})\) HBM accesses where 𝑠 is the fraction of nonzero blocks in the block-sparsity mask.</p> </blockquote> <h2 id="experimant">Experimant</h2> <p>FlashAttention은 tiling을 통해 속도가 빠르고 recomputation을 통해 메모리가 줄어들었다. 이를 이용하여 sequnce length를 늘릴 수 있었고 이는 추가적인 성능향상을 가져왔다.</p> <h3 id="bert">Bert</h3> <p align="center"> <img src="/assets/post/image/legacy/fastattention-bert-performance.png" width="80%"> </p> <p>Bert 학습 시 MLPerf 1.1기준 학습시간이 15% 개선되었다.</p> <h3 id="gpt-2">GPT-2</h3> <p>GPT-2는 Huggingface, Megatron-LM과 비교했는데 각각 3배, 1.7배의 speed up이 생겼다.</p> <p align="center"> <img src="/assets/post/image/legacy/fastattention-gpt-2-performace.png" width="80%"> </p> <h3 id="long-range-arena">Long-range Arena</h3> <p>LRA에서도 기존대비 2.4x speed up을 보였으며 다른 attention method보다 성능도 좋았다.</p> <p align="center"> <img src="/assets/post/image/legacy/fastattention-long-reange-arena-performace.png" width="80%"> </p> <h2 id="better-models-with-longer-sequences">Better Models with Longer Sequences</h2> <h3 id="language-modeling-with-long-context">Language Modeling with Long Context.</h3> <p>Recomputing으로 메모리사용량이 줄어들면서 더 긴 input sequce를 다룰 수 있게 되었다. 이를통해 추가적인 성능향상을 가져왔다.</p> <p align="center"> <img src="/assets/post/image/legacy/fastattention-bert-with-long-sequence.png" width="80%"> </p> <h3 id="long-document-classification">Long Document Classification</h3> <p align="center"> <img src="/assets/post/image/legacy/fastattention-long-document-classification.png" width="80%"> </p> <h3 id="path-x-and-path-256">Path-X and Path-256</h3> <p align="center"> <img src="/assets/post/image/legacy/fastattention-path-x.png" width="80%"> </p> <p>Path-X와 Path-256은 long context로 기존의 모델들은 random한 결과와 비슷하게 나왔다. FlashAttention은 해당 데이터셋에 random 이상의 결과를 가져온 첫 번째 모델이다.</p> <h2 id="benchmarking-attention">Benchmarking Attention</h2> <p align="center"> <img src="/assets/post/image/legacy/fastattention-banchmarking.png" width="80%"> </p> <p>Attention계열(Attention, FlashAttention)은 메모리 사용량이 \(O(N^2)\)이지만 approximate attetion(sparse attention)은 \(O(n)\)이다. 따라서 seqence length를 키우다보면 approximate attention이 더 좋은 결과를 보여주지만 성능에서 우수하다.</p> <h1 id="limitation">Limitation</h1> <p>FlashAttention은 CUDA kernel를 사용해야하므로 엔지니어링이 필요하다. 그리고 GPU마다 컴파일이 필요하는 등 확장성이 문제가 있다. 그리고 현재는 single gpu를 기준으로 만들어져있어서 multi-GPU를 위한 알고리즘도 제작해야한다.</p> <hr> <p><strong>추가글…</strong><br> 아앗… 포스트를 올리고 4개월만에 url이 틀렸다는 것을 깨달았다… 하지만 어쩔 수 없다… 그냥 간다… fastattentiondl 아니라 flashattention으로 해야하는데….</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">읽어볼 거리</h2> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/flashattention-2/">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/mobile-vit/">MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/tinyvit/">TinyViT</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/mobileone/">MobileOne: An Improved One millisecond Mobile Backbone</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/edgevit/">EdgeViT</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"wonbeomjang/wonbeomjang.github.io","data-repo-id":"R_kgDOIsrE7Q","data-category":"Comments","data-category-id":"DIC_kwDOIsrE7c4CaZnD","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"kr",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Wonbeom Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: October 24, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>