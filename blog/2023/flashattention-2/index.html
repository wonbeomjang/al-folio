<!DOCTYPE html> <html lang="kr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning | Wonbeom Jang </title> <meta name="author" content="Wonbeom Jang"> <meta name="description" content="개발을 좋아하는 딥러닝 리서쳐 장원범입니다. "> <meta name="keywords" content="computer-vision, machine-learning, software-engineer, software-engineering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.wonbeomjang.kr/blog/2023/flashattention-2/"> </head> <body class="fixed-top-nav "> <header> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1352545434285249" crossorigin="anonymous"></script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wonbeom </span> Jang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</h1> <p class="post-meta"> August 06, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>   <a href="/blog/tag/hardware-optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> hardware-optimization</a>   <a href="/blog/tag/paper"> <i class="fa-solid fa-hashtag fa-sm"></i> paper</a>     ·   <a href="/blog/category/attention"> <i class="fa-solid fa-tag fa-sm"></i> attention</a>   <a href="/blog/category/hardware-optimization"> <i class="fa-solid fa-tag fa-sm"></i> hardware-optimization</a>   <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> paper</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>GPT부터 시작해서 ViT 등 여러 분야에서 attention layer를 많이 쓰고 있다. 그런데 이 attention layer는 dimension의 제곱에 비례해서 계산 비용이 커서 모델의 병목이 될 수 있다. 그래서 attention layer를 효율적으로 만드는 여러 시도가 있는데, 그 중 하나가 FlashAttention이다. FlashAttention은 tiling과 kernel fusion을 사용해서 기존 attention layer보다 2.4배 더 빠르게 동작한다. 하지만 FlashAttention도 GPU의 이론적 성능에 비해 25~40%밖에 성능을 내지 못한다고 한다.</p> <p>이런 문제를 해결하기 위해 저자는 FlashAttention을 분석하면서 thread block 간 work partitioning이 비효율적이라는 점을 발견했다. 이로 인해 GPU에서 low-occupancy와 불필요한 memory IO가 발생한다고 느꼈다. 그래서 저자는 이를 개선하기 위해 세 가지 방법을 제안했다.</p> <ol> <li>Output을 바꾸지 않고 non-matmul operation의 FLOPS를 줄인다.</li> <li>Single head attention이라도 병렬 처리를 하도록 연산 순서를 바꾼다.</li> <li>Thread block 내에서 warps 간 통신을 줄인다.</li> </ol> <p>저자는 이 세 가지 방법을 통해 기존 FlashAttention보다 2배 빠른 성능을 달성하고, GPU 이론적 성능의 50~73%까지 성능을 끌어올렸다.</p> <h1 id="background">Background</h1> <p>하드웨어 최적화 관련 논문은 익숙하지 않아서 배경 부분을 꼼꼼하게 읽어보자.</p> <h2 id="hardware-characteristics">Hardware characteristics</h2> <h3 id="gpu-performance-characteristics">GPU performance characteristics</h3> <p>GPU는 compute element와 memory hierarchy를 가지고 있다. 예를 들어, Nvidia의 tensor core는 FP16/BF16 같은 저정밀도 연산을 matmul에 최적화하고 있다. 하지만 non-matmul 연산은 최적화가 부족해서 matmul보다 최대 16배 느리다.</p> <p>메모리 계층 구조에 대해 보면, GPU는 기본적으로 high bandwidth memory(HBM)와 on-chip SRAM(공유 메모리)을 갖고 있다. A100 기준으로 40~80GB의 HBM은 1.5~2.0TB/s의 대역폭을 가지며, 108개의 stream multiprocessor는 각각 192KB의 on-chip SRAM을 갖고 있어 19TB/s의 대역폭을 제공한다. L2 캐시도 있지만, 이는 사용자가 컨트롤할 수 없어서 논의에서는 제외한다.</p> <h3 id="execution-model">Execution Model</h3> <p>GPU는 수많은 thread로 구성되며, 이 thread들이 모여 thread block을 구성한다. 각 thread block은 stream multiprocessor(SM)에서 실행된다. thread block 내에서 thread는 warp이라는 단위로 묶이며, 이 warp들은 공유 메모리를 통해 서로 통신한다.</p> <h2 id="standard-attention-implementation">Standard Attention Implementation</h2> <p>기존의 attention은 query, key, value들 간의 연산으로 구성된다. 시퀀스 길이를 N, head dimension을 d라고 하자. Input sequence \(Q, K, V \in \mathbb{R}^{N\times d}\)에 대해 attention output \(O \in \mathbb{R}^{N \times d}\)를 계산하는 방식은 아래와 같다.</p> \[S=QK^{\intercal}\in \mathbb{R}^{N\times N}\] \[P=\text{softmax}(S)\in\mathbb{R}^{N\times N}\] \[O=PV\in \mathbb{R}^{N\times d}\] <p>여기서 softmax는 row-wise로 적용된다. Backward pass는 아래 과정을 거친다.</p> \[dV=P^{\intercal}dO\in\mathbb{R}^{N\times d}\] \[dP=dOV^{\intercal}\in\mathbb{R}^{N\times N}\] \[dS=\text{dsoftmax}(dP)\in\mathbb{R}^{N\times N}\] \[dQ=dSK\in\mathbb{R}^{N\times d}\] \[dK=QdS^\intercal\in\mathbb{R}^{N\times d}\] <p>FlashAttention에 대해 더 자세한 내용은 다른 포스트에서 확인할 수 있다.</p> <h2 id="flashattention">FlashAttention</h2> <p>FlashAttention의 구체적인 내용은 이전에 다뤘던 <a href="https://www.wonbeomjang.kr/blog/2023/fastattention/">FlashAttention 1 포스트</a>에서 참고할 수 있다.</p> <h3 id="forward-pass">Forward pass</h3> <p>FlashAttention은 K와 V를 tiling하여 병렬적으로 계산한 뒤, on-line softmax를 통해 병렬적으로 softmax를 적용한다. 그 후 tiling한 Q를 불러와 on-chip 연산을 한다. 이를 통해 연산을 fusion하고, Q, K, V는 HBM에서 불러와 연산을 마친 후 다시 HBM에 저장한다. 연산 과정은 아래와 같다. 여기서 \(S\)는 \(S=QK^T\)이다.</p> \[m^{(1)}=\text{rowmax}(S^{(1)})\in\mathbb{R}^{B_r}\] \[l^{(1)}=\text{rowsum}(e^{S^{(1)}-m^{(1)}})\in\mathbb{R}^{B_r\times B_c}\] \[\tilde{P}^{(1)}=\text{diag}(l^{(1)})^{-1}e^{S^{(1)}-m^{(1)}}\in\mathbb{R}^{B_r\times B_c}\] \[O^{(1)}=\tilde{P}^{(1)}V^{(1)}=\text{diag}(l^{(1)})^{-1}e^{S^{(1)}-m^{(1)}}V^{(1)}\in\mathbb{R}^{B_r\times d}\] \[m^{(2)}=\text{max}(m^{(1)},\text{rowmax}(S^{(2)}))=m\] \[l^{(2)}=e^{m^{(1)}-m^{(2)}}l^{(1)}+\text{rowsum}(e^{S^{(2)}-m})=\text{rowsum}(e^{S^{(1)}-m})+\text{rowsum}(e^{S^{(2)}-m})=l\] \[\tilde{P}^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m^{(2)}}\] \[O^{(2)}=\text{diag}(l^{(1)}/l^{(2)})^{-1}O^{(1)}+\tilde{P}^{(2)}V^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(1)}-m}V^{(1)}+\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m}V^{(2)}=O\] <p>이 과정에서 vector를 쪼개고 합치는 방식으로 memory IO를 줄여서 속도를 높였다.</p> <p align="center"> <img src="/assets/post/image/flashattention2/fig1.png" width="80%"> </p> <h3 id="backward-pass">Backward Pass</h3> <p>Backward pass에서는 attention 연산 중에 계산된 \(m\)과 \(l\)을 사용해서 다시 연산을 재구성할 수 있다.</p> <h1 id="flashattention-2">FlashAttention-2</h1> <p>FlashAttention-2는 기존 FlashAttention보다 non-matmul FLOPs를 줄인다. 예를 들어, Nvidia의 A100 GPU는 FP16/BF16 matmul 연산에서 이론적으로 312 TFLOPs/s의 성능을 보이지만, non-matmul 연산은 19.5 TFLOPs/s로 훨씬 느리다. 그래서 non-matmul 연산이 전체 연산에서 차지하는 비중이 크면, 이를 최적화하는 것이 중요하다.</p> <h2 id="forward-pass-1">Forward pass</h2> <p>FlashAttention에서는 on-line softmax를 먼저 주목하고, 이를 개선할 수 있는 방법을 제시했다.</p> <h3 id="recalling">Recalling</h3> <p>기존에는 \(\text{diag}(l^{(2)})^{-1}\)를 두 번 rescaling했으나, FlashAttention-2에서는 마지막 결과 \(\tilde{O}^{(last)}\)를 계산하고, 한 번에 \(\text{diag}(l^{(last)})^{-1}\)으로 rescaling을 한다.</p> \[\tilde{O}^{(2)}=\text{diag}(l^{(1)})^{-1}O^{(1)}+e^{S^{(2)}-m^{(2)}}V^{(2)}\] \[O^{(2)}=\tilde{O}^{(2)}\text{diag}(l^{(2)})^{-1}\] <h3 id="memorization">Memorization</h3> <p>Backward pass를 위해 \(m\)과 \(l\)을 저장할 필요 없이 \(L^{(j)}=m^{(j)}+\text{log}(l^{(j)})\)을 저장해도 같은 결과를 얻을 수 있다. 그래서 \(m\)과 \(l\) 대신 \(L\)을 저장한다.</p> <h3 id="result">Result</h3> <p>결과적으로 FlashAttention-2는 다음과 같은 방법으로 attention을 구현한다.</p> \[m^{(1)}=\text{rowmax}(S^{(1)})\in\mathbb{R}^{B_r}\] \[l^{(1)}=\text{rowsum}(e^{S^{(1)}-m^{(1)}})\in\mathbb{R}^{B_r\times B_c}\] \[\tilde{O}^{(1)}=e^{S^{(1)}-m^{(1)}}V^{(1)}\in\mathbb{R}^{B_r\times d}\] \[m^{(2)}=\text{max}(m^{(1)},\text{rowmax}(S^{(2)}))=m\] \[l^{(2)}=e^{m^{(1)}-m^{(2)}}l^{(1)}+\text{rowsum}(e^{S^{(2)}-m})=\text{rowsum}(e^{S^{(1)}-m})+\text{rowsum}(e^{S^{(2)}-m})=l\] \[\tilde{P}^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m^{(2)}}\] \[\tilde{O}^{(2)}=\text{diag}(e^{m^{(1)}-m^{(2)}})^{-1}\tilde{O}^{(1)}+e^{S^{(2)}-m^{(2)}}V^{(2)}=e^{S^{(1)}-m}V^{(1)}+e^{S^{(2)}-m}V^{(2)}\] \[O^{(2)}=\text{diag}(l^{(2)})^{-1}\tilde{O}^{(2)}=O\] <p>FlashAttention-2에서는 기존과 달리 term 자체가 줄어들었다. Forward pass 알고리즘을 정리하면 다음과 같다.</p> <p align="center"> <img src="/assets/post/image/flashattention2/alg1.png" width="100%"> </p> <h2 id="backward">Backward</h2> <p align="center"> <img src="/assets/post/image/flashattention2/alg2.png" width="100%"> </p> <p>Backward는 \(L\)을 사용한다는 것 말고는 다른 차이점은 없다.</p> <h2 id="parallelism">Parallelism</h2> <p>기본적으로 GPU는 병렬 처리가 가능하다. 각 GPU thread block마다 1개의 attention module이 들어가고, 보통 batch size와 self-attention head 수에 맞춰 thread block을 구성한다. 만약 sequence 길이가 길어지거나, batch size가 작거나, self-attention head가 적으면 병렬 처리가 잘 이루어지지 않는다. 그래서 저자는 sequence length dimension에 따라 병렬 처리를 하도록 했다.</p> <p><strong>Forward pass</strong> 저자는 sequence length dimension으로 병렬처리를 하도록 했으며, 이는 한 sequence 내에서 독립적으로 처리되게 구성했다. 물론 기존처럼 batch와 multi-head 간 병렬처리는 계속 유지된다.</p> <p><strong>Backward pass</strong> Algorithm 2에 따르면, backward pass는 column block 간 병렬처리가 이루어진다. 또한 sequence length dimension을 병렬 처리할 수 있도록 추가적인 방법을 사용한다.</p> <p align="center"> <img src="/assets/post/image/flashattention2/fig2.png" width="80%"> </p> <p>결과적으로 worker마다 병렬 처리가 잘 되도록 구성되었다.</p> <h2 id="work-partitioning-between-warp">Work Partitioning Between Warp</h2> <h3 id="forward">Forward</h3> <p align="center"> <img src="/assets/post/image/flashattention2/fig3.png" width="100%"> </p> <p>기존 FlashAttention에서는 \(K\)와 \(V\)를 각각의 warp에 나눠서 partition하고, \(Q\)는 모든 warp이 접근할 수 있도록 했다. 이를 “split-K”라고 부른다. 하지만 이 방식은 \(QK^T\)와 \(V\)의 연산이 partition된 후 중간 계산 결과를 저장하고, 읽고, 동기화를 자주 해야 해서 IO에서 병목이 생긴다. 그래서 \(Q\)를 partition하고, \(K\)와 \(Q\)는 공유하게 해서 IO를 줄여 속도를 높였다.</p> <h2 id="backward-1">Backward</h2> <p>“split-K”를 지양한다는 것 밖에 이해를 못했다.</p> <h3 id="tuning-block-sizes">Tuning block sizes</h3> <p>Block size를 늘리면 memory IO가 줄어든다. 하지만 block 수가 많아지면 registers 수가 늘어나고, total shared memory 크기가 커져 비효율적이 될 수 있다. 너무 많은 registers는 속도를 저하시킬 수 있고, shared memory가 너무 커지면 GPU 메모리가 부족해질 수 있다. 그래서 GPU마다 적절한 block size를 조정해야 한다.</p> <h1 id="empirical-validation">Empirical Validation</h1> <p>결과적으로 FlashAttention-2는 기존 FlashAttention, xFormer보다 2배 이상의 성능 향상을 보였고, 특히 A100 GPU에서 2.7배까지 성능 향상이 일어났다. 전체적으로 FlashAttention-2는 GPU의 이론적인 성능에 가까운 성능을 보여주었다.</p> <p align="center"> <img src="/assets/post/image/flashattention2/fig4.png" width="80%"> </p> <h2 id="conclusion">Conclusion</h2> <p>FlashAttention-2는 기존 FlashAttention을 개선한 방법으로, 다양한 최적화를 통해 성능을 2배 이상 향상시켰다. GPU의 이론적 성능에 근접한 성능을 내면서, low-occupancy와 memory IO를 줄였다. 이 논문은 attention을 최적화하고 성능을 향상시키기 위한 여러 기술적 접근을 다뤘다.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/fastattention/">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/integral-neural-network/">Integral Neural Network</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/tinyvit/">TinyViT</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/edgevit/">EdgeViT</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/mobileone/">MobileOne: An Improved One millisecond Mobile Backbone</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"wonbeomjang/wonbeomjang.github.io","data-repo-id":"R_kgDOIsrE7Q","data-category":"Comments","data-category-id":"DIC_kwDOIsrE7c4CaZnD","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"ko",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Wonbeom Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 24, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-NH0GKRG1BP"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-NH0GKRG1BP");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>