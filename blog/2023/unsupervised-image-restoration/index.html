<!DOCTYPE html> <html lang="kr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1352545434285249" crossorigin="anonymous"></script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Invariant Representation for Unsupervised Image Restoration | Wonbeom Jang</title> <meta name="author" content="Wonbeom Jang"> <meta name="description" content="First unsupervised image restoration"> <meta name="keywords" content="computer-vision, machine-learning, software-engineer, software-engineering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.wonbeomjang.kr/blog/2023/unsupervised-image-restoration/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Wonbeom </span>Jang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Invariant Representation for Unsupervised Image Restoration</h1> <p class="post-meta">April 29, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/image-restoration"> <i class="fa-solid fa-tag fa-sm"></i> image-restoration</a>   <a href="/blog/category/unsupervised-learning"> <i class="fa-solid fa-tag fa-sm"></i> unsupervised-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>Image restortation은 대부분 pair한 dataset이 필요하다. 하지만 이를 구하는 것은 어려워서 CycleGAN을 이용하여 Image Restoration을 진행하는 경우가 있다. 하지만 CycleGAN과 같은 Image to Image translation과 DIRT와 같은 unsupervised domain adaptation은 다음과 같은 단점을 가지고 있다.</p> <h3 id="indistint-domain-boundary">Indistint Domain Boundary</h3> <p>Horse to zebra와 같이 image translation은 분명한 domain boundary가 존재한다. 하지만 image restoration은 noise level과 복잡한 backbond가 domain boundary를 희미하게 만들어서 이미지 퀄리티가 낮아진다.</p> <h3 id="weak-representation">Weak Representation</h3> <p>Unsupervised Domain Adaptation은 high-level representation만 추출한다. 이는 domain shift problem을 야기하여 low-quality reconstruction을 만들어낸다.</p> <h3 id="poor-generalization">Poor Generalization</h3> <p>One-to-one image translation은 semantic representation과 texture representation을 분리하여 잡아내기 힘들다.</p> <p>따라서 이 논문에서 위의 문제를 해결하며 다음의 contribution을 남겼다.</p> <ol> <li>Image restoration분야에서 unsuperviesd representation learning method를 제안했다.</li> <li>Dual domain constraint를 통해 semantic representation과 texture representation; 두 개의 representation을 분리했다.</li> <li>Domain transfer를 통해 unsupervised image restoration을 제안했다.</li> </ol> <h1 id="the-proposed-method">The Proposed Method</h1> <p align="center"> <img src="/assets/post/image/unsupervised-image-restoration/Untitled.png" width="80%"> </p> <p>먼저 문제에 대한 정의를 하겠다. \(\mathcal{X}\)를 Noisy Image Domain, \(\mathcal{Y}\)를 Clean Image Domain이라고 하겠다. Encoder는 각각의 도메인을 같은 vector space인 shared-latent space \(\mathcal{Z}\)로 projection 시킨다. 따라서 vector space에 대하여 다음의 식이 성립한다.</p> \[z=E_\mathcal{X}(x)=E_\mathcal{Y}(y)\] <p>Generator는 shared-latent space \(\mathcal{Z}\)에서 image를 만들어낸다. 따라서 다음과 같은 식이 성립한다.</p> \[x=G_\mathcal{X}(z),y=G_\mathcal{Y}(z)\] <p>이 때 각각의 도메인에대해 Encoder와 Generator는 \(\{E_\mathcal{X}, G_\mathcal{X}\}, \{E_\mathcal{Y}, G_\mathcal{Y}\}\) 각각 존재한다. 각각의 encoder가 shared-latent space \(\mathcal{Z}\)로 projection을 시킨다고 하더라도 각각의 latent vector는 다르다. 따라서 latent vector를 구분하여 적어주겠다.</p> \[z_\mathcal{X}=E_\mathcal{X}(x), z_\mathcal{Y}=E_\mathcal{Y}(y)\] <p>따라서 우리가 하고싶어하는 Image restoration과정은 다음과 같다.</p> \[F^{\mathcal{X}\rightarrow\mathcal{Y}}(x)=G_\mathcal{Y}(z_\mathcal{X})\] <h2 id="discrete-representation-learning">Discrete Representation Learning</h2> <p>먼저 one-to-one image translation의 poor generalization 문제를 해결하기 위해 semantic representation과 texture(noise) representation을 분리시켜야한다. 따라서 저자는 다음 4가지의 방법론을 제시했다.</p> <ol> <li>Detangling Representation</li> <li>Forward Cross Translation</li> <li>Backward Cross Reconstruction</li> <li>Adversarial Domain Adaptation</li> </ol> <h3 id="detangling-representation">Detangling Representation</h3> <p align="center"> <img src="/assets/post/image/unsupervised-image-restoration/Untitled%201.png" width="80%"> </p> <p>먼저 extra noise encoder(\(E_\mathcal{X}^N\))을 도입을 한다. \(E_\mathcal{X}^N\)은 noise를 나타내는 texture latent vector를 뽑아내는 역할로 이를 도입해서 semantic representation과 texture representation을 분리했다. 이를 통해 \(z_\mathcal{X}$와\)z_\mathcal{Y}\(는 같은 distribution을 가지게 된다. 만약 noise image를 self-reconstruction하려면\)x=G_\mathcal{X}(z_\mathcal{X}, z_\mathcal{X}^N)$$$을 통하여 같이 reconstruction하면 된다.</p> <h3 id="forward-cross-translation">Forward Cross Translation</h3> <p>CycleGAN처럼 noise image에서 clean image 변환과 clean image에서 noise이미지의 변환이 되어야한다. 따라서 다음과 같은 방법으로 이미지 변환을 한다. 이 때 \(\mathcal{Y}\)에 \(\mathcal{X}\)의 noise를 추가하기 위해 \(z_\mathcal{X}^N\)를 이용한다.</p> <ol> <li>Noise image → clean image: \(\tilde{x}^{\mathcal{X}\rightarrow\mathcal{Y}} =G_\mathcal{Y}(z_\mathcal{X})\)</li> <li>Clean image → noise image: \(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}} =G_\mathcal{X}(z_\mathcal{Y}\oplus z_\mathcal{X}^N)\)</li> </ol> <h3 id="backward-cross-translation">Backward Cross Translation</h3> <p>Forward cross translation을 했으니 backward cross translation을 할 수 있다. 이 때 \(\mathcal{X}\)에 \(\mathcal{Y}\)의 noise를 추가하기 위해 \(E_\mathcal{X}^N(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}})\)$를 이용한다.</p> <ol> <li>Noise image → clean image: \(\hat{x}=G_\mathcal{X}(E_\mathcal{Y}(\tilde{x}^{\mathcal{X}\rightarrow\mathcal{Y}})\oplus E_\mathcal{X}^N(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}}))\)</li> <li>Clean image → noise image: \(\hat{y}=G_\mathcal{Y}(E_\mathcal{X}(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}}))\)</li> </ol> <p>Backward cross translatio를 학습하기 위해 loss를 다음과 같이 구성한다.</p> \[\mathcal{L}_\mathcal{X}^{CC}(G_\mathcal{X},G_\mathcal{Y},E_\mathcal{X},E_\mathcal{Y},E_\mathcal{X}^N)=\mathbb{E}_\mathcal{X}[||G_\mathcal{X}(E_\mathcal{Y}(\tilde{x}^{\mathcal{X}\rightarrow\mathcal{Y}})\oplus E_\mathcal{X}^N(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}}))-x||_1]\] \[\mathcal{L}_\mathcal{Y}^{CC}(G_\mathcal{X},G_\mathcal{Y},E_\mathcal{X},E_\mathcal{Y},E_\mathcal{X}^N)=\mathbb{E}_\mathcal{X}[||G_\mathcal{Y}(E_\mathcal{X}(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}}))-y||_1]\] <h3 id="adversarial-domain-adaptation">Adversarial Domain Adaptation</h3> <p>Semantic representation (\(z_\mathcal{X}, z_\mathcal{Y}\))은 같은 vector space를 사용해야한다. 따라서 이를 강제하기 위해서 reprenentation discriminator \(D_r\)를 사용한다.</p> \[\mathcal{L}^\mathcal{R}_{adv}(E_\mathcal{X},E_\mathcal{Y},D_\mathcal{R})=\mathbb{E}_\mathcal{X}[\frac{1}{2}logD_\mathcal{R}(z_\mathcal{X}+\frac{1}{2}(1-logD_\mathcal{R}(z_\mathcal{X})))] + \mathbb{E}_\mathcal{Y}[\frac{1}{2}logD_\mathcal{R}(z_\mathcal{Y}+\frac{1}{2}(1-logD_\mathcal{R}(z_\mathcal{Y})))]\] <h2 id="self-supervised-constraint">Self-Supervised Constraint</h2> <h3 id="background-consistency-loss">Background Consistency Loss</h3> <p align="center"> <img src="/assets/post/image/unsupervised-image-restoration/Untitled%202.png" width="50%"> </p> <p>복잡한 background의 Indistint Domain Boundary을 해결하기 위해 backgound consistency loss를 제안한다. Noise가 있는 이미지와 깨끗한 이미지는 gaussian blur를 하면 structure 정보만 남기 때문에 background의 비교가 가능하다. 따라서 저자는 다음과 같은 loss를 추가한다.</p> \[\mathcal{L}_{BC}=\sum_{\sigma=5,9,15} \lambda_\sigma||B_\sigma(\mathcal{X})-B_\sigma(\tilde{\mathcal{X}})||_1\] <h3 id="semantic-consistency-loss">Semantic Consistency Loss</h3> <p>perception loss에서 영감을 받아 pretrained-backbone을 통한 semantic한 정보는 noise가 줄어들 것이라고 기대가 된다. 따라서 VGG19에서 conv5-1 layer와 같이 깊은 layer에서 feature map을 뽑아 비교하여 semantic representations의 consistency를 유지한다.</p> \[\mathcal{L}_{SC}=||\phi_l(\mathcal{X}-\phi_l(\tilde{\mathcal{X}})||_2^2\] <h2 id="joint-optimizing">Joint Optimizing</h2> <p>좋은 성능을 위하여 다른 여러가지도 추가했다.</p> <h3 id="target-domain-adversarial-loss">Target Domain Adversarial Loss</h3> <p>Noise Domain과 clean image domain에서 결과물을 더 잘만들기 위해 GAN loss를 추가한다.</p> \[\mathcal{L}_{adv}^\mathcal{X}=\mathbb{E}_{x\sim P_\mathcal{X}(x)}[logD_\mathcal{X}(x)]+\mathbb{E}_{y\sim P_\mathcal{Y}(y), x \sim P_\mathcal{X}(x)}[log(1-D_\mathcal{X}(G_\mathcal{X}(E_\mathcal{Y}(y), E_\mathcal{X}^N(x))))]\] \[\mathcal{L}_{adv}^\mathcal{Y}=\mathbb{E}_{y\sim P_\mathcal{Y}(y)}[logD_\mathcal{Y}(y)]+\mathbb{E}_{x \sim P_\mathcal{Y}(y)}[log(1-D_\mathcal{Y}(G_\mathcal{Y}(E_\mathcal{X}(x)))]\] <h3 id="self-reconstruction-loss">Self Reconstruction Loss</h3> <p>안정적인 학습 진행을 위해서 self reconstruction loss도 추가해였다.</p> \[\hat{x}=G_\mathcal{X}(E_\mathcal{X}(x)\oplus E_\mathcal{X}^N(x)), \hat{y}=G_\mathcal{Y}(E_\mathcal{Y}(y))\] \[\mathcal{L}^\mathcal{X}_{rec}=||\hat{x} - x||_1, \mathcal{L}^\mathcal{Y}_{rec}=||\hat{y} - y||_1\] <h3 id="kl-divergence-loss">KL Divergence Loss</h3> <p>Noise는 보통 normal distribution을 따른다. 따라서 이 논문에서도 latent-vector가 normal distribution을 따르도록 KL divergence loss를 추가했다.</p> \[p(z_\mathcal{X}^N\sim N(0, 1))\] <h2 id="total-loss">Total Loss</h2> <p>모든 loss를 합치면 다음과 같다.</p> \[\underset{E_\mathcal{X},E_\mathcal{X}^N,E_\mathcal{Y},G_\mathcal{X},G_\mathcal{Y}}{\operatorname{min}} \underset{D_\mathcal{X},D_\mathcal{Y},D_\mathcal{R}}{\operatorname{max}} =\lambda_\mathcal{R}\mathcal{L}^\mathcal{R}_{adv}+\lambda_{adv}\mathcal{L}^{domain}_{adv}+\lambda_{CC}\mathcal{L}^{CC}+\lambda_{rec}\mathcal{L}^{Rec}+\lambda_{bc}\mathcal{L}^{BC}+\lambda_{sc}\mathcal{L}^{SC}+\lambda_{KL}\mathcal{L}^{KL}\] <h2 id="restoration">Restoration</h2> <p align="center"> <img src="/assets/post/image/unsupervised-image-restoration/Untitled%203.png" width="80%"> </p> <p>학습이 끝난 후에 noise가 있는 이미지를 보구언하려면 cross encoder-generator $${ E_\mathcal{X}, E_\mathcal{Y}}$ 를 사용하면 된다.</p> \[\tilde{x}^{\mathcal{X}\rightarrow \mathcal{Y}}=G_\mathcal{Y}(E_\mathcal{X}(x))\] <h1 id="experiment">Experiment</h1> <p>성능은 unsupervised방법들 중에선 성능이 좋다.</p> <p align="center"> <img src="/assets/post/image/unsupervised-image-restoration/Untitled%204.png" width="70%"> <img src="/assets/post/image/unsupervised-image-restoration/Untitled%205.png" width="70%"> <img src="/assets/post/image/unsupervised-image-restoration/Untitled%206.png" width="70%"> </p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">읽어볼 거리</h2> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/dine/">DINE: Domain Adaptation from Single and Multiple Black-box Predictors</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/multimodal-vs-unimodal/">What Makes Multi-modal Learning Better than Single (Provably)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/integral-neural-network/">Integral Neural Network</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/simple-baselines-for-image-retoration/">Simple Baselines for Image Restoration</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/rethinking-batch-in-batchnorm/">Rethinking “Batch” in BatchNorm</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusAttributes={src:"https://giscus.app/client.js","data-repo":"wonbeomjang/wonbeomjang.github.io","data-repo-id":"R_kgDOIsrE7Q","data-category":"Comments","data-category-id":"DIC_kwDOIsrE7c4CaZnD","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":"light","data-lang":"ko",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Wonbeom Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 18, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>