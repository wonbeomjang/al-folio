<!DOCTYPE html> <html lang="kr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1352545434285249" crossorigin="anonymous"></script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>What Makes Multi-modal Learning Better than Single (Provably) | Wonbeom Jang</title> <meta name="author" content="Wonbeom Jang"> <meta name="description" content="NeurIPS 2021"> <meta name="keywords" content="computer-vision, machine-learning, software-engineer, software-engineering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.wonbeomjang.kr/blog/2023/multimodal-vs-unimodal/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Wonbeom </span>Jang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">What Makes Multi-modal Learning Better than Single (Provably)</h1> <p class="post-meta">November 19, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/multi-modal"> <i class="fa-solid fa-hashtag fa-sm"></i> multi-modal</a>   <a href="/blog/tag/paper"> <i class="fa-solid fa-hashtag fa-sm"></i> paper</a>     ·   <a href="/blog/category/multi-modal"> <i class="fa-solid fa-tag fa-sm"></i> multi-modal</a>   <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> paper</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>우리 세상은 많은 modality가 존재한다. 그리고 관념적으로도 여러 modal의 네트워크들을 fusion시키면 uni-modal보다 성능이 더 좋게 나온다. 그렇다면 우리는 이러한 궁금증이 생긴다.</p> <p align="center"> *multi-modal learning이 uni-modal learning보다 좋은 성능을 제공할까?* </p> <p>저자는 이 궁금증에서 연구를 시작했고, 다음 두 가지를 중점적으로 살펴봤다.</p> <ul> <li>(When) 어떤 상황에서 multi-modal이 uni-modal 보다 성능이 좋은가?</li> <li>(Why) 무엇이 이런 성능을 유도했는가?</li> </ul> <p>그리고 연구를 통해서 저자가 한 comtribution은 다음과 같다.</p> <ul> <li>Multi-modal learning을 population risk로 설명하고, 이는 latent representation quality의 bound 되어있다는 것을 밝혔다.</li> <li>전체 modality의 subset으로 훈련시킨 network의 quality의 upper bound를 유도했다.</li> <li>Modality의 subset으로 학습시키면 성능이 하락한다는 것을 이론적으로 분석했다.</li> </ul> <p>참고로 결론은 다음과 같다.</p> <ul> <li>Multiple modality는 그 modal의 subset보다 적은 population risk를 갖는다.</li> <li>이는 multi-modal이 더 정확한 latent space representation을 학습할 수 있다는 것이다.</li> </ul> <p>이제 하나씩 살펴보자.</p> <h1 id="the-multi-modal-learning-formulation">The Multi-modal Learning Formulation</h1> <p align="center"> <img src="/assets/post/image/multi-modal-vs-uni-modal/figure1.png" width="80%"> </p> <p>먼저 수식을 정리하자. K개의 modalities에 대해서 data는 \(\mathbb{x}:=(x^{(1)},\cdots,x^{(K)})\) 으로 표현한다. 이 때 \(x^{(k)} \in \mathcal{X}^{(k)}\) 이다. 우리는 K개의 modalities를 보유하기 때문에 전체 input data space는</p> \[\mathcal{X}=\mathcal{X}^{1} \times \cdots \times \mathcal{X}^{k}\] <p>로 표현된다. 그리고 target domain을 \(\mathcal{Y}\) , multi-modal의 공통된 latent space를 \(\mathcal{Z}\) 라 하자. 우리는 이제 true mapping을 다음과 같이 쓸 수 있다.</p> \[g^\star: \mathcal{X} \mapsto \mathcal{Z}, g^\star \in \mathcal{G}\] \[h^\star: \mathcal{Z} \mapsto \mathcal{Y}, h^\star \in \mathcal{H}\] <p>그렇다면 이제 우리는 \(\mathbb{x}\) 의 data distribution을 정의할 수 있다.</p> \[\mathbb{P}_\mathcal{D}(\mathbb{x},y)\triangleq\mathbb{P}_{y|x}(y|h^\star\circ g^\star(\mathbb{x}))\mathbb{P}_\mathbb{x}(\mathbb{x})\] <p>참고로 \(h^\star\circ g^\star(\mathbb{x})=h^\star(g^\star(\mathbb{x}))\) 로 합성함수를 의미한다.</p> <p>우리는 일반화를 위해 \(\mathcal{N} \leq \mathcal{M}\) 인 modalitie의 subset에 대해서 살펴볼 것이다. Modality의 superset을 정의하자.</p> \[\mathcal{X}^\prime := (\mathcal{X}^{(1)}\cup\bot)\times\cdots\times(\mathcal{X}^{(K)}\cup\bot)\] <p>이때, \(\bot\) 은 k번째의 modality는 쓰지 않는다는 것이다. 간단하게 시각화하면 다음과 같다.</p> <p align="center"> <img src="/assets/post/image/multi-modal-vs-uni-modal/img1.png" width="80%"> </p> <p>이제 modalities를 선택하는 함수를 정의하자.</p> \[p_\mathcal{M}(\mathbb{x})^{(k)}= \begin{cases} \mathbb{x}^{(k)} \text{ if } k\in\mathcal{M} \\ \bot \text{ else } \end{cases}\] <p>이때, 우리는 다음과 같은 식을 만들수도 있다. \(p^\prime_\mathcal{M} := \mathcal{X}^\prime\mapsto\mathcal{X}^\prime\)<br> 우리의 목표는 Empirical Risk Minimization (ERM) principle에 따라서 learning objective를 minimize하는 것이다.</p> \[\text{min } \hat{r}(h\circ g_\mathcal{M} \triangleq\frac{1}{m}\sum_{i=1}^ml(h\circ g_\mathcal{M}(\mathbb{x}_i),y_i) \text{ s.t. } h \in \mathcal{H}, g_\mathcal{M} \in \mathcal{G}\] <p>여기서 \(l\) 은 loss fuction이고, 최종적으로 정의하는 population risk는 다음과 같다.</p> \[r(h\circ g_\mathcal{M})=\mathbb{E}_{(\mathbb{x}_i, y_i)\sim\mathcal{D}}[\hat{r}(h\circ g_\mathcal{M})]\] <h1 id="main-result">Main Result</h1> <blockquote> <p><strong>Definition 1.</strong> Given a data distribution with the form in (1), for any learned latent representation mapping \(g \in \mathcal{G}\) , the <em>latent representation quality</em> is defined as</p> \[\eta(g)=\text{inf}_{h\in\mathcal{H}}[r(h\circ g)-r(h(h^*\circ g^*))]\] </blockquote> <p>즉, \(\eta(g))\) 는 mapping function의 \(g \in \mathcal{G}\) 에 대해서 true latent space와 차이이기 때문에 latent space quality라고 할 수 있다.</p> <h3 id="rademacher-complexity">Rademacher complexity</h3> <p>이제 model complexity를 측정하는 Rademacher complexity에 대해서 알아보자. \(\mathcal{F}\) 를 \(\mathbb{R}^d \mapsto \mathbb{R}\) 인 vector-valued function으로 정의하자. \(\mathbb{R}^d\) 에서 iid 한 \(Z_1,...,Z_m\) 에 대해 sample를 \(S=(Z_1,...,Z_m)\) 라고 하자. Empirical Rademacher complexity는 다음과 같이 정의된다.</p> \[\hat{\mathfrak{R}}_S(\mathcal{F}):=\mathbb{E}_\sigma[ \underset{f\in\mathcal{F}}{\text{sup}}\frac{1}{m}\sum_{i=1}^m\sigma_if(Z_i)]\] <p>이 때, \(\sigma=(\sigma_1,...,\sigma_n)^\top\) with \(\sigma_i \sim \text{unif}\{-1, 1\}\) 이다. 전체적인 Rademacher complexity은 다음과 같다.</p> \[\mathfrak{R}_S(\mathcal{F})=\mathbb{E}[\hat{\mathfrak{R}}_S(\mathcal{F})]\] <p>이해하기 어려우니 다른 블로그의 설명을 인용하겠다.</p> <blockquote> <p><em>Rademacher complexity가 1이라는 것은 모델이 위와 같은 random한 setup에서도 잘 fitting 했다는 것이므로, complexity가 크고 따라서 generalize를 잘 못할 것이라고 이야기 할 수 있다는 개념이다.<br> <a href="https://yun905.tistory.com/68" rel="external nofollow noopener" target="_blank">https://yun905.tistory.com/68</a></em></p> </blockquote> <h2 id="connection-to-latent-representation-quality">Connection to Latent Representation Quality</h2> <p>이제 latent space quality와 population risk의 관계를 살펴보자.</p> <blockquote> <p><strong>Theorem 1</strong>. Let \(S = ((x_i,y_i))^m_{i=1}\) be a dataset of m examples drawn i.i.d. according to \(\mathcal{D}\) . Let M, N be two distinct subsets of [ \(K\) ]. Assuming we have produced the empirical risk minimizers \((\hat{h}_\mathcal{M}, \hat{g}_\mathcal{M})\) and \((\hat{h}_\mathcal{N}, \hat{g}_\mathcal{N})\) , training with the \(\mathcal{M}\) and \(\mathcal{N}\) modalities separately. Then, for all \(1 &gt; \delta &gt; 0\) , with probability at least \(1-\frac{\delta}{2}\) :</p> </blockquote> <p>\(r(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}) - r(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}) \leq \gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N})+8L\mathfrak{R}(\mathcal{H} \circ \mathcal{G}_{\mathcal{M}})+\frac{4C}{\sqrt{m}}+2C\sqrt{\frac{2\text{ln}(2/\delta)}{m}}\)$</p> \[\text{where}, \gamma_S(\mathcal{M},\mathcal{N})\triangleq\eta(\hat{g}_\mathcal{M})-\eta(\hat{g}_\mathcal{N})\] <p>즉, population risk의 차이는 latent space quality 차이와 model complexity에 upper bound가 된다는 것이다. 이는 그대로 사용하지 않고, 추후에 식 정리할 때 사용할 것이다. 여기서 sample size \(m\) 에 대해 \(\mathfrak{R}_S(\mathcal{F})\) 은 보통 \(\sqrt{C(\mathcal{F})/m}\) 에 bound된다. 따라서 우리는 다음과 같이 다시 쓸 수 있다.</p> \[r(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}) - r(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}) \leq \gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N})+\text{O}(1/m)\] <h2 id="upper-bound-for-latent-space-exploration">Upper Bound for Latent Space Exploration</h2> <blockquote> <p><strong>Theorem 2</strong>. Let \(S={(x_i, y_i)}^m_{i=1}\) be a dataset of m examples drawn i.i.d. according to D. Let M be a subset of [ \(K\) ]. Assuming we have produced the empirical risk minimizers \((\hat{h}_\mathcal{M}, \hat{g}_\mathcal{M})\) training with the M modalities. Then, for all \(1 &gt; \delta &gt; 0$, with probability at least\)1 − \delta$$ :</p> </blockquote> \[\eta(\hat{g}_{\mathcal{M}})\leq 4L\mathfrak{R}(\mathcal{H} \circ \mathcal{G}_{\mathcal{M}})+4L\mathfrak{R}(\mathcal{H} \circ \mathcal{G})+6C\sqrt{\frac{2\text{ln}(2/\delta)}{m}}+\hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S)\] \[\text{where } \hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S) \triangleq \hat{r}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}})-\hat{r}(h^\star\circ g^\star)\] <p>위에서 처럼 Rademacher complexity은 \(O(1/m)\) 이기 때문에</p> \[\eta(\hat{g}_{\mathcal{M}})\leq \hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S)+\text{O}(1/m)\] <p>이 성립한다. 이 때, assumption 3에 의해</p> \[\hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S) \leq \hat{L}(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}, S)\] <p>이 성립한다.</p> <h2 id="result">Result</h2> <p><em>그렇다면 언제 multi-modal을 사용해야하냐?</em></p> \[\hat{L}(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}, S) - \hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S) \geq \sqrt{\frac{C(\mathcal{H}\circ\mathcal{G}_\mathcal{M})}{m}}-\sqrt{\frac{C(\mathcal{H}\circ\mathcal{G}_\mathcal{N})}{m}}\] <p>저자는 다음과 같이 말한다.</p> <blockquote> <p><em>(i) When the number of sample size m is large, the impact of intrinsic complexity of function classes will be reduced. (ii) Using more modalities can efficiently optimize the empirical risk, hence improve the latent representation quality.</em></p> </blockquote> <p>Sample size m이 충분히 클 때 Theorem 1에 적용하면 다음과 같은 식이 성립한다.</p> <p>\(\gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N})= \eta(\hat{g}_{\mathcal{M}}) - \eta(\hat{g}_{\mathcal{N}})\leq \hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S) - \hat{L}(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}, S) \leq 0\)$</p> \[r(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}) \leq r(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}})\] <p>즉, 데이터셋의 크기가 클 때 modality의 수가 많은 것을 사용하는 것이 좋다.</p> <h2 id="non-positivity-guarantee">Non-Positivity Guarantee</h2> <p>sample size s가 클 때 \(\gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N})\) 이 non-positive라는 것을 증명할 수 있다. 이것의 증명은 여기서 다루지 않겠다.</p> <h1 id="experiment">Experiment</h1> <p>이제 실험을 보자. Dataset으로는 Interactive Emotional Dyadic Motion Capture (IEMO- CAP) database을 사용했다. 이 데이터셋에는 여러 모달에 대해서 여러 사람이 대화하는 것이 들어있으며 발화자가 누구인지 맞추는 것이 목표이다. 여기에는 Text, Video, Audio 정보가 들어가있다.</p> <h2 id="number-of-modalities">Number of Modalities</h2> <p align="center"> <img src="/assets/post/image/multi-modal-vs-uni-modal/table1.png" width="80%"> </p> <p>Modal이 늘어날 수록 정확도가 상승하는 것을 볼 수 있다.</p> <h2 id="number-of-samples">Number of Samples</h2> <p>위에서 sample의 수가 클 때 multi-modal이 좋다고 했다. 따라서 이를 살펴보자.</p> <p align="center"> <img src="/assets/post/image/multi-modal-vs-uni-modal/table2.png" width="80%"> </p> <p>여기서 볼 수 있듯, sample의 수가 줄어들면 madality의 수가 적을 때 성능이 좋은 경우가 있다.</p> <h2 id="quality-of-latent-spaces">Quality of Latent Spaces</h2> <p>multi-modal은 latent space quality가 좋다고 했다. 이를 확인해보자.</p> <p align="center"> <img src="/assets/post/image/multi-modal-vs-uni-modal/table3.png" width="80%"> </p> <p>Sample의 수와 modal의 수로 비교해도 같은 결과를 낸다.</p> <p align="center"> <img src="/assets/post/image/multi-modal-vs-uni-modal/figure2.png" width="80%"> </p> <h2 id="synthetic-data">Synthetic Data</h2> <p>실제 데이터에서 sample의 수가 많을 때 multi-modal이 좋다는 것을 확인했다. 인공데이터는 어떨까?</p> <p align="center"> <img src="/assets/post/image/multi-modal-vs-uni-modal/table4.png" width="80%"> </p> <p>저자가 만든 인공데이터도 같은 모습을 보였다.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">읽어볼 거리</h2> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/tinyvit/">TinyViT</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/keras-3/">Keras 3.0 설명</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/integral-neural-network/">Integral Neural Network</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/byol/">Bootstrap your own latent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/unsupervised-image-restoration/">Invariant Representation for Unsupervised Image Restoration</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusAttributes={src:"https://giscus.app/client.js","data-repo":"wonbeomjang/wonbeomjang.github.io","data-repo-id":"R_kgDOIsrE7Q","data-category":"Comments","data-category-id":"DIC_kwDOIsrE7c4CaZnD","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":"light","data-lang":"ko",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Wonbeom Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: January 03, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>