<?xml version="1.0" encoding="UTF-8"?> <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"> <channel> <title> blank </title> <description>개발을 좋아하는 딥러닝 리서쳐 장원범입니다. </description> <link>https://www.wonbeomjang.kr/</link> <atom:link href="https://www.wonbeomjang.kr/feed.xml" rel="self" type="application/rss+xml"/> <pubDate>Thu, 19 Dec 2024 01:34:03 +0000</pubDate> <lastBuildDate>Thu, 19 Dec 2024 01:34:03 +0000</lastBuildDate> <generator>Jekyll v4.3.4</generator> <item> <title>Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method 설명</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;많은 model 개발자들은 LLM을 개발할 때 사용한 corpus를 비공개로 처리한다. &lt;ul&gt; &lt;li&gt;저작권, 윤리적 문제가 존재하기 때문이다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;저자는 black-box LLM과 text가 주어졌을 때 해당 text가 training data에 포함되어있는지 확인하는 방법론을 제시한다.&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;아이디어&quot;&gt;아이디어&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Divergence-from-randomness에서 영감을 받는다.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;특정 단어가 한 문서 내에서 사용되는 빈도(Within-document term-frequency)&lt;/em&gt;&lt;/strong&gt;와 &lt;strong&gt;&lt;em&gt;그 단어가 전체 문서 컬렉션 내에서 사용되는 빈도(frequency of a word within the collection)&lt;/em&gt;&lt;/strong&gt;가 얼마나 차이가 나는지를 측정하면, 그 단어가 해당 문서에서 얼마나 중요한 정보를 담고 있는지 알 수 있다. &lt;ul&gt; &lt;li&gt;Within-document term-frequency &lt;ul&gt; &lt;li&gt;LLM predicted token의 probability&lt;/li&gt; &lt;li&gt;Token probability distribution&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Frequency of a word within the collection &lt;ul&gt; &lt;li&gt;Corpus에서 해당 token의 빈도수&lt;/li&gt; &lt;li&gt;Token frequency distribution&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Token probability distribution와 Token frequency distribution의 divergence가 높으면 해당 text가 모델의 training corpus에 있다는 의미다.&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;방법론&quot;&gt;방법론&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Within-document term-frequency&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;특정 text에서 probability distribution을 계산한다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frequency within the collection&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;전체 corpus에서 해당 token이 평균적으로 얼마나 자주 등장하는지를 나타낸다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Divergence&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Within-document term-frequency와 Frequency within the collection을 비교한다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h1 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Text \(x\), LLM \(\mathcal{M}\), 정보가 없는 pretraining corpus \(D\), pretraining data detection task \(A\)에 대해서 &lt;ul&gt; &lt;li&gt; \[\mathcal{A}(x,\mathcal{M})\rightarrow\{0,1\}\] &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Token probability distribution computation &lt;ul&gt; &lt;li&gt;\(\mathcal{M}\)에 텍스트 \(x\)를 query하여 각 token probability를 계산한다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Token probability distribution computation &lt;ul&gt; &lt;li&gt;접근 가능한 대규모 참조 말뭉치 \(\mathcal{D}^\prime\)를 사용하여 토큰 빈도를 추정한다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Score calculation via &lt;em&gt;comparison&lt;/em&gt; &lt;ul&gt; &lt;li&gt;두 분포를 비교하여 각 token의 probability을 calibration하고, calibration된 probability를 기반으로 pretraining data인지 판별하는 점수를 계산한다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Binary decision &lt;ul&gt; &lt;li&gt;Score에 threshold를 적용하여 \(x\)가 모델 \(\mathcal{M}\)의 pretraining corpus에 있는지 예측한다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;32-token-probability-distribution-computation&quot;&gt;3.2 Token Probability Distribution Computation&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;\(x_0\): start-of-sentence token&lt;/li&gt; &lt;/ul&gt; \[x^\prime=x_0x_1x_2...x_n\] &lt;ul&gt; &lt;li&gt;\(\mathcal{M}\)에 \(x\)를 query한다.&lt;/li&gt; &lt;/ul&gt; \[\{p(x_i|x_{\lt i};\mathcal{M}):0\lt i \le n \}\] &lt;h2 id=&quot;33-frequency-of-a-word-within-the-collection&quot;&gt;3.3 Frequency of a word within the collection&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;다음의 term으로 계산한다.&lt;/li&gt; &lt;/ul&gt; \[p(x_i,\mathcal{D}^\prime)=\frac{\text{count}(x_i)}{N^\prime}\] &lt;ul&gt; &lt;li&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;\(x_i\)가 없는 경우를 위해 Laplace smoothing을 추가한다. $$&lt;/td&gt; &lt;td&gt;V&lt;/td&gt; &lt;td&gt;$$는 vocabulary size다.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/li&gt; &lt;/ul&gt; \[p(x_i;D^\prime)=\frac{\text{count}(x_i)+1}{N^\prime+|V|}\] &lt;h2 id=&quot;34-score-calculation-through-compression&quot;&gt;3.4 Score Calculation through Compression&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;\(p(x_i;\mathcal{M})\)과 \(p(x_i;D^\prime)\)의 cross-entropy를 계산한다.&lt;/li&gt; &lt;/ul&gt; \[\alpha_i = -p(x_i; \mathcal{M}) \cdot \log p(x_i; D^\prime).\] &lt;ul&gt; &lt;li&gt;특정 token이 우세한 영향을 미치지 않도록 upper bound를 정의한다.&lt;/li&gt; &lt;/ul&gt; \[\begin{equation} \alpha_i = \begin{cases} \alpha_i, &amp;amp; \text{if } \alpha_i &amp;lt; a \\ a, &amp;amp; \text{if } \alpha_i \geq a \end{cases} \end{equation}\] &lt;ul&gt; &lt;li&gt;Text \(x\)에 대해서 token \(x_i\)가 여러 개 존재할 수 있다. &lt;ul&gt; &lt;li&gt;이럴 때는 첫 번째 토큰의 결과를 가져온다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; \[\beta=\frac{1}{|\text{FOS}(x)|}\sum_{x_j \in \text{FOS(x)}}\alpha_j\] &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%201.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;35-binary-decision&quot;&gt;3.5 Binary Decision&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Threshold로 pretraining corpus \(D\)에 있는지 결정&lt;/li&gt; &lt;/ul&gt; \[\text{Decision}(x, \mathcal{M}) = \begin{cases} 0 \quad (x \notin \mathcal{D}), &amp;amp; \text{if } \beta &amp;lt; \tau, \\ 1 \quad (x \in \mathcal{D}), &amp;amp; \text{if } \beta \geq \tau. \end{cases}\] &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%202.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; </description> <pubDate>Tue, 17 Dec 2024 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2024/pretraining-data-dection-for-large-language-models/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2024/pretraining-data-dection-for-large-language-models/</guid> <category>paper</category> <category>llm</category> <category>paper</category> <category>llm</category> </item> <item> <title>META-REWARDING LANGUAGE MODELS: Self-Improving Alignment with LLM-as-a-Meta-Judge 설명</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;InstructGPT가 성공한 이후로 LLM의 instruction following 능력은 중요하다는 것을 알게 되었다. 따라서 SFT나 preference optimization(RLHF, DPO, PPO, KPO, …)을 통해 human alignment를 높이려고 했다. 하지만 이와 같은 방법들은 많은 시간과 돈이 소요된다는 단점이 있다.&lt;/p&gt; &lt;p&gt;따라서 Self-Reward라는 방법이 제시되었다. 이 방법론은 하나의 LLM이 Actor, Judge 두 가지 역할을 수행하면서 자체적으로 preference optimization을 수행한다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Actor: Specific instruction에 대한 response를 생성한다.&lt;/li&gt; &lt;li&gt;Judge: Actor가 생성한 response를 LLM-as-a-Judge 방식으로 수행하여 reward가 되는 preference pair를 생성한다.&lt;br /&gt; 하지만 이와 같은 방법도 Actor가 좋은 response를 생성하는 데만 관심이 있고, judge의 성능에는 관심이 없다는 것에 대한 단점이 있다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;따라서 저자는 Judge의 성능을 높이기 위해서 LLM-as-a-Meta-Judge를 제안했다. 핵심 아이디어는 하나의 LLM이 Actor, Judge뿐만 아니라 Meta-Judge 역할도 수행한다는 것이다. 이를 통해 모델의 judge 능력에 대한 reward를 줄 수 있다.&lt;/p&gt; &lt;h1 id=&quot;meta-rewarding&quot;&gt;Meta-Rewarding&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/fig1.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Meta Rewarding은 response를 생성하는 actor, response를 평가하는 judge, 그리고 judge를 평가하는 meta-judge로 구성된다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Actor&lt;/strong&gt;&lt;br /&gt; Actor는 각각의 instruction에 대하여 다수의 response를 생성한다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Judge&lt;/strong&gt;&lt;br /&gt; Judge는 LLM-as-a-Judge 프롬프트로 각 response에 대해 score가 포함된 judge를 생성한다. 여기서 생성된 score는 actor를 학습시키기 위한 preference pair가 된다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Meta-Judge&lt;/strong&gt;&lt;br /&gt; 하나의 response에 대하여 여러 가지의 judge를 뽑아 어떤 judge가 좋은지 판단한다. LLM-as-a-Meta-Judge prompt가 사용되고, 여기서 생성된 결과는 judge를 학습시키기 위한 preference pair가 된다.&lt;/p&gt; &lt;h2 id=&quot;actor-preference-dataset-creation&quot;&gt;Actor Preference Dataset Creation&lt;/h2&gt; &lt;h3 id=&quot;1-sample-responses-from-actor&quot;&gt;1. Sample Responses from Actor&lt;/h3&gt; &lt;p&gt;Iteration \(t\)일 때 현재 모델 \(M_t\)를 이용하여 \(K\)개의 response를 생성한다.&lt;/p&gt; \[\{y_1,...,y_{K}\}\] &lt;h3 id=&quot;2-aggregate-multiple-judgements&quot;&gt;2. Aggregate Multiple Judgements&lt;/h3&gt; &lt;p&gt;각 response \(y_k\)에 대하여 N개의 서로 다른 judge를 생성한다. 5점 scale로 평가하되 parsing이 되지 않으면 drop한다.&lt;/p&gt; \[\{j_k^1,...,j_k^N\}\] &lt;h3 id=&quot;3-preference-data-selection-with-length-control&quot;&gt;3. Preference Data Selection with Length-Control&lt;/h3&gt; &lt;p&gt;가장 높은 점수 \(S_{\text{max}}\)를 가진 \(y_c\), 가장 낮은 점수 \(S_{\text{min}}\)을 가진 \(y_r\)를 선택한다. 단, 해당 response를 그대로 쓰지 않고, length control을 통해 길이 조정을 한다.&lt;/p&gt; \[[(1 - \rho) S_{\text{max}} + \rho S_{\text{min}}, S_{\text{max}}]\] &lt;p&gt;위 식 안에 점수가 들어가면 비슷한 quality로 판단하여 drop한다. 그리고 최대한 짧은 답변을 고르려고 노력했다.&lt;/p&gt; &lt;h2 id=&quot;judge-preference-data-creation&quot;&gt;Judge Preference Data Creation&lt;/h2&gt; &lt;h3 id=&quot;1-responses-selection&quot;&gt;1. Responses Selection&lt;/h3&gt; &lt;p&gt;모든 데이터를 활용하는 것은 비효율적이다. 따라서 학습 효율을 위해 judge confidence가 가장 낮은 데이터에 집중한다. 따라서 하나의 instruction에 대해 response score의 variance가 가장 높은 데이터부터 시작한다.&lt;/p&gt; &lt;h2 id=&quot;2-pairwise-meta-judge-evaluation&quot;&gt;2. Pairwise Meta-Judge Evaluation&lt;/h2&gt; &lt;p&gt;\(\{j^1, ..., j^N\}\)에서 두 가지 judgement를 뽑아 \((j^m, j^n)\)을 구성하고 LLM-as-a-Meta-Judge를 수행한다. 이때 position bias를 해결하기 위해 두 judge의 순서를 바꿔서 다시 수행한다. 그리고 만약 결과가 같으면 accept하고, 결과가 다르면 reject한다. 또한 first position과 second position의 가중치를 계산하여 보정했다.&lt;/p&gt; \[\omega_{1} = \frac{\text{win}_{\text{2nd}}}{\text{win}_{\text{1nd}} + \text{win}_{\text{2nd}}}, \text{ } \omega_{2} = \frac{\text{win}_{\text{1nd}}}{\text{win}_{\text{1nd}} + \text{win}_{\text{2nd}}}\] &lt;p&gt;그리고 각 judge 결과를 이용하여 battle result를 만든다.&lt;/p&gt; \[r_{mn} = \begin{cases} 1 &amp;amp; \text{if the meta-judge prefers } j_m \\ -1 &amp;amp; \text{if the meta-judge prefers } j_n \\ 0 &amp;amp; \text{if tie or parse error.} \end{cases}\] \[B_{mn} = \omega_1 \mathbb{1}[r^{mn} = 1] + \omega_2 \mathbb{1}[r^{nm} = -1]\] &lt;h2 id=&quot;3-elo-score-and-pair-selection&quot;&gt;3. Elo Score and Pair Selection&lt;/h2&gt; &lt;p&gt;이후에 Elo score를 계산하여 reward를 구한다.&lt;/p&gt; \[\arg\max_{\varepsilon} \sum_{m,n} B_{mn} \log \left( \frac{e^{\varepsilon_m - \varepsilon_n}}{1 + e^{\varepsilon_m - \varepsilon_n}} \right).\] &lt;p&gt;이때도 judge의 length가 너무 길어지면 reject한다.&lt;/p&gt; &lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt; &lt;h2 id=&quot;experiment-set-up&quot;&gt;Experiment Set-up&lt;/h2&gt; &lt;p&gt;각 Iteration마다 학습 방법을 바꾼다.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Iter 1 Obtain \(M_1\) by training using DPO (initialized from the SFT model) on both actor and judge preference pairs generated by the SFT model.&lt;br /&gt; Iter 2 Obtain \(M_2\) by training \(M_1\) using DPO on actor and judge preference pairs generated by \(M_1\).&lt;br /&gt; Iter 3 Obtain \(M_3\) by training \(M_2\) using DPO exclusively on actor preference pairs generated by \(M_2\).&lt;br /&gt; Iter 4 Obtain \(M_4\) by training \(M_3\) using DPO exclusively on actor preference pairs generated by \(M_3\).&lt;/p&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;instruction-following-evaluation&quot;&gt;Instruction Following Evaluation&lt;/h2&gt; &lt;h3 id=&quot;meta-rewarding-iterations-significantly-improves-the-win-rate&quot;&gt;Meta-Rewarding iterations significantly improves the win rate&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/fig3.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;저자는 Length Control win rate에서 좋은 성능을 보인다고 했다.&lt;/p&gt; &lt;h3 id=&quot;the-meta-judge-and-length-control-mechanism-are-important&quot;&gt;The meta-judge and length-control mechanism are important.&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table1.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Table 1에서 볼 수 있듯, average lengths는 iteration에 따라 증가하지 않는다는 것을 보이고 있다.&lt;/p&gt; &lt;h3 id=&quot;meta-rewarding-improves-nearly-all-instruction-categories&quot;&gt;Meta-Rewarding improves nearly all instruction categories.&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/fig4.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Fig4에서 볼 수 있듯 거의 모든 카테고리에서 성능 향상이 일어났다.&lt;/p&gt; &lt;h3 id=&quot;meta-rewarding-improves-answering-of-complex-and-hard-questions&quot;&gt;Meta-Rewarding improves answering of complex and hard questions.&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table2.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Arena-hard에서도 좋은 성능을 보였다.&lt;/p&gt; &lt;h3 id=&quot;meta-rewarding-does-not-sacrifice-multi-turn-ability-despite-training-only-on-single-turn&quot;&gt;Meta-Rewarding does not sacrifice multi-turn ability despite training only on single-turn&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table6.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Single-turn으로만 학습했음에도 불구하고 multi-turn의 성능을 떨어뜨리지 않았다.&lt;/p&gt; &lt;h2 id=&quot;reward-modeling-evaluation&quot;&gt;Reward Modeling Evaluation&lt;/h2&gt; &lt;h3 id=&quot;the-model-improves-in-judging-after-performing-judge-training&quot;&gt;The model improves in judging after performing judge training&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table3.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Meta-Rewarding 방법은 GPT-4와의 judge 상관관계를 높여주는 것으로 나왔다.&lt;/p&gt; &lt;h3 id=&quot;meta-rewarding-training-improve-judging-correlation-with-human&quot;&gt;Meta-Rewarding training improve judging correlation with Human&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table7.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Meta-Rewarding 방법은 사람과의 judge 상관관계를 높여주는 것으로 나왔다.&lt;/p&gt; &lt;h2 id=&quot;ablations-and-analysis&quot;&gt;Ablations and Analysis&lt;/h2&gt; &lt;h3 id=&quot;length-control-mechanism&quot;&gt;Length-Control Mechanism&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table4.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Length control을 안 썼을 때 verbosity가 발생하는 것을 보였다.&lt;/p&gt; &lt;h3 id=&quot;meta-judge-biases&quot;&gt;Meta-Judge Biases&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table5.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Meta-Rewarding 방법은 높은 점수를 준 judge를 선호하는 것으로 나왔다.&lt;/p&gt; &lt;h3 id=&quot;judge-scoring-shift&quot;&gt;Judge Scoring Shift&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/fig5.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;위의 문제를 보기 위해 Gaussian kernel density estimation을 이용해서 score의 분포를 보았다. 이는 score-bias로 학습하는 동안 score 분포를 5점에 가까운 분포로 바꾸게 되었다.&lt;/p&gt; &lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;/h3&gt; &lt;p&gt;Judge 모델이 판단할 때 적은 quality 차이는 tie를 주는 경향이 있으므로 평균낼 때 주의해야 한다.&lt;br /&gt; 또한 meta-judge에서 bias가 있다.&lt;/p&gt; </description> <pubDate>Thu, 19 Sep 2024 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2024/llm-as-a-meta-judge/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2024/llm-as-a-meta-judge/</guid> <category>paper</category> <category>llm</category> <category>paper</category> <category>llm</category> </item> <item> <title>2023년 회고록</title> <description>&lt;h1 id=&quot;전체적인-평가&quot;&gt;전체적인 평가&lt;/h1&gt; &lt;p&gt;2023년을 한마디로 정리하자면 ‘노력의 결실을 맺은 해’이다.&lt;/p&gt; &lt;p&gt;대학교 입학했을 때부터 지금까지 딥러닝/컴퓨터비전을 좋아해서 열심히 공부, 프로젝트를 했다. 이를 기반으로 CV와 포트폴리오를 작성했다. 이후에는 ‘뉴로클’에서 리서치 인턴을 진행하면서 제품 개발 및 고도화를 진행했고 성과도 내었다. 앞선 경험을 기반으로 SK텔레콤 공채에 지원하여 합격하게 되었다.&lt;/p&gt; &lt;p&gt;4년 동안 열심히 준비하고 노력한 결과이기도 하지만 좋은 기회가 주어져서 감사하게도 성공으로 한 해가 채워졌다. 1년 동안 무슨 일이 있었는지 확인해보자.&lt;/p&gt; &lt;h1 id=&quot;뉴로클에서의-리서치인턴-116825&quot;&gt;뉴로클에서의 리서치인턴 (1/16~8/25)&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2023-review/2F49877F-F3D5-427E-91EA-A482AC3E65EA_1_105_c.jpeg&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;1월부터 뉴로클이라는 스타트업에서 리서치 인턴을 진행했다. 산업계로 갈 것인지 학계로 갈 것인지 고민을 했고 이를 알아보기 위해 스타트업에서 인턴을 했다.&lt;/p&gt; &lt;p&gt;8개월 동안 정말 다양한 논문들을 읽고 공부했으며 실험도 했다. 적어도 1주에 1~2개의 논문을 읽었으니 단순 계산으로도 30개 이상의 논문이다. 그만큼 많은 성장을 했고, 뜻깊은 시간이었다. OCR 고도화 연구뿐만 아니라 Smart labeling (Detection/Segmentation) 기술 검증 및 모델 변환, Transfer Learning 성능검증, Super Resolution 기술 검토, Anomaly Detection 경량화 등 여러 직무를 수행했다.&lt;/p&gt; &lt;p&gt;이렇게 하다 보니 느낀 것이 나는 새로운 모델을 개발하고 성능을 높이는 것보다는 기존에 있는 모델을 memory, inference time, energy consumption 줄여가며 서빙하고 개인화하는 것에 관심이 많다는 것을 깨달았다. 그래서 low-precision, PEFT, hardware optimization, neuromorphic engineering 등에 관심을 두게 되었고 산업계로 가야겠다는 확신이 들었다.&lt;/p&gt; &lt;p&gt;자세한 이야기는 &lt;a href=&quot;https://www.wonbeomjang.kr/blog/2023/startup-research-intern-review/&quot;&gt;스타트업 리서치 인턴 후기&lt;/a&gt;에서 볼 수 있다.&lt;/p&gt; &lt;h1 id=&quot;발리-서핑캠프-82692&quot;&gt;발리 서핑캠프 (8/26~9/2)&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2023-review/Pasted image 20240103224106.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;취미다운 취미를 찾고자 발리에 서핑 캠프를 다녀왔다. 1주 동안 내 나쁜 습관을 고치기에는 짧은 시간이었지만 그래도 재밌게 탔다. 시간만 허락한다면 발리에서 몇 달 살면서 서핑만 하고 싶다.&lt;/p&gt; &lt;h1 id=&quot;복학-및-취준-941221&quot;&gt;복학 및 취준 (9/4~12/21)&lt;/h1&gt; &lt;p&gt;휴학을 하고 놀까 아니면 빨리 졸업을 할까 고민을 하다가 빨리 취준을 시작하지 않으면 채용문이 더 닫힐 것 같다는 생각이 들어서 복학하기로 했다. 그래도 11학점만 들으면 돼서 여행과 취미활동을 하면서 여유롭게 다녔고, 취준도 시작했다. 이전부터 ‘나의 무엇을 기업에게 팔면 기업은 나를 채용할까?’라는 생각을 갖고 있었고, 나의 강점을 어필하려고 많은 노력을 한 것 같다. 다음은 내가 에브리타임에 썼던 취업 후기이다.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;사실 1학년 때부터 퍼스널브랜딩과 커리어를 쌓는 것에 관심이 많아서 미리 준비를 많이 한 것 같아요. 취준하면서 느낀 것이 단순히 스펙이 얼마나 좋고, 어학이 얼마나 좋냐 그런 것이 중요한 게 아닙니다. ‘나’가 누구인지, 다른 지원자와 비교했을 때 어떠한 강점을 가지고 있는지, 이를 생각하고 지금까지 어떠한 커리어를 쌓았는지가 중요한 것 같아요. 퍼스널 브랜딩이랄까…. 그래서 저는 다음을 3가지를 강조했습니다.&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;사용자에 대한 이해: 디자이너, 기획자 등과 함께 여러 사용자를 대상으로 서비스를 기획하고 개발하고 수상 경험 있음&lt;/li&gt; &lt;li&gt;실용적인 인공지능 개발자: 그저 인공지능 모델 성능 높이기에 집중한 것이 아닌 inference time을 유지하거나 줄이면서 성능을 높이는 것에 관심이 많음. 그래서 경량화 하드웨어 최적화를 매 프로젝트에 적용하려 함.&lt;/li&gt; &lt;li&gt;연구능력: 딥러닝 스타트업에서 리서치 인턴을 하면서 제품의 성능을 높이고 학습시간을 줄임.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;석사랑 비교했을 때 연구능력을 주되게 어필하기에는 어려울 것 같아서, 실용적인 연구, 사용자를 위한 연구개발로 밀고갔네요. 이런 걸 정리하니까 제 컨셉이 명확해졌고, 기업의 호불호 때문에 job description이 안 맞으면 바로 서탈이었지만 서류랑 코테 붙으면 큰 걱정 없이 최종까지 간 것 같아요.&lt;/p&gt; &lt;/blockquote&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2023-review/5230885D-4877-43F1-BE47-B9B51AAD363A_1_105_c.jpeg&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;결과론적으로 6개 회사에 지원해서 4개는 서류탈락, 2개가 최종면접까지 갔고, SK텔레콤에 합격하게 되었다. 마음에 드는 기업에 갔으니 나름 성공한 인생 아닐까 싶다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2023-review/Pasted image 20240102221917.png&quot; width=&quot;40%&quot; /&gt;&lt;/p&gt; &lt;p&gt;그리고 마지막 시험을 보고 성적이 나왔는데 괜찮게 나왔다. 그래서 전체 평점은 4.45, 전공은 4.46으로 나름 성공적인 대학생활을 보낸 것 같다.&lt;/p&gt; &lt;h1 id=&quot;끄라비-암벽등반-여행-12211231&quot;&gt;끄라비 암벽등반 여행 (12/21~12/31)&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2023-review/Pasted image 20240102222158.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;편하게 여행 갈 수 있는 것은 마지막이라고 생각하여 끄라비로 바로 암벽등반 여행을 떠났다. 여기서도 인생 첫 5.11c/6c+를 달성해서 기뻤다.&lt;/p&gt; &lt;h1 id=&quot;sk-신입구성원-과정-12&quot;&gt;SK 신입구성원 과정 (1/2~)&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2023-review/Pasted image 20240103231802.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;현재는 SKT 신입연수를 받고 있다. 오늘 하루이지만 그래도 SKT에 잘 왔다는 생각이 든다.&lt;/p&gt; &lt;h1 id=&quot;앞으로의-계획&quot;&gt;앞으로의 계획&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; 올해 목표: 발전을 위한 초석 다지기 &lt;/p&gt; &lt;p&gt;현재는 첫 직장에 적응하고 빠르게 흡수하는 것이 최우선이다. 하지만 3년 뒤에 회사 내적으로나 외적으로 더 큰 기회가 올 것이라고 확신하고 있고 이를 준비해야 한다. 따라서 올해 목표는 기회를 위하여 발전의 초석 다지기를 해야겠다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;논문 꾸준히 읽기&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;이건 너무 당연해서 생략&lt;/p&gt; &lt;p&gt;&lt;strong&gt;수학공부&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;FlashAttention을 보면서 인공지능 최적화를 위해서는 엄밀한 증명이 필요한 분야가 많다는 것을 느꼈다. 앞으로도 이런 분야가 있을 것으로 생각된다. 따라서 학과과정에서 아주 기본적인 수학만 겉핥기식으로 했지만 이제는 따로 제대로 해야겠다는 생각이 들었다.&lt;/p&gt; &lt;p&gt;빠르게 학습해야 할 것&lt;/p&gt; &lt;ul&gt; &lt;li&gt;선형대수&lt;/li&gt; &lt;li&gt;수리통계학&lt;/li&gt; &lt;li&gt;표본론 아마 이렇게만 잡아도 1년이 빠듯할 것 같다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;코딩공부&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;내가 개발자이긴 한데 컴퓨터비전만 해서 뭔가 애매한 느낌이 든다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;개발을 잘하는가? (X)&lt;/li&gt; &lt;li&gt;개발을 못하는가? (X)&lt;/li&gt; &lt;li&gt;알고리즘을 잘하는가? (X)&lt;/li&gt; &lt;li&gt;프로그램 아키텍처 설계를 잘할 줄 아는가? (X)&lt;/li&gt; &lt;li&gt;디자인패턴에 대해서 아는가? (X) &lt;del&gt;내가 컴공이 맞긴 한가….?&lt;/del&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;너무 개발자가 아닌 느낌이 들어서 이번 년도에는 디자인패턴과 서버, 클라우드를 공부할 것 같다. (CUDA 공부가 제일 급해서 먼저 할 듯…)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;하드웨어 공부&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;앞으로 수많은 NPU들이 나오고 이를 고려한 프로그래밍이 필요할 것이다. 물론 최적화하는 솔루션이 있지만 Edge TPU를 위한 네트워크가 따로 있듯 하드웨어에 대한 이해가 꼭 필요할 것이다.&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; </description> <pubDate>Wed, 27 Dec 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/2023-review/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/2023-review/</guid> <category>daily</category> <category>self-reflection</category> <category>daily</category> <category>self-refection</category> </item> <item> <title>Keras 3.0 설명</title> <description>&lt;h1 id=&quot;why&quot;&gt;Why?&lt;/h1&gt; &lt;p&gt;과거 keras는 Tensorflow, Theano, MXNet등 여러 deep learning backend framework가 있을 때 multi-backend 지원의 강점을 가지며 출시되었다. 하지만 Teano와 MXNet등 여러 framework들은 쇄퇴의 길을 걸었고, tensorflow만 살아남게 되었다. 하지만 당시 tensorflow도 문제점은 가지고 있었다. 그것은 model 선언이 비직관적이라는 것이다. 따라서 tensorflow는 keras를 공식 레포에 집어넣어 keras.layer로 모델을 만들고 tensorflow backend로 학습하는 구조로 발전했다.&lt;/p&gt; &lt;p&gt;하지만 현재는 여러 연구에서 pytorch를 사용하고, pytorch 기반의 huggingface가 등장하면서 keras입장에서는 pytorch가 매력적인 시장으로 보였다. 그리고 tensorflow의 사용자가 줄어가고, 윈도우 네이티브 업데이트 지원을 종료하면서 다시 multi-backend의 강점을 다시 살리기로 했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;그래서 keras는 다음과 같은 기능을 강조하면서 keras3.0을 출시했다.&lt;/p&gt; &lt;h1 id=&quot;주요-기능&quot;&gt;주요 기능&lt;/h1&gt; &lt;p&gt;Keras 3.0을 출시하면서 다음과 같은 중요한 기능을 제시했다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The full Keras API, available for TensorFlow, JAX, and PyTorch&lt;/li&gt; &lt;li&gt;A cross-framework low-level language for deep learning&lt;/li&gt; &lt;li&gt;Seamless integration with native workflows in JAX, PyTorch, and TensorFlow&lt;/li&gt; &lt;li&gt;Support for cross-framework data pipelines with all backends&lt;/li&gt; &lt;li&gt;A new distribution API for large-scale data parallelism and model parallelism&lt;/li&gt; &lt;li&gt;Pretrained models&lt;/li&gt; &lt;li&gt;Progressive disclosure of complexity&lt;/li&gt; &lt;li&gt;A new stateless API for layers, models, metrics, and optimizers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;하나씩 살펴보자&lt;/p&gt; &lt;h2 id=&quot;the-full-keras-api-available-for-tensorflow-jax-and-pytorch&quot;&gt;The full Keras API, available for TensorFlow, JAX, and PyTorch&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;keras로 선언된 모델과 함수들은 tensorflow, jax, pytorch에서 모두 사용가능하다. 즉, 3개의 프레임워크에서 모두 keras 함수를 사용할 수 있다는 것이다. 여기서 재밌는 점은 기존에 tf.keras로 선언된 모델도 jax, pytorch에서 실행 가능하다.&lt;/p&gt; &lt;h2 id=&quot;a-cross-framework-low-level-language-for-deep-learning&quot;&gt;A cross-framework low-level language for deep learning&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;딥러닝 모델을 구성하다보면 matmul, stack 등 기본적인 연산자가 필요할 때 있다. 이럴때는 keras.ops를 사용하여 기본적인 연산자를 구성하면 tensorflow, jax, pytorch에서 모두 사용가능하다. 이 떄 keras는 두 가지를 중심으로 구현했다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Numpy에 관련한 연산자는 모두 구현한다. ex) ops.matmul, ops.sum, ops.stack, ops.einsum&lt;/li&gt; &lt;li&gt;Neural-specific function을 구현한다. ex) ops.softmax, ops.binary_crossentropy, ops.conv&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;seamless-integration-with-native-workflows-in-jax-pytorch-and-tensorflow&quot;&gt;Seamless integration with native workflows in JAX, PyTorch, and TensorFlow&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_5.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Integration하다보면 기존의 training loop 등 workflow를 그대로 유지해야할 경우가 있다. 물론 keras3.0은 이 경우도 지원한다.&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;Write a low-level JAX training loop to train a Keras model using an optax optimizer, jax.grad, jax.jit, jax.pmap.&lt;/li&gt; &lt;li&gt;Write a low-level TensorFlow training loop to train a Keras model using tf.GradientTape and tf.distribute.&lt;/li&gt; &lt;li&gt;Write a low-level PyTorch training loop to train a Keras model using a torch.optim optimizer, a torch loss function, and the torch.nn.parallel.DistributedDataParallel wrapper.&lt;/li&gt; &lt;li&gt;Use Keras layers in a PyTorch Module (because they are Module instances too!)&lt;/li&gt; &lt;li&gt;Use any PyTorch Module in a Keras model as if it were a Keras layer.&lt;/li&gt; &lt;li&gt;etc.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;a-new-distribution-api-for-large-scale-data-parallelism-and-model-parallelism&quot;&gt;A new distribution API for large-scale data parallelism and model parallelism&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_7.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_8.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;keras에서는 여러 data parallelism을 제공한다. 단 두줄 만으로 분산학습이 된다는게 신기하긴 하다.&lt;/p&gt; &lt;h2 id=&quot;support-for-cross-framework-data-pipelines-with-all-backends&quot;&gt;Support for cross-framework data pipelines with all backends&lt;/h2&gt; &lt;p&gt;각 framework별로 다른 dataset 객체를 사용한다. Keras3.0은 이를 모두 지원한다.&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;tf.data.Dataset pipelines: the reference for scalable production ML.&lt;/li&gt; &lt;li&gt;torch.utils.data.DataLoader objects.&lt;/li&gt; &lt;li&gt;NumPy arrays and Pandas dataframes.&lt;/li&gt; &lt;li&gt;Keras’s own keras.utils.PyDataset objects.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;pretrained-models&quot;&gt;Pretrained models&lt;/h2&gt; &lt;p&gt;Keras3.0은 다음과 같은 pretrained model을 지원한다.&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;BERT&lt;/li&gt; &lt;li&gt;OPT&lt;/li&gt; &lt;li&gt;Whisper&lt;/li&gt; &lt;li&gt;T5&lt;/li&gt; &lt;li&gt;StableDiffusion&lt;/li&gt; &lt;li&gt;YOLOv8&lt;/li&gt; &lt;li&gt;SegmentAnything&lt;/li&gt; &lt;li&gt;etc.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;progressive-disclosure-of-complexity&quot;&gt;Progressive disclosure of complexity&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_6.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;개발하다보면 pytorch lightening, pytorch ignite, tensorflow orbit등 disclosure를 위한 툴을 쓴 경험이 있을 것이다. Keras는 이것이 keras api의 핵심 디자인으로 삼았으며 이를 지원한다 한다. &lt;del&gt;그냥 다른거 쓸 것 같긴한데…&lt;/del&gt;&lt;/p&gt; &lt;h2 id=&quot;a-new-stateless-api-for-layers-models-metrics-and-optimizers&quot;&gt;A new stateless API for layers, models, metrics, and optimizers.&lt;/h2&gt; &lt;p&gt;함수형 프로그래밍을 좋아하는 사람을 위해 stateless한 함수들을 만들었다.&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;All layers and models have a stateless_call() method which mirrors &lt;strong&gt;call&lt;/strong&gt;().&lt;/li&gt; &lt;li&gt;All optimizers have a stateless_apply() method which mirrors apply().&lt;/li&gt; &lt;li&gt;All metrics have a stateless_update_state() method which mirrors update_state() and a stateless_result() method which mirrors result().&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h1 id=&quot;example&quot;&gt;Example&lt;/h1&gt; &lt;p&gt;Tensorflow는 기존의 방법과 동일해서 설명을 생략하겠다.&lt;/p&gt; &lt;h3 id=&quot;mnist-with-keras-vgg19-pytorch-beckend&quot;&gt;MNIST with keras vgg19 (Pytorch Beckend)&lt;/h3&gt; &lt;script src=&quot;https://gist.github.com/wonbeomjang/e935128f7f55045ab2d08e091cc2b8e2.js&quot;&gt;&lt;/script&gt; &lt;p&gt;이렇게 하면 기존 tensorflow나 keras vgg를 weight를 포함하여 사용할 수 있다. 여기서 주의할 점은 dataset augmentation 부분에서 CHW를 HWC로 바꿔줘야한다는 것이다.&lt;/p&gt; &lt;h3 id=&quot;declare-pytorch-model-using-keras-application&quot;&gt;Declare Pytorch Model Using Keras Application&lt;/h3&gt; &lt;script src=&quot;https://gist.github.com/wonbeomjang/c76b1da2952d231e209a0d03896c4aef.js&quot;&gt;&lt;/script&gt; &lt;p&gt;재밌는 것은 keras.layer가 torch.nn.Module과 호환이되어 다음과 같이 모델을 선언할 수 있다.&lt;/p&gt; &lt;h1 id=&quot;맺으며&quot;&gt;맺으며&lt;/h1&gt; &lt;p&gt;너무 많은 담기 그래서 이쯤으로 마치고, 더 많은 예제는 다음에 다루기로 하겠다.&lt;/p&gt; </description> <pubDate>Sun, 03 Dec 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/keras-3/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/keras-3/</guid> <category>framework</category> <category>framework</category> </item> <item> <title>What Makes Multi-modal Learning Better than Single (Provably)</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;우리 세상은 많은 modality가 존재한다. 그리고 관념적으로도 여러 modal의 네트워크들을 fusion시키면 uni-modal보다 성능이 더 좋게 나온다. 그렇다면 우리는 이러한 궁금증이 생긴다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; _multi-modal learning이 uni-modal learning보다 좋은 성능을 제공할까?_ &lt;/p&gt; &lt;p&gt;저자는 이 궁금증에서 연구를 시작했고, 다음 두 가지를 중점적으로 살펴봤다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;(When) 어떤 상황에서 multi-modal이 uni-modal 보다 성능이 좋은가?&lt;/li&gt; &lt;li&gt;(Why) 무엇이 이런 성능을 유도했는가?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;그리고 연구를 통해서 저자가 한 comtribution은 다음과 같다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-modal learning을 population risk로 설명하고, 이는 latent representation quality의 bound 되어있다는 것을 밝혔다.&lt;/li&gt; &lt;li&gt;전체 modality의 subset으로 훈련시킨 network의 quality의 upper bound를 유도했다.&lt;/li&gt; &lt;li&gt;Modality의 subset으로 학습시키면 성능이 하락한다는 것을 이론적으로 분석했다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;참고로 결론은 다음과 같다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multiple modality는 그 modal의 subset보다 적은 population risk를 갖는다.&lt;/li&gt; &lt;li&gt;이는 multi-modal이 더 정확한 latent space representation을 학습할 수 있다는 것이다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;이제 하나씩 살펴보자.&lt;/p&gt; &lt;h1 id=&quot;the-multi-modal-learning-formulation&quot;&gt;The Multi-modal Learning Formulation&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/figure1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;먼저 수식을 정리하자. K개의 modalities에 대해서 data는 \(\mathbb{x}:=(x^{(1)},\cdots,x^{(K)})\) 으로 표현한다. 이 때 \(x^{(k)} \in \mathcal{X}^{(k)}\) 이다. 우리는 K개의 modalities를 보유하기 때문에 전체 input data space는&lt;/p&gt; \[\mathcal{X}=\mathcal{X}^{1} \times \cdots \times \mathcal{X}^{k}\] &lt;p&gt;로 표현된다. 그리고 target domain을 \(\mathcal{Y}\) , multi-modal의 공통된 latent space를 \(\mathcal{Z}\) 라 하자. 우리는 이제 true mapping을 다음과 같이 쓸 수 있다.&lt;/p&gt; \[g^\star: \mathcal{X} \mapsto \mathcal{Z}, g^\star \in \mathcal{G}\] \[h^\star: \mathcal{Z} \mapsto \mathcal{Y}, h^\star \in \mathcal{H}\] &lt;p&gt;그렇다면 이제 우리는 \(\mathbb{x}\) 의 data distribution을 정의할 수 있다.&lt;/p&gt; \[\mathbb{P}_\mathcal{D}(\mathbb{x},y)\triangleq\mathbb{P}_{y|x}(y|h^\star\circ g^\star(\mathbb{x}))\mathbb{P}_\mathbb{x}(\mathbb{x})\] &lt;p&gt;참고로 \(h^\star\circ g^\star(\mathbb{x})=h^\star(g^\star(\mathbb{x}))\) 로 합성함수를 의미한다.&lt;/p&gt; &lt;p&gt;우리는 일반화를 위해 \(\mathcal{N} \leq \mathcal{M}\) 인 modalitie의 subset에 대해서 살펴볼 것이다. Modality의 superset을 정의하자.&lt;/p&gt; \[\mathcal{X}^\prime := (\mathcal{X}^{(1)}\cup\bot)\times\cdots\times(\mathcal{X}^{(K)}\cup\bot)\] &lt;p&gt;이때, \(\bot\) 은 k번째의 modality는 쓰지 않는다는 것이다. 간단하게 시각화하면 다음과 같다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/img1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;이제 modalities를 선택하는 함수를 정의하자.&lt;/p&gt; \[p_\mathcal{M}(\mathbb{x})^{(k)}= \begin{cases} \mathbb{x}^{(k)} \text{ if } k\in\mathcal{M} \\ \bot \text{ else } \end{cases}\] &lt;p&gt;이때, 우리는 다음과 같은 식을 만들수도 있다. \(p^\prime_\mathcal{M} := \mathcal{X}^\prime\mapsto\mathcal{X}^\prime\) 우리의 목표는 Empirical Risk Minimization (ERM) principle에 따라서 learning objective를 minimize하는 것이다.&lt;/p&gt; \[\text{min } \hat{r}(h\circ g_\mathcal{M} \triangleq\frac{1}{m}\sum_{i=1}^ml(h\circ g_\mathcal{M}(\mathbb{x}_i),y_i) \text{ s.t. } h \in \mathcal{H}, g_\mathcal{M} \in \mathcal{G}\] &lt;p&gt;여기서 \(l\) 은 loss fuction이고, 최종적으로 정의하는 population risk는 다음과 같다.&lt;/p&gt; \[r(h\circ g_\mathcal{M})=\mathbb{E}_{(\mathbb{x}_i, y_i)\sim\mathcal{D}}[\hat{r}(h\circ g_\mathcal{M})]\] &lt;h1 id=&quot;main-result&quot;&gt;Main Result&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Definition 1.&lt;/strong&gt; Given a data distribution with the form in (1), for any learned latent representation mapping \(g \in \mathcal{G}\) , the &lt;em&gt;latent representation quality&lt;/em&gt; is defined as&lt;/p&gt; \[\eta(g)=\text{inf}_{h\in\mathcal{H}}[r(h\circ g)-r(h(h^*\circ g^*))]\] &lt;/blockquote&gt; &lt;p&gt;즉, \(\eta(g))\) 는 mapping function의 \(g \in \mathcal{G}\) 에 대해서 true latent space와 차이이기 때문에 latent space quality라고 할 수 있다.&lt;/p&gt; &lt;h3 id=&quot;rademacher-complexity&quot;&gt;Rademacher complexity&lt;/h3&gt; &lt;p&gt;이제 model complexity를 측정하는 Rademacher complexity에 대해서 알아보자. \(\mathcal{F}\) 를 \(\mathbb{R}^d \mapsto \mathbb{R}\) 인 vector-valued function으로 정의하자. \(\mathbb{R}^d\) 에서 iid 한 \(Z_1,...,Z_m\) 에 대해 sample를 \(S=(Z_1,...,Z_m)\) 라고 하자. Empirical Rademacher complexity는 다음과 같이 정의된다.&lt;/p&gt; \[\hat{\mathfrak{R}}_S(\mathcal{F}):=\mathbb{E}_\sigma[ \underset{f\in\mathcal{F}}{\text{sup}}\frac{1}{m}\sum_{i=1}^m\sigma_if(Z_i)]\] &lt;p&gt;이 때, \(\sigma=(\sigma_1,...,\sigma_n)^\top\) with \(\sigma_i \sim \text{unif}\{-1, 1\}\) 이다. 전체적인 Rademacher complexity은 다음과 같다.&lt;/p&gt; \[\mathfrak{R}_S(\mathcal{F})=\mathbb{E}[\hat{\mathfrak{R}}_S(\mathcal{F})]\] &lt;p&gt;이해하기 어려우니 다른 블로그의 설명을 인용하겠다.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;Rademacher complexity가 1이라는 것은 모델이 위와 같은 random한 setup에서도 잘 fitting 했다는 것이므로, complexity가 크고 따라서 generalize를 잘 못할 것이라고 이야기 할 수 있다는 개념이다. &lt;a href=&quot;https://yun905.tistory.com/68&quot;&gt;https://yun905.tistory.com/68&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;connection-to-latent-representation-quality&quot;&gt;Connection to Latent Representation Quality&lt;/h2&gt; &lt;p&gt;이제 latent space quality와 population risk의 관계를 살펴보자.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;. Let \(S = ((x_i,y_i))^m_{i=1}\) be a dataset of m examples drawn i.i.d. according to \(\mathcal{D}\) . Let M, N be two distinct subsets of [ \(K\) ]. Assuming we have produced the empirical risk minimizers \((\hat{h}_\mathcal{M}, \hat{g}_\mathcal{M})\) and \((\hat{h}_\mathcal{N}, \hat{g}_\mathcal{N})\) , training with the \(\mathcal{M}\) and \(\mathcal{N}\) modalities separately. Then, for all \(1 &amp;gt; \delta &amp;gt; 0\) , with probability at least \(1-\frac{\delta}{2}\) :&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;\(r(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}) - r(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}) \leq \gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N})+8L\mathfrak{R}(\mathcal{H} \circ \mathcal{G}_{\mathcal{M}})+\frac{4C}{\sqrt{m}}+2C\sqrt{\frac{2\text{ln}(2/\delta)}{m}}\)$&lt;/p&gt; \[\text{where}, \gamma_S(\mathcal{M},\mathcal{N})\triangleq\eta(\hat{g}_\mathcal{M})-\eta(\hat{g}_\mathcal{N})\] &lt;p&gt;즉, population risk의 차이는 latent space quality 차이와 model complexity에 upper bound가 된다는 것이다. 이는 그대로 사용하지 않고, 추후에 식 정리할 때 사용할 것이다. 여기서 sample size \(m\) 에 대해 \(\mathfrak{R}_S(\mathcal{F})\) 은 보통 \(\sqrt{C(\mathcal{F})/m}\) 에 bound된다. 따라서 우리는 다음과 같이 다시 쓸 수 있다.&lt;/p&gt; \[r(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}) - r(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}) \leq \gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N})+\text{O}(1/m)\] &lt;h2 id=&quot;upper-bound-for-latent-space-exploration&quot;&gt;Upper Bound for Latent Space Exploration&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Theorem 2&lt;/strong&gt;. Let \(S={(x_i, y_i)}^m_{i=1}\) be a dataset of m examples drawn i.i.d. according to D. Let M be a subset of [ \(K\) ]. Assuming we have produced the empirical risk minimizers \((\hat{h}_\mathcal{M}, \hat{g}_\mathcal{M})\) training with the M modalities. Then, for all \(1 &amp;gt; \delta &amp;gt; 0$, with probability at least\)1 − \delta$$ :&lt;/p&gt; &lt;/blockquote&gt; \[\eta(\hat{g}_{\mathcal{M}})\leq 4L\mathfrak{R}(\mathcal{H} \circ \mathcal{G}_{\mathcal{M}})+4L\mathfrak{R}(\mathcal{H} \circ \mathcal{G})+6C\sqrt{\frac{2\text{ln}(2/\delta)}{m}}+\hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S)\] \[\text{where } \hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S) \triangleq \hat{r}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}})-\hat{r}(h^\star\circ g^\star)\] &lt;p&gt;위에서 처럼 Rademacher complexity은 \(O(1/m)\) 이기 때문에&lt;/p&gt; \[\eta(\hat{g}_{\mathcal{M}})\leq \hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S)+\text{O}(1/m)\] &lt;p&gt;이 성립한다. 이 때, assumption 3에 의해&lt;/p&gt; \[\hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S) \leq \hat{L}(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}, S)\] &lt;p&gt;이 성립한다.&lt;/p&gt; &lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt; &lt;p&gt;&lt;em&gt;그렇다면 언제 multi-modal을 사용해야하냐?&lt;/em&gt;&lt;/p&gt; \[\hat{L}(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}, S) - \hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S) \geq \sqrt{\frac{C(\mathcal{H}\circ\mathcal{G}_\mathcal{M})}{m}}-\sqrt{\frac{C(\mathcal{H}\circ\mathcal{G}_\mathcal{N})}{m}}\] &lt;p&gt;저자는 다음과 같이 말한다.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;(i) When the number of sample size m is large, the impact of intrinsic complexity of function classes will be reduced. (ii) Using more modalities can efficiently optimize the empirical risk, hence improve the latent representation quality.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Sample size m이 충분히 클 때 Theorem 1에 적용하면 다음과 같은 식이 성립한다.&lt;/p&gt; &lt;p&gt;\(\gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N})= \eta(\hat{g}_{\mathcal{M}}) - \eta(\hat{g}_{\mathcal{N}})\leq \hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S) - \hat{L}(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}, S) \leq 0\)$&lt;/p&gt; \[r(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}) \leq r(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}})\] &lt;p&gt;즉, 데이터셋의 크기가 클 때 modality의 수가 많은 것을 사용하는 것이 좋다.&lt;/p&gt; &lt;h2 id=&quot;non-positivity-guarantee&quot;&gt;Non-Positivity Guarantee&lt;/h2&gt; &lt;p&gt;sample size s가 클 때 \(\gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N})\) 이 non-positive라는 것을 증명할 수 있다. 이것의 증명은 여기서 다루지 않겠다.&lt;/p&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;p&gt;이제 실험을 보자. Dataset으로는 Interactive Emotional Dyadic Motion Capture (IEMO- CAP) database을 사용했다. 이 데이터셋에는 여러 모달에 대해서 여러 사람이 대화하는 것이 들어있으며 발화자가 누구인지 맞추는 것이 목표이다. 여기에는 Text, Video, Audio 정보가 들어가있다.&lt;/p&gt; &lt;h2 id=&quot;number-of-modalities&quot;&gt;Number of Modalities&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/table1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Modal이 늘어날 수록 정확도가 상승하는 것을 볼 수 있다.&lt;/p&gt; &lt;h2 id=&quot;number-of-samples&quot;&gt;Number of Samples&lt;/h2&gt; &lt;p&gt;위에서 sample의 수가 클 때 multi-modal이 좋다고 했다. 따라서 이를 살펴보자.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/table2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;여기서 볼 수 있듯, sample의 수가 줄어들면 madality의 수가 적을 때 성능이 좋은 경우가 있다.&lt;/p&gt; &lt;h2 id=&quot;quality-of-latent-spaces&quot;&gt;Quality of Latent Spaces&lt;/h2&gt; &lt;p&gt;multi-modal은 latent space quality가 좋다고 했다. 이를 확인해보자.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/table3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Sample의 수와 modal의 수로 비교해도 같은 결과를 낸다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/figure2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;synthetic-data&quot;&gt;Synthetic Data&lt;/h2&gt; &lt;p&gt;실제 데이터에서 sample의 수가 많을 때 multi-modal이 좋다는 것을 확인했다. 인공데이터는 어떨까?&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/table4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;저자가 만든 인공데이터도 같은 모습을 보였다.&lt;/p&gt; </description> <pubDate>Sun, 19 Nov 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/multimodal-vs-unimodal/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/multimodal-vs-unimodal/</guid> <category>multi-modal</category> <category>paper</category> <category>multi-modal</category> <category>paper</category> </item> <item> <title>스타트업 리서치 인턴 후기</title> <description>&lt;h1 id=&quot;왜-시작했나요&quot;&gt;왜 시작했나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/img.jpeg&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; *&apos;Researcher일까? Engineer일까?&apos;* &lt;/p&gt; &lt;p&gt;나는 대학에 다니면서 항상 그런 고민을 했다. 고등학생 때 우연히 DeepMind에서 발표한 Playing Atari with Deep Reinforcement Learning이라는 논문을 보게되었고 인공지능에 빠져들었다. 대학 와서는 computer vision을 공부하게 되었다. 그저 인공지능이 좋아 backend, frontend 등 다른 분야보다는 인공지능 공부와 개발만 하게 되었다. 그러다 대학을 졸업할 때가 되었고, researcher와 engineer를 선택해야 할 순간이 다가왔다.&lt;/p&gt; &lt;p&gt;불행인지 다행인 건지 중앙대학교에서는 인턴을 해야지 졸업을 할 수 있었고, 관심 있는 두 군데 스타트업에 접촉하여 그중 한 회사인 뉴로클에서 인턴을 진행하게 되었다. 뉴로클을 선택한 이유는 간단했다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Computer Vision을 중점으로 한다.&lt;/li&gt; &lt;li&gt;자체 서비스를 판매하고 있다.&lt;/li&gt; &lt;li&gt;기업매출을 보니 매출도 성장세였고, 흑자를 내기 시작했다.&lt;/li&gt; &lt;li&gt;내가 내 일을 할 수 있고, 주체적으로 일할 수 있는 규모가 작지도 않고 크지도 않는 회사이다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;결론적으로 이러한 예측이 맞았고 성공적으로 인턴을 만들 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;무엇을-했나요&quot;&gt;무엇을 했나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/img.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;기본적으로 리서치 인턴의 역할을 수행했으나 후반에는 리서치 엔지니어의 역할을 하게 되었다. 퍼포먼스가 좋아서 그런지 생각보다 많은 일을 하게 되었다. (외부에 공개적으로 자료가 나간 것들만 포함했다)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Pretrained-OCR: OCR Auto-labeling 기능 추가 및 OCR 모델 성능 향상. 학습 속도 기존의 30%, 정확도 4%p 향상&lt;/li&gt; &lt;li&gt;Smart labeling (segmentation): 기술 검토 및 테스트, 모델 변환&lt;/li&gt; &lt;li&gt;Smart labeling (object detection): 기술 검토 및 모델 변환&lt;/li&gt; &lt;li&gt;회사 블로그 제작&lt;/li&gt; &lt;li&gt;리서치팀 docker 등 개발환경 관리&lt;/li&gt; &lt;li&gt;(방향성만 제시했지만) Neuro-I 성능개선, 다른 사람 연구 해결책 찾기 등등…&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;8개월동안 이걸 다 했다고?? 놀랍게도 그렇다. 개발 base로 computer vision을 공부했다 보니 구현 속도와 실험 속도가 압도적으로 빠른 것 같다.&lt;/p&gt; &lt;h1 id=&quot;무슨-경험이-도움이-되었나요&quot;&gt;무슨 경험이 도움이 되었나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/project.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;동아리에서 공모전에서 프로젝트 경험이 많았고 협업도 많이 진행했다. 그래서 computer vision, 선형대수, 수치해석, 위상수학, 표 본론 등 과 같은 지식뿐만 아니라 tensorrt, onnx, quantization 등 많은 기술, pandas, matplotlib, seaborn과 같은 데이터 시각화, 딥러닝 모델이 제품에 어떻게 탑재해야 하는지에 대한 감도 있었다. 이 모든 경험을 회사에서 다 썼다. (진짜 다 썼다) 이러한 다양한 경험은 여러 기능에 기여를 할 수 있었던 것 같다.&lt;/p&gt; &lt;h1 id=&quot;무엇을-얻었나요&quot;&gt;무엇을 얻었나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/company.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;협업&quot;&gt;협업&lt;/h2&gt; &lt;p&gt;학생 때와 차원이 다른 협업을 하게 되었다. 학교에는 기껏해야 backed, fronted고 인원도 적어서 체계도 없이 작업을 해도 되었다. 하지만 인턴을 하면서 backed, fronted뿐만 아니라 영업, 마케팅, backbend, 기획 등 여러 사람과 협업을 진행했다.&lt;/p&gt; &lt;h3 id=&quot;요구사항을-명확하게-하자&quot;&gt;요구사항을 명확하게 하자&lt;/h3&gt; &lt;p&gt;협업은 기본적으로 background가 완전하게 동일하지 않은 사람들끼리 작업을 한다. 따라서 같은 목표를 바라보고있어도 세부 사항이 다를 수 있다. 만약 이를 조정하지 않고 일을 진행하다 보면 다음에 다시 조정하고 어려울뿐더러 비용 역시 많이 발생한다.&lt;/p&gt; &lt;h3 id=&quot;방향성-설정&quot;&gt;방향성 설정&lt;/h3&gt; &lt;p&gt;하나의 기능이 만들어지기 위해서 다음과 같은 과정을 거친다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;새로운 연구 주제로 연구한다.&lt;/li&gt; &lt;li&gt;기획 및 디자인팀에서 제품 탑재 방향을 결정한다.&lt;/li&gt; &lt;li&gt;개발팀에서 제품을 개발한다.&lt;/li&gt; &lt;li&gt;마케팅팀에서 협력사에 제공할 데이터와 대외 홍보용 자료를 제작한다.&lt;/li&gt; &lt;li&gt;영업을 통해 제품을 판매한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;라이프 사이클에서 연구는 최상단을 차지한다. 따라서 첫 단추가 잘못 채워지면 전체적인 제품 방향이 엇나갈 수 있다.&lt;/p&gt; &lt;h3 id=&quot;문서화를-체계적으로-하자&quot;&gt;문서화를 체계적으로 하자&lt;/h3&gt; &lt;p&gt;내가 하는 연구를 follow up 하는 사람은 극소수다. 연구 이후 제품화할 때 조직되는 관련자들은 내가 작성해 놓은 document를 보고 작업을 시작한다. 그래서 진행한 연구와 모델을 제대로 이해시키려면 구조적으로 잘 문서화를 해야 한다.&lt;/p&gt; &lt;h2 id=&quot;지식&quot;&gt;지식&lt;/h2&gt; &lt;h2 id=&quot;tensorflow&quot;&gt;Tensorflow&lt;/h2&gt; &lt;p&gt;나는 지금까지 pytorch를 이용하여 작업을 했다. Tensorflow는 회사 들어와서 거의 처음 쓰게 된 것이다. Tensorflow는 기본적으로 eager mode가 제공되지 않아 코딩하는 것이 힘들었지만 tensorflow와 tensorflow orbit을 익히게 되는 좋은 기회가 되었다.&lt;/p&gt; &lt;h3 id=&quot;논문&quot;&gt;논문&lt;/h3&gt; &lt;p&gt;회사에 들어와서 논문을 진짜 많이 읽었다. 연구에 기반이 되는 논문뿐만 아니라 적용할 만한 최신논문, 기술 리포트, 워크숍 논문 등 다양하게 많이 읽었다. 이를 통해 ViT, active learning, OCR, anomaly detection, super resolution, backbone for edge device, federated learning 등 다양한 분야에 대해 기초지식을 쌓을 수 있었다.&lt;/p&gt; &lt;h3 id=&quot;데이터-분석&quot;&gt;데이터 분석&lt;/h3&gt; &lt;p&gt;기본적으로 데이터가 부족한 상황에서 모델의 성능을 올리는 방법을 고민을 했다. 이 때문에 사용자가 다룰 예상데이터의 특성을 분석하고 데이터에 적합한 방법론을 사용하여 성능을 높일 수 있었다.&lt;/p&gt; &lt;h2 id=&quot;일은-잘했나요&quot;&gt;일은 잘했나요?&lt;/h2&gt; &lt;p&gt;나에 대한 평가가 긍정적인 것을 보면 일을 잘했던 것 같다. 무엇보다도 나랑 같이 인턴을 진행한 분과 잘하는 것이 달라서 서로 시너지가 났던 것 같다. 원래 회사에도 리서치 인턴이 없었는데 인턴 둘이 좋은 선례를 만들어서 앞으로 계속 채용할 예정이다. (사실 면접이 완료되어 다음 리서치 인턴도 정해졌다.)&lt;/p&gt; &lt;h1 id=&quot;앞으로-무엇을-할-것인가요&quot;&gt;앞으로 무엇을 할 것인가요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/todo.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;감사하게도 회사에서 정규직 제안을 받았다. 스타트업 리서치 엔지니어에 가깝지만, 학사 출신으로 연구를 할 수 있다는 것이 흔치 않은 기회이다. 하지만 회사에서 원하는 인재와 내가 가고자 하는 방향이 달랐다. 회사에서 원하는 인재는 generalist이지만 나는 한 분에서 specialist가 되고 싶었다. 분야 또한 일반적인 모델링이 아닌 모델 경량화, hardware optimization, low cost serving 쪽으로 가고 싶다. 그리고 내가 다루어야 하는 target data가 무엇인지 명확하게 정할 수 있는 연구개발을 하고 싶다.&lt;/p&gt; &lt;p&gt;이제 학교에 다시 돌아간다. 8개월 동안 뉴로클 덕분에 좋은 경험을 했고 내 실력도 엄청나게 향상되었다. 이제 4학년 2학기이다. 졸업도 얼마 안 남아서 취업 준비나 대학원 준비를 해야겠지만 DL engineer 쪽 공부도 더욱 열심히 하면서 내가 목표하는 커리어를 만들어 가야겠다.&lt;/p&gt; </description> <pubDate>Tue, 22 Aug 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/startup-research-intern-review/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/startup-research-intern-review/</guid> <category>daily</category> <category>daily</category> </item> <item> <title>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;현재 GPT부터 시작해서 ViT 등 여러 분야에서 attention layer를 사용하고 있다. 하지만 attention layer는 dimension의 제곱에 비례하여 cost가 들어 모델의 병목인 부분이기도 하다. 이에 따라 attention layer를 효율적으로 만드는 시도가 많이 있는데, 그중 하나가 FlashAttention이다. FlashAttention은 tiling과 kernel fusion으로 기존 attention layer 대비 2.4배 속도가 향상되었다. 하지만 FlashAttention 또한 기존 GPU의 이론적 성능에 25~40% 정도의 속도밖에 내지 못한다.&lt;/p&gt; &lt;p&gt;저자는 FlashAttention을 분석하던 중 thread block 간 work를 partitioning할 때 비효율성을 발견했고, 이로 인해 GPU에서 low-occupancy와 불필요한 memory IO가 일어나는 것을 깨달았다. 따라서 저자는 이를 해결하기 위해 3가지를 제안했다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Output을 바꾸지 않고 non-matmul operation의 FLOPS를 줄인다.&lt;/li&gt; &lt;li&gt;Single head attention일지라도 병렬처리를 하도록 연산 순서를 변경한다.&lt;/li&gt; &lt;li&gt;Thread block 내에 warps 간 통신을 줄인다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;저자는 위 3가지를 통해 기존 FlashAttention 대비 2배 빠른 속도를 달성하고 GPU의 이론적 성능의 50~73%까지 성능을 끌어올렸다.&lt;/p&gt; &lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt; &lt;p&gt;하드웨어 최적화에 관한 논문은 익숙하지 않으니 background까지 꼼꼼하게 읽어보자.&lt;/p&gt; &lt;h2 id=&quot;hardware-characteristics&quot;&gt;Hardware characteristics&lt;/h2&gt; &lt;h3 id=&quot;gpu-performance-characteristics&quot;&gt;GPU performance characteristics&lt;/h3&gt; &lt;p&gt;GPU는 compute element와 memory hierarchy를 가지고 있다. Nvidia tensor core와 같은 최신 GPU compute element는 FP16/BF16과 같은 low-precision에서 matmul operation을 최적화하고 있다. 반면에 non-matmul operation은 최적화가 되어있지 않아 matmul operation보다 최대 16배가 느리다.&lt;/p&gt; &lt;p&gt;Memory hierarchy에 관해서는 기본적으로 GPU는 high bandwidth memory (HBM)과 on-chip SRAM (shared memory)를 가지고 있다. A100 기준 40~80GB의 HBM은 1.5~2.0TB/s의 bandwidth를 가지고 있고, 108개의 stream multiprocessor는 각각 192KB의 on-chip SRAM을 가지고 있으며 이는 19TB/s의 bandwidth를 가지고 있다. L2 cache도 있으나, 이것은 사용자가 컨트롤할 수 없으므로 논의에서 제외하도록 하자.&lt;/p&gt; &lt;h3 id=&quot;execution-model&quot;&gt;Execution Model&lt;/h3&gt; &lt;p&gt;GPU는 수많은 thread로 구성되어 있으며, thread가 모여서 thread block을 구성한다. 이 thread block은 stream multiprocessor (SMs)를 통해 실행된다. Thread block 내에서 thread는 warp이라는 단위로 묶이게 되는데, 이 warp들은 공유 메모리를 통해 communication을 한다.&lt;/p&gt; &lt;h2 id=&quot;standard-attention-implementation&quot;&gt;Standard Attention Implementation&lt;/h2&gt; &lt;p&gt;기존 attention은 query, key, value들 간의 연산으로 구성된다. Sequence length를 N, head dimension을 d라고 하자. Input sequence \(Q, K, V \in \mathbb{R}^{N\times d}\)에 대해 attention output \(O \in \mathbb{R}^{N \times d}\)를 계산하기 위해 아래의 식을 이용한다.&lt;/p&gt; \[S=QK^{\intercal}\in \mathbb{R}^{N\times N}\] \[P=\text{softmax}(S)\in\mathbb{R}^{N\times N}\] &lt;p&gt;\(O=PV\in \mathbb{R}^{N\times d}\) 이때 softmax는 row-wise로 적용하게 된다. Backward pass는 다음과 같은 과정을 거친다.&lt;/p&gt; \[dV=P^{\intercal}dO\in\mathbb{R}^{N\times d}\] \[dP=dOV^{\intercal}\in\mathbb{R}^{N\times N}\] \[dS=\text{dsoftmax}(dP)\in\mathbb{R}^{N\times N}\] \[dQ=dSK\in\mathbb{R}^{N\times d}\] \[dK=QdS^\intercal\in\mathbb{R}^{N\times d}\] &lt;p&gt;더 자세한 것은 FlashAttention 설명을 참고하면 된다.&lt;/p&gt; &lt;h2 id=&quot;flashattention&quot;&gt;FlashAttention&lt;/h2&gt; &lt;p&gt;자세한 것은 FlashAttention 설명을 참고하기 바란다. &lt;a href=&quot;https://www.wonbeomjang.kr/blog/2023/fastattention/&quot;&gt;FlashAttention 1 포스트&lt;/a&gt;&lt;/p&gt; &lt;h3 id=&quot;forward-pass&quot;&gt;Forward pass&lt;/h3&gt; &lt;p&gt;간단하게 이야기하자면, K, V를 tiling하여 병렬적으로 계산 후 on-line softmax를 통해 병렬적으로 softmax를 적용한다. 이후에 tiling한 Q를 불러와 on-chip 연산으로 만든다. 또한 이를 통해 연산을 fusion할 수 있으며 Q, K, V는 HBM에서 load한 이후 모든 연산을 수행한 후 HBM에 저장하게 된다. 연산은 다음과 같고, 아래서 표시한 \(S\)는 \(S=QK^T\)이다.&lt;/p&gt; \[m^{(1)}=\text{rowmax}(S^{(1)})\in\mathbb{R}^{B_r}\] \[l^{(1)}=\text{rowsum}(e^{S^{(1)}-m^{(1)}})\in\mathbb{R}^{B_r\times B_c}\] \[\tilde{P}^{(1)}=\text{diag}(l^{(1)})^{-1}e^{S^{(1)}-m^{(1)}}\in\mathbb{R}^{B_r\times B_c}\] \[O^{(1)}=\tilde{P}^{(1)}V^{(1)}=\text{diag}(l^{(1)})^{-1}e^{S^{(1)}-m^{(1)}}V^{(1)}\in\mathbb{R}^{B_r\times d}\] \[m^{(2)}=\text{max}(m^{(1)},\text{rowmax}(S^{(2)}))=m\] \[l^{(2)}=e^{m^{(1)}-m^{(2)}}l^{(1)}+\text{rowsum}(e^{S^{(2)}-m})=\text{rowsum}(e^{S^{(1)}-m})+\text{rowsum}(e^{S^{(2)}-m})=l\] \[\tilde{P}^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m^{(2)}}\] \[O^{(2)}=\text{diag}(l^{(1)}/l^{(2)})^{-1}O^{(1)}+\tilde{P}^{(2)}V^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(1)}-m}V^{(1)}+\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m}V^{(2)}=O\] &lt;p&gt;즉, figure 1처럼 vector를 쪼개고, 합치는 과정을 통해 memory IO를 줄여 연산 속도를 빠르게 만들었다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h3 id=&quot;backward-pass&quot;&gt;Backward Pass&lt;/h3&gt; &lt;p&gt;Backward pass는 attention 연산을 하는 과정에서 \(m, l\)이 계산되는데 이를 이용하면 다시 연산을 recompute할 수 있다.&lt;/p&gt; &lt;h1 id=&quot;3-flashattention-2&quot;&gt;3. FlashAttention-2&lt;/h1&gt; &lt;p&gt;FlashAttention은 기본적으로 non-matmul FLOPs를 줄인다. 예를 들어 Nvidia의 A100 GPU는 FP16/BF16의 matmul 연산은 이론적으로 312 TFLOPs/s의 연산량을 가지지만, non-matmul 연산은 19.5 TFLOPs/s의 연산량을 가진다. 즉 non-matmul 연산이 matmul 연산보다 16배 느려 non-matmul 연산이 전체 연산의 일부를 차지하더라도 이를 최적화시켜야 한다.&lt;/p&gt; &lt;h2 id=&quot;forward-pass-1&quot;&gt;Forward pass&lt;/h2&gt; &lt;p&gt;저자는 FlashAttention에서 on-line softmax를 먼저 주목했다.&lt;/p&gt; &lt;h3 id=&quot;recaling&quot;&gt;Recaling&lt;/h3&gt; &lt;p&gt;기존에는 \(\text{diag}(l^{(2)})^{-1}\)를 두 항 모두 rescaling했다.&lt;/p&gt; \[O^{(2)}=\text{diag}(l^{(1)}/l^{(2)})^{-1}O^{(1)}+\tilde{P}^{(2)}V^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(1)}-m}V^{(1)}+\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m}V^{(2)}=O\] &lt;p&gt;이렇게 한다면 두 텀을 각각 읽어 각각 나눠야 하기 때문에 memory IO가 많아진다. 따라서 마지막 결과 \(\tilde{O}^{(last)}\)를 계산 후에 한꺼번에 \(\text{diag}(l^{(last)})^{-1}\)으로 rescaling 한다.&lt;/p&gt; \[\tilde{O}^{(2)}=\text{diag}(l^{(1)})^{-1}O^{(1)}+e^{S^{(2)}-m^{(2)}}V^{(2)}\] \[O^{(2)}=\tilde{O}^{(2)}\text{diag}(l^{(2)})^{-1}\] &lt;h3 id=&quot;memorization&quot;&gt;Memorization&lt;/h3&gt; &lt;p&gt;Backward에 사용하기 위해서 \(m, l\)을 저장한 후 재구성한다고 했다. 각각을 저장하는 대신 \(L^{(j)}=m^{(j)}+\text{log}(l^{(j)})\)를 저장해도 똑같이 backward를 재구성할 수 있어 \(m, l\) 대신 \(L\)을 저장하게 된다.&lt;/p&gt; &lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt; &lt;p&gt;결론적으로 FlashAttention 2에서는 다음과 같은 방법으로 attention을 구현하게 된다.&lt;/p&gt; \[m^{(1)}=\text{rowmax}(S^{(1)})\in\mathbb{R}^{B_r}\] \[l^{(1)}=\text{rowsum}(e^{S^{(1)}-m^{(1)}})\in\mathbb{R}^{B_r\times B_c}\] \[\tilde{O}^{(1)}=e^{S^{(1)}-m^{(1)}}V^{(1)}\in\mathbb{R}^{B_r\times d}\] \[m^{(2)}=\text{max}(m^{(1)},\text{rowmax}(S^{(2)}))=m\] \[l^{(2)}=e^{m^{(1)}-m^{(2)}}l^{(1)}+\text{rowsum}(e^{S^{(2)}-m})=\text{rowsum}(e^{S^{(1)}-m})+\text{rowsum}(e^{S^{(2)}-m})=l\] \[\tilde{P}^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m^{(2)}}\] \[\tilde{O}^{(2)}=\text{diag}(e^{m^{(1)}-m^{(2)}})^{-1}\tilde{O}^{(1)}+e^{S^{(2)}-m^{(2)}}V^{(2)}=e^{S^{(1)}-m}V^{(1)}+e^{S^{(2)}-m}V^{(2)}\] \[O^{(2)}=\text{diag}(l^{(2)})^{-1}\tilde{O}^{(2)}=O\] &lt;p&gt;기존 FlashAttention과 다르게 term 자체가 줄어들었다. Forward pass에 관한 알고리즘을 정리하자면 다음과 같다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/alg1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;backward&quot;&gt;Backward&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/alg2.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Backward 자체는 \(L\)을 사용한다는 것 말고는 별다른 이야기는 없다.&lt;/p&gt; &lt;h2 id=&quot;parallelism&quot;&gt;Parallelism&lt;/h2&gt; &lt;p&gt;기본적으로 GPU는 병렬처리가 가능하다. 각각의 GPU thread block마다 1개의 attention module이 들어간다. 따라서 보통 # batch size x # self-attention head로 thread block을 구성하게 되고 이를 stream multiprocessor가 나눠 가진다. 그래서 만약 sequence 길이가 길어 small batch size나 small number of self-attention head를 가지게 된다면 병렬처리를 잘 활용하지 못한다. 따라서 저자는 sequence length dimension에 따른 병렬처리를 하게 된다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Forward pass&lt;/strong&gt; 저자는 sequence length dimension으로 병렬처리를 한다. 하지만 이는 한 sequence 내에서는 독립적으로 처리되어야 함으로 다른 sequence와 통신을 하지 못하도록 구성했다. 물론 이전과 마찬가지로 batch, multi-head 간 병렬처리는 유지한다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Backward pass&lt;/strong&gt; Algorithm 2에 의하면 column block 간에 병렬처리만 한다. 위의 경우와 같이 sequence length dimension으로도 병렬처리가 가능하여 추가하게 된다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;결과적으로 worker마다 병렬처리가 잘 되게 된다.&lt;/p&gt; &lt;h2 id=&quot;work-partitioning-between-warp&quot;&gt;Work Partitioning Between Warp&lt;/h2&gt; &lt;h3 id=&quot;forward&quot;&gt;Forward&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig3.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;기존의 FlashAttention은 \(K\)와 \(V\)를 각각의 warp에 K개로 partitioning 했고, \(Q\)는 모든 warp이 접근 가능하도록 했다. 그리고 이를 “split-K”라고 한다. 하지만 이러한 방법은 partition된 \(QK^T\)를 partition된 \(V\)에 곱하게 된다. 따라서 중간 계산 결과를 저장하고, 읽고, 동기화를 많이 해야 해서 IO에서 속도가 느려진다. 따라서 \(Q\)를 partition하고, \(K, Q\)를 공유하게 해 이런 IO를 줄여 속도를 높이게 된다.&lt;/p&gt; &lt;h2 id=&quot;backward-1&quot;&gt;Backward&lt;/h2&gt; &lt;p&gt;“split-K”를 지양한다는 것 밖에 이해를 못했다.&lt;/p&gt; &lt;h3 id=&quot;tuning-block-sizes&quot;&gt;Tuning block sizes&lt;/h3&gt; &lt;p&gt;Block size를 늘리면 memory IO의 수가 줄어든다. 하지만 block 수가 많아지면서 registers의 수가 늘어나고, total shared memory 크기가 커져 비효율성이 늘어난다. 많은 registers는 프로그램 속도를 느리게 만들고, total shared memory의 크기가 너무 커지면 GPU memory가 부족하다. 따라서 GPU마다 적절한 block size를 조정한다.&lt;/p&gt; &lt;h1 id=&quot;empirical-validation&quot;&gt;Empirical Validation&lt;/h1&gt; &lt;p&gt;이제 속도를 보자.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig4.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig5.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig6.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig7.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;FlashAttention-2는 기존 FlashAttention, xFormer 대비 2배의 속도를 보여줬고, Triton으로 구현된 FlashAttention보다 1.3~1.5배 빨라진 속도를 보여줬다. 놀라운 것은 PyTorch에서 naive하게 구현한 것 대비 10배의 속도 차이를 보여준다. 이로 인해 기존의 large model에서도 더 빠른 연산 속도를 보여준다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/table1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; </description> <pubDate>Sun, 06 Aug 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/flashattention-2/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/flashattention-2/</guid> <category>attention</category> <category>hardware-optimization</category> <category>paper</category> <category>attention</category> <category>hardware-optimization</category> <category>paper</category> </item> <item> <title>Fairness-aware Data Valuation for Supervised Learning</title> <description>&lt;h1 id=&quot;들어가기-앞서&quot;&gt;들어가기 앞서&lt;/h1&gt; &lt;p&gt;Active learning과 class imbalance를 찾던 도중 발견한 논문이자. 그래서 FairML 분야는 아는 것이 없고, 이 논문이 좋은지 나쁜지도 판단이 안 된다. 하지만 해당 논문의 개념도 간단하고, 이런 것을 고려하면서 sampling을 하는 것도 좋겠다는 생각에 논문을 정리하고자 한다.&lt;/p&gt; &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;기존의 data valueation 연구는 데이터를 특정한 performace related value로 embedding한다. Active learning에서 “해당 train instance를 추가하면 모델의 성능이 올라가겠다”를 목적으로 entropy를 사용하여 embedding하는 방법도 있다. 하지만 추천시스템에서 해당 value만 사용할 경우 특정 그룹이나 인종에 안 좋은 데이터를 추천해주는 등 여러 안 좋은 점들이 있다. 따라서 fairness도 고려되어야 하는데 기존까지는 performance와 fairness를 동시에 고려하는 연구는 많지 않았다. 저자는 entropy를 기반으로 하여 두 가지 factor를 모두 고려하여 data를 utility value에 embedding하는 framework를 제안하였고 이를 이용하여 더 좋은 data sampling, re-weighting을 할 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;fairness-aware-data-valuation&quot;&gt;Fairness-aware Data Valuation&lt;/h1&gt; &lt;h2 id=&quot;framework&quot;&gt;Framework&lt;/h2&gt; &lt;p&gt;일단 저자는 utility개념을 빌려왔는데, 해당 논문에서 utility는 performance와 fairness를 종합하는 function을 의미한다. Single data instance \(i\) 에 대해 perforamce-related valuation은 \(v_{y_i}\) , protected attribute에 대한 fairness-related valuation은 \(v_{z_i}\) 로 표시한다. Utility function은 다음과 같다.&lt;/p&gt; \[U_i(v_{y_i},v_{z_i})=\alpha(v_{y_i})+(1-\alpha)v_{z_i}\] &lt;p&gt;이 때 \(\alpha \in [0,1]\) 이다. 만약 fairness를 subgroup으로 나눈다면 다음의 식으로 확장할 수 있다.&lt;/p&gt; \[U_i(v_{y_i},v_{z_i})=\alpha(v_{y_i})+\sum_{j=1}^{k}\beta v_{z_{j_i}}\] &lt;h2 id=&quot;entorpy-metric&quot;&gt;Entorpy metric&lt;/h2&gt; &lt;p&gt;먼저 저자는 performace related value를 instance \(i\) 의 prediction \(y_{i}\) 의 entropy로 정의했다.&lt;/p&gt; \[V_{y_i}=E_{y_i}=\hat{y}_i\cdot {log}_2\hat{y}_i+(1-\hat{y}_i)\cdot {log}_2(1-\hat{y}_i)\] &lt;p&gt;해당 수식은 active learning에서 영감을 받았다. Entropy가 높다는 것은 모델이 해당 instance를 잘 예측하지 못한다는 이야기이고, 추후에 이를 집어넣으면 성능이 높아진다는 것을 예상할 수 있다. 하지만 실제 상황에서는 애매한 instance뿐만 아니라 noise 또한 entropy가 높아져서 성능이 더 낮아질 가능성도 있다. 하지만 여러 task에서 해당 방법은 성능이 준수하다는 것으로 나와서 저자는 entropy를 사용했다.&lt;/p&gt; &lt;p&gt;저자는 fairness-related valueation또한 entropy로 정의했다.&lt;/p&gt; \[V_{z_i}=E_{z_i}=\hat{z}_i\cdot {log}_2\hat{z}_i+(1-\hat{z}_i)\cdot {log}_2(1-\hat{z}_i)\] &lt;p&gt;이 수식이 왜 되는지는 이해가 잘 안되지만 저자에 말은 이러하다.&lt;/p&gt; &lt;blockquote&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;In the second case, where the variable in question (Z) is not the target for the task at hand, and is seen by the model even at inference time, we prioritize observations where the model had more difficulty in establishing a relationship among X, Y, and Z, leveraging the fact the model has no explicit incentive to draw such relationships. This is directly related to mitigating the base bias condition of the taxonomy of Pombal et al. (2022a) ( P[X, Y] 6= P[X, Y&lt;/td&gt; &lt;td&gt;Z] ), and so related to promoting fairness.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;ul&gt; &lt;li&gt;Fairness-aware data valuation for supervised learning, Pombal et al, 2023 -&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;p&gt;내가 이해한 대로 적자면 여러 예측 모델이 있는데 이들 모두의 예측값의 entropy가 높다면 bias가 없는 데이터이다. 하지만 특정 모델에서 entropy가 높아 잘 예측한다면 그 데이터는 특정 모델에 적합한 bias가 있는 데이터일 것이다. 따라서 fairness 또한 entropy로 표현할 수 있게 되는 것이다.&lt;/p&gt; &lt;p&gt;이렇게 구한 perforamce related valuation과 fairness related valuation을 종합하여 utility function을 제작하게 된다.&lt;/p&gt; \[U_i=\alpha E_{y_i}+(1-\alpha)E_{z_i}\] &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt; &lt;p&gt;Dataset으로 bank account-opening fraud dataset을 사용했다. 해당 데이터에서 fraud rate는 1%이다. 해당 데이터는 사기계좌를 찾는 것으로 True Positive Rate (TPR)이 높아야한다. 반면에 False Positive Rate (FPR)가 높으면 사용자의 계좌사용이 불편해지기 떄문에 FPR을 낮추는 것을 목표로 하고 있다.&lt;/p&gt; &lt;h1 id=&quot;model&quot;&gt;Model&lt;/h1&gt; &lt;p&gt;Model은 tublar data에서 SOTA를 찍고있는 LightGBM을 사용한다.&lt;/p&gt; &lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt; &lt;p&gt;Data sampling, re-weighting은 다음의 과정을 거친다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;각각의 예측모델 Y, Z에 대해 tranining instance의 value를 계산한다.&lt;/li&gt; &lt;li&gt;각각의 train instance에 대해 utility value를 계산한다.&lt;/li&gt; &lt;li&gt;Utility value를 기반으로 Utility-aware prevalence sampling (UASP) 또는 Utility-aware reweighting (UAR) 수행한다. 이 때 UASP는 under sampling을 이야기하는 것이다.&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/fado/fig1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;결과는 위와 같다. 해당 표를 보는 방법은 모르나 저자말로는 자신이 제안한 framework가 balance를 잘 잡는다고 주장한다.&lt;/p&gt; </description> <pubDate>Wed, 12 Jul 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/fado/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/fado/</guid> <category>data-sampling</category> <category>fair-ml</category> <category>data-sampling</category> <category>fair-ml</category> </item> <item> <title>TinyViT</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;ViT는 많은 발전이 있었지만 edge device에 적용하기에는 모델이 너무 컸다. 하지만 모두들 알다싶이 작은 모델 representation이 작다. 따라서 small dataset에 적합할지 몰라도 large dataset에서는 빠르게 saturation이 되면서 underfitting이 발생하게 되어 잘 사용하지 못한다. 저자는 이에 대하여 고민을 했고 knowledge distillation을 사용해야한다는 결론에 도달하게 된다. 이에따라 small model이 downstream task에 transfer가 잘 되는 것을 확인했다. 하지만 기존의 knowledge distillation은 train시 teacher network를 메모리에 올리기 때문에 teacher network가 gpu memory를 다수 잡아먹게 되어 batch size 조절이 힘들다. 또한 학습에 필요한 soft-label을 그때그때 제작하기 때문에 학습속도도 느려진다. 따라서 저자는 이를 해결하기 위해 data augmentation과 soft-label을 먼저 저장하여 student model이 학습 시 이를 사용하는 방법을 사용했다. 결론적으로 TinyViT는 21M의 파라미터로 ImageNet에서 84.8% top-1 accuracy를 달성했으며 88M으로 85.8%를 달성한 Swin-B보다 4.2배 적은 파라미터이다. 이미지 크기를 키웠을 때 SOTA를 찍었으며 COCO object detection도 Swin-T 우위에 있다는 것을 확인했다.&lt;/p&gt; &lt;h1 id=&quot;tinyvit&quot;&gt;TinyViT&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;fast-pretraining-distillation&quot;&gt;Fast Pretraining Distillation&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;위 사진에서 볼 수 있듯 small model을 바로 큰 데이터셋에 학습하면 성능이 낮아진다. 따라서 knowledge distillation을 이용하려고 하는데 저자는 downstream task를 위해 finetuning-distillation이 아니라 pretraining distillation에 주목을 한다. 이에 따라 ImageNet-1K에는 distillation없이 finetuning을 이용하여 학습을 진행한다. 하지만 pretraining distillation은 large teacher model이 많은 데이터셋을 inference 해야 하기 때문에 비용이 많이 들고 비효율적이다. 따라서 Fig.2에서 나오듯 사전에 teacher model에 사용되는 augmentation과 그에 맞는 label을 만들어 스토리지에 저장하게 된다. 이를 통해 student model이 학습 시 teacher model의 forward computation 없이 knoweldge distillation을 할 수 있다. 수학적으로 표현하자면, input image를 \(x\), RandAugment나 CutMix와 같은 strong augmentation을 \(\mathcal{A}\)라 하자. Teacher 학습시 augmentation \(\mathcal{A}\), techer prediction \(\hat{y}=T(\mathcal{A}(s))\)의 pair인 \((\mathcal{A},\hat{y})\)를 저장한다. 이 때 \(T(\cdot )\)은 teacher model을 의미한다. 그 후 student \(S(\cdot)\)과 cross entropy \(CE(\cdot)\)에 대해 loss 연산을 진행한다.&lt;/p&gt; \[\mathcal{L}=CE(\hat{y},S(\mathcal{A}(x)))\] &lt;h3 id=&quot;sparse-soft-label&quot;&gt;Sparse Soft Label&lt;/h3&gt; &lt;p&gt;Teacher model의 output을 그대로 저장하는 것은 storage를 많이 사용한다. 따라서 저자는 \(\hat{y}\)의 top-K value과 그들의 indices \(\{\mathcal{I}(k)\}_{k=1}^K\)만을 저장하고 나머지는 label smoothing을 이용하여 reconstruct한다. 학습 시 ground truth인 hard label을 사용하지 않고 pseudo label인 soft label만을 사용하여 학습한다.&lt;/p&gt; \[\hat{y}_c = \begin{cases} \hat{y}_{\mathcal{I}(k)} &amp;amp; \text{if} \ c=\mathcal{I}(k)\\ \frac{1-\sum_{k=1}^K\hat{y}_{\mathcal{I}(k)}}{C-K} &amp;amp; \text{otherwise} \end{cases}\] &lt;p&gt;이 때 \(\hat{y}_c\)는recovered teacher logit for student model distillation이라고 하고, \(\hat{y}=[\hat{y}_1, ... ,\hat{y}_c, ... ,\hat{y}C]\)이다. 만약 \(K &amp;lt;&amp;lt; C\)라면 메모리 감소가 크다.&lt;/p&gt; &lt;h3 id=&quot;data-augmentation-encoding&quot;&gt;Data augmentation encoding&lt;/h3&gt; &lt;p&gt;Data augmentation 정보 또한 그대로 저장하면 storage를 많이 사용한다. (rotation degree, crop coordinate 등) 따라서 set of data augmenatation parameter를 \(\mathbf{d}\)라고 하고, encoder \(\mathcal{E}(\cdot)\) 를 사용하여 \(d_0=\mathcal{E}(\mathbf{d})\)로 변환하여 저장한다. 그 후 training process에는 \(\mathcal{E}^{-1}(\cdot)\)을 decoder로사용하여 augmetation을 진행한다.&lt;/p&gt; &lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt; &lt;p&gt;저자는 기본적으로 ViT를 기반으로 모델을 설계했다. 또한 Swin Transformer나 LeViT와 같이 hierarchical한 구조를 채택했다. Patch embedding block은 kernel size 3, stride 2, padding 1인 두 개의 convolution을 사용했다. 하지만 처음부터 끝까지 transformer block을 사용하는 것은 연산적으로 무리가 된다. 따라서 MobileNetV2에서 사용하는 MBConv를 사용하여 stage 1과 downsampling을 구성했다. 또한 MBConv에 residual conntection 또한 적용했다. 마지막 3개 stage 에서는 transformer block을 사용했다. 모든 layer와 block에서 activation function은 요즘 성능이 좋은 GELU를 이용했고, normalization은 conolution layer에는 batch norm, linear layer에는 layer norm을 사용했다.&lt;/p&gt; &lt;h3 id=&quot;contraction-factors&quot;&gt;Contraction factors&lt;/h3&gt; &lt;p&gt;Modern model와 같이 factor를 사용하여 모델의 크기를 조절한다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;\(\gamma_{D_{1-4}}\): embeded demension in four stages&lt;/li&gt; &lt;li&gt;\(\gamma_{N_{1-4}}\): number of block in four stages&lt;/li&gt; &lt;li&gt;\(\gamma_{W_{2-4}}\): window size of last three stages&lt;/li&gt; &lt;li&gt;\(\gamma_{R}\): channel expansion ratio of MBConv&lt;/li&gt; &lt;li&gt;\(\gamma_{M}\): channel expansion MLP of transformer blocks&lt;/li&gt; &lt;li&gt;\(\gamma_{E}\): the dimension of each head in multi-head attention&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;모든 모델이 \(\gamma_{D_{1-4}}\)을 제외하고 같은 factor를 갖는다.&lt;/p&gt; \[\{\gamma_{N_1}, \gamma_{N_1}, \gamma_{N_1}, \gamma_{N_1}\} = \{2, 2, 6, 2\}\] \[\{\gamma_{W_1}, \gamma_{W_1}, \gamma_{W_1}\} = \{7, 14, 7\}\] \[\{\gamma_{R}, \gamma_{M}, \gamma_{E}\} = \{4, 4, 32\}\] &lt;p&gt;Embeded dimensiondm \(\{\gamma_{D_1}, \gamma_{D_2}, \gamma_{D_3}, \gamma_{D_4}\}\)는 TinyViT-21M: {96, 192, 384, 576},TinyViT-11M: {64, 128, 256, 448},TinyViT-5M: {64, 128, 160, 320} 으로 구성했다.&lt;/p&gt; &lt;h1 id=&quot;analysis-and-discussion&quot;&gt;Analysis and Discussion&lt;/h1&gt; &lt;p&gt;작은 모델은 Image21K와 같은 large scale dataset에서는 underfitting이 발생하여 학습이 잘되지 않는다. 그렇다면 다음 두 가지의 의문이 들 수 있다.&lt;/p&gt; &lt;h3 id=&quot;1-어떠한-요소가-small-model의-학습을-방해하는가&quot;&gt;1. 어떠한 요소가 small model의 학습을 방해하는가?&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;이를 확인하려면 IN-21K의 문제점을 알아야한다. Label의 오류와 비슷한 이미지가 다른 label인 경우가 존재한다. 이와 같은 데이터가 전체의 10%로 hard sample이라고 불린다. Large model은 이러한 hard sample을 학습할 수 있자만 small model은 hard sample을 예측하기 어렵고 이는 small model을 large model 보다 낮은 성능으로 이끈다. 이를 증명하기 위해 강력한 모델인 Florence를 이용하여 data마다 florence prediction의 top 5 중에 label이 존재하는지 찾아봤다. 그 결과 ImageNet-21K의 14%에 해당하는 hard sample을 골라냈다. 또한 Florence를 이용하여 TinyViT-21M/Swin-T에 knowledge distllation을 적용했다. 그 결과 small model을 곧바로 ImageNet-21K에 학습하는 것은 성능에서 제약이 존재했고, ImageNet-21K에서 hard sample을 제거하면 1%p 정도 성능향상이 있었다. 놀라운 점은 knowledge distillation이 hard sample의 defact을 줄여줬다.&lt;/p&gt; &lt;h3 id=&quot;2-왜-distillation이-small-model의-large-data-학습에-도움이-되는가&quot;&gt;2. 왜 distillation이 small model의 large data 학습에 도움이 되는가?&lt;/h3&gt; &lt;p&gt;Student model이 teacher model의 지식을 바로 학습할 수 있기 때문이다. Gound truth는 각 물체간 상간관계를 보여주지 못한다. 하지만 teacher model의 inference 값은 그것을 알 수 있다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;위의 그림을 보았을 때도 distillation 없이 TinyViT를 학습했을 때 비슷한 물체간 상관관계가 크지 않았으나 distillation 진행시 상관관계가 크게 나와 teacher model을 제대로 따라할 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;h2 id=&quot;impact-of-pretraining-distillation-on-existing-small-vits&quot;&gt;Impact of pretraining distillation on existing small ViTs&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;앞서 설명을 했 듯 ImageNet-21K로 distillation을 진행한 후 down stream task를 진행했다. 이 학습방법론이 효과가 있을지 다른 가벼운 ViT를 사용하여 실험한 결과 해당 방법론은 효과가 있었다.&lt;/p&gt; &lt;h2 id=&quot;impact-of-pretraining-data-scale&quot;&gt;Impact of pretraining data scale&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Distillation을 pretranining할 때 사용한다. 이 때 사용한 데이터에 따라 성능차이를 확인해보니 데이터가 많아질수록 성능이 좋아졌다.&lt;/p&gt; &lt;h2 id=&quot;impact-of-the-number-of-saved-logits&quot;&gt;Impact of the number of saved logits&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig5.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Soft-label의 용량을 줄이기 위해서 top-K만 저장한다고 했다. K가 늘어날 수록 메모리 사용량은 늘어나나 정확도는 비슷하다. 따라서 이를 밸런스 있게 가져가기 위해서 ImageNet-1K에서는 \(K=10\), ImageNet-21K에서는 \(K=100\)으로 설정했다.&lt;/p&gt; &lt;h3 id=&quot;impact-of-teacher-models&quot;&gt;Impact of teacher models&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Teacher Model을 어떤 것으로 설정하는지에 따라서도 성능차이가 났다. 더 강력하고 좋은 teacher model을 사용하면 성능이 좋아지나 그 만큼 학습시간이 늘어나는 것을 볼 수 있었다.&lt;/p&gt; &lt;h2 id=&quot;result-on-imagenet&quot;&gt;Result on ImageNet&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig6.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;위 표에서 볼 수 있듯 TinyViT는 경량화 ViT 중에서 좋은 성능을 내었다.&lt;/p&gt; &lt;h2 id=&quot;transfer-learning-results&quot;&gt;Transfer Learning Results&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table5.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Self-supervised learning으로 MOCO를 사용하여 Linear probe를 하거나 few shot learning을 할 때도 성능이 좋았다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table6.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;또한 MS COCO를 사용하여 object detection에 사용할 때도 성능이 좋았다.&lt;/p&gt; &lt;h1 id=&quot;comment&quot;&gt;Comment&lt;/h1&gt; &lt;p&gt;사실 TinyViT 자체는 knowedge distillation 내용이라 새로운 것이 별로 없다. 결과들을 미리 저장하는 것 역시 가끔씩 쓰는 트릭이라 크게 중요하지 않다. 해당 논문은 그런 것 보다 small ViT가 ImageNet에서 hard example에 취약하여 이를 골라내는 과정이 중요한 것 같다. 또한 저자는 말하지 않았지만 (몰랐을 수도 있지만) sparse soft label 자체가 teacher model의 noise를 제거하는 역할을 하여 small ViT에 더 좋은 성능을 가져와 주었을 것이다. (비교적 어려운 이미지들은 confidence가 낮은 이미지들의 probability가 noise로 작용하기도 한다.) 끝마치면서 드는 생각은 ImageNet-21K로 pretrain하고 이를 ImageNet-1K로 finetuning하는 것이 workshop에서 맞는 방법이지 않나 싶으면서도 ViT를 생각해보면 그럴수도 있다는 생각이 든다.&lt;/p&gt; </description> <pubDate>Wed, 28 Jun 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/tinyvit/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/tinyvit/</guid> <category>backbone</category> <category>paper</category> <category>cvpr</category> <category>vit</category> <category>efficient-architecture</category> <category>knowledge-distillation</category> <category>backbone</category> <category>paper</category> <category>vit</category> <category>knowledge-distillation</category> </item> <item> <title>EdgeViT</title> <description>&lt;h1 id=&quot;intorduction&quot;&gt;Intorduction&lt;/h1&gt; &lt;p&gt;ViT는 global한 representation을 학습하면서 imagenet banchmark에서 압도적인 성능을 내고있다. 하지만 self-attention이라는 연산의 비용이 커서 inference speed나 power efficiency가 떨어진다. 이를 해결하기 위한 기존의 연구는 크게 다음과 같았다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Spatial Resolution에서 hierarchical architecture을 만들어 연산량을 줄인다.&lt;/li&gt; &lt;li&gt;Locally-grouped self-attention mechanism을 사용한다.&lt;/li&gt; &lt;li&gt;Key, value를 pooling하여 subsampling한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;하지만 이러한 방법은 time cost가 중요한 mobile이나 edge platform에서 충분하지 못하다. 따라서 저자는 다음의 요소를 고려하여 EdgeViT를 설계했다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Inference efficiency&lt;/strong&gt; EdgeViT는 on-device에서 사용할 정도로 가볍고 에너지를 적게 사용해야한다. 기존에는 이를 측정하기위해 FLOPS를 주로 사용했지만 이는 latency과 energy consumption을 제대로 반영하지 못한다. 따라서 FLOPS는 추정치로만 참고하고 실제 mobile device에서 밴치마크를 진행한다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Model size&lt;/strong&gt; 현대 스마트폰은 RAM이 32GB일정도로 메모리량이 충분하다. 따라서 절대적인 모델크기를 고려하는 것은 현대 mobile device에는 적합하지 않아서 이는 크게 고려하지 않는다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Implementation Friendliness&lt;/strong&gt; 현실적으로 implementation을 하기위해서는 ONNX, TensorRT, TorchScript등 기존의 framework와의 호환성을 고려해야한다.&lt;/p&gt; &lt;p&gt;이를 고려하여 저자는 local-global-local bottleneck을 제안했고 이는 energy efficient하고 inference speed도 빠르다.&lt;/p&gt; &lt;h1 id=&quot;local-global-local-bottleneck&quot;&gt;Local-Global-Local-BottleNeck&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/Fig3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;ViT의 성능이 좋은 이유는 global한 representation을 학습하기 유리하기 때문이다. Multi-head self-attention이 이를 가능하게 만들었으니 이를 모사하는 연산을 만드는 것을 필수적이다. 따라서 저자는 factorization을 통해 해당 연산을 3가지로 쪼개 모사했다.&lt;/p&gt; &lt;h2 id=&quot;local-aggregation&quot;&gt;Local Aggregation&lt;/h2&gt; &lt;p&gt;연산을 가볍게 만들기 위해서는 절대적인 연산량을 줄여야 한다. 다행이도 이미지에서는 주변 픽셀은 비슷하다는 inductive bias가 존재한다. 따라서 전체 token에 대하여 attention을 계산하지 않고 주변 픽셀의 정보를 aggregation함으로써 scope를 줄일 수 있다. 따라서 attention 계산에 앞서서 locally proximate tokens의 signals을 integrate한다. 이는 기존 depth-wise separable convolution을 사용한다.&lt;/p&gt; &lt;h2 id=&quot;global-sparse-attention&quot;&gt;Global Sparse Attention&lt;/h2&gt; &lt;p&gt;이제 attention에 대한 scope을 줄였으니 local window를 나타내는 representaion인 delegate token간 attention을 계산하자. 이 때 \(r \times r\) local window의 center값을 나타내는 token을 뽑아서 log-range relation을 계산하기 위해 self-attention을 계산한다. 이때 \(r\) 은 sampling rate을 나타내게 된다.&lt;/p&gt; &lt;h2 id=&quot;local-propagation&quot;&gt;Local Propagation&lt;/h2&gt; &lt;p&gt;이제 global contextual information을 계산했으니 local window에 전파해야한다. 이는 간단하게 transpose convolution으로 구성했다.&lt;/p&gt; &lt;p&gt;위의 3가지 연산을 통해 local-global-local bottleneck을 구성한다. 그리고 이는 다음과 같은 식으로 연결된다. \(X=LocalAgg(Norm(X_{in}))+X_{in}\) \(Y=FFN(Norm(X))+X\) \(Z=LocalProp(GlobalSpaarseAttn(Norm(Y)))+Y\) \(X_{out}=FFN(Norm(Z))+Z\) 이 때 FFN은 fully conntected layer 2개로 구성되어있으며 Normalization은 Layer Normalization을 사용한다.&lt;/p&gt; &lt;h2 id=&quot;model-archtecture&quot;&gt;Model Archtecture&lt;/h2&gt; &lt;p&gt;``&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/Fig2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;전체적인 모델구조는 위와 같다. Spatial resolution을 기준으로 hierachical 구조를 가지고 있다. Downsampling은 1번째만 제외하고 2x2 convolotion을 stride 2로 구성한다. 첫 번째 downsamping은 4x4 convolution은 stride 4로 연산한다. Patch embedding은 요즘 성능이 좋은 relative positional embedding[&lt;a href=&quot;https://arxiv.org/abs/1803.02155&quot;&gt;paper&lt;/a&gt;]을 사용한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/table1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;또한 scalarbility를 위해 3개의 모델 구조로 만든다.&lt;/p&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;p&gt;밴치마크를 위해서 삼성스마트폰을 사용했으며 CPU는 Snapdragon 888을 사용했다. 또한 TorchScript lite를 사용하여 50 step을 기준으로 pytorch에서 제공하는 android benchmarking app을 사용하여 측정했다. 또한 전력을 측정하기 위해서 Monsoon High Voltage Power Monitor울 Snapdragon 888 Hardware Development Kit (HDK8350)과 연결하여 측정했으며, NPU는 범용성이 떨어져서 측정하지 않았다고 한다.&lt;/p&gt; &lt;h2 id=&quot;imagenet&quot;&gt;ImageNet&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/table2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;가벼운 ViT 모델 중에서는 좋은 성능을 보여준다. 하지만 CNN 계열과 비교했을 때는 다소 아쉬운 성능을 보인다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/table3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;실험하면서 정확도가 낮을수록 전력이 줄어든다는 것을 발견했고, EdgeViT는 전력대비 정확도를 측정하면 좋은 결과를 낸다&lt;/p&gt; </description> <pubDate>Wed, 28 Jun 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/edgevit/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/edgevit/</guid> <category>backbone</category> <category>paper</category> <category>cvpr</category> <category>vit</category> <category>efficient-architecture</category> <category>backbone</category> <category>paper</category> <category>vit</category> </item> </channel> </rss>