<?xml version="1.0" encoding="UTF-8"?> <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"> <channel> <title>blank</title> <description>개발을 좋아하는 딥러닝 리서쳐 장원범입니다. </description> <link>https://www.wonbeomjang.kr/</link> <atom:link href="https://www.wonbeomjang.kr/feed.xml" rel="self" type="application/rss+xml"/> <pubDate>Tue, 24 Oct 2023 12:30:49 +0000</pubDate> <lastBuildDate>Tue, 24 Oct 2023 12:30:49 +0000</lastBuildDate> <generator>Jekyll v4.3.2</generator> <item> <title>스타트업 리서치 인턴 후기</title> <description>&lt;h1 id=&quot;왜-시작했나요&quot;&gt;왜 시작했나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/img.jpeg&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &apos;Researcher일까? Engineer일까?&apos; &lt;/p&gt; &lt;p&gt;나는 대학에 다니면서 항상 그런 고민을 했다. 고등학생 때 우연히 DeepMind에서 발표한 Playing Atari with Deep Reinforcement Learning이라는 논문을 보게되었고 인공지능에 빠져들었다. 대학 와서는 computer vision을 공부하게 되었다. 그저 인공지능이 좋아 backend, frontend 등 다른 분야보다는 인공지능 공부와 개발만 하게 되었다. 그러다 대학을 졸업할 때가 되었고, researcher와 engineer를 선택해야 할 순간이 다가왔다.&lt;/p&gt; &lt;p&gt;불행인지 다행인 건지 중앙대학교에서는 인턴을 해야지 졸업을 할 수 있었고, 관심 있는 두 군데 스타트업에 접촉하여 그중 한 회사인 뉴로클에서 인턴을 진행하게 되었다. 뉴로클을 선택한 이유는 간단했다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Computer Vision을 중점으로 한다.&lt;/li&gt; &lt;li&gt;자체 서비스를 판매하고 있다.&lt;/li&gt; &lt;li&gt;기업매출을 보니 매출도 성장세였고, 흑자를 내기 시작했다.&lt;/li&gt; &lt;li&gt;내가 내 일을 할 수 있고, 주체적으로 일할 수 있는 규모가 작지도 않고 크지도 않는 회사이다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;결론적으로 이러한 예측이 맞았고 성공적으로 인턴을 만들 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;무엇을-했나요&quot;&gt;무엇을 했나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/img.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;기본적으로 리서치 인턴의 역할을 수행했으나 후반에는 리서치 엔지니어의 역할을 하게 되었다. 퍼포먼스가 좋아서 그런지 생각보다 많은 일을 하게 되었다. (외부에 공개적으로 자료가 나간 것들만 포함했다)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Pretrained-OCR: OCR Auto-labeling 기능 추가 및 OCR 모델 성능 향상. 학습 속도 기존의 30%, 정확도 4%p 향상&lt;/li&gt; &lt;li&gt;Smart labeling (segmentation): 기술 검토 및 테스트, 모델 변환&lt;/li&gt; &lt;li&gt;Smart labeling (object detection): 기술 검토 및 모델 변환&lt;/li&gt; &lt;li&gt;회사 블로그 제작&lt;/li&gt; &lt;li&gt;리서치팀 docker 등 개발환경 관리&lt;/li&gt; &lt;li&gt;(방향성만 제시했지만) Neuro-I 성능개선, 다른 사람 연구 해결책 찾기 등등…&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;8개월동안 이걸 다 했다고?? 놀랍게도 그렇다. 개발 base로 computer vision을 공부했다 보니 구현 속도와 실험 속도가 압도적으로 빠른 것 같다.&lt;/p&gt; &lt;h1 id=&quot;무슨-경험이-도움이-되었나요&quot;&gt;무슨 경험이 도움이 되었나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/project.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;동아리에서 공모전에서 프로젝트 경험이 많았고 협업도 많이 진행했다. 그래서 computer vision, 선형대수, 수치해석, 위상수학, 표 본론 등 과 같은 지식뿐만 아니라 tensorrt, onnx, quantization 등 많은 기술, pandas, matplotlib, seaborn과 같은 데이터 시각화, 딥러닝 모델이 제품에 어떻게 탑재해야 하는지에 대한 감도 있었다. 이 모든 경험을 회사에서 다 썼다. (진짜 다 썼다) 이러한 다양한 경험은 여러 기능에 기여를 할 수 있었던 것 같다.&lt;/p&gt; &lt;h1 id=&quot;무엇을-얻었나요&quot;&gt;무엇을 얻었나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/company.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;협업&quot;&gt;협업&lt;/h2&gt; &lt;p&gt;학생 때와 차원이 다른 협업을 하게 되었다. 학교에는 기껏해야 backed, fronted고 인원도 적어서 체계도 없이 작업을 해도 되었다. 하지만 인턴을 하면서 backed, fronted뿐만 아니라 영업, 마케팅, backbend, 기획 등 여러 사람과 협업을 진행했다.&lt;/p&gt; &lt;h3 id=&quot;요구사항을-명확하게-하자&quot;&gt;요구사항을 명확하게 하자&lt;/h3&gt; &lt;p&gt;협업은 기본적으로 background가 완전하게 동일하지 않은 사람들끼리 작업을 한다. 따라서 같은 목표를 바라보고있어도 세부 사항이 다를 수 있다. 만약 이를 조정하지 않고 일을 진행하다 보면 다음에 다시 조정하고 어려울뿐더러 비용 역시 많이 발생한다.&lt;/p&gt; &lt;h3 id=&quot;방향성-설정&quot;&gt;방향성 설정&lt;/h3&gt; &lt;p&gt;하나의 기능이 만들어지기 위해서 다음과 같은 과정을 거친다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;새로운 연구 주제로 연구한다.&lt;/li&gt; &lt;li&gt;기획 및 디자인팀에서 제품 탑재 방향을 결정한다.&lt;/li&gt; &lt;li&gt;개발팀에서 제품을 개발한다.&lt;/li&gt; &lt;li&gt;마케팅팀에서 협력사에 제공할 데이터와 대외 홍보용 자료를 제작한다.&lt;/li&gt; &lt;li&gt;영업을 통해 제품을 판매한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;라이프 사이클에서 연구는 최상단을 차지한다. 따라서 첫 단추가 잘못 채워지면 전체적인 제품 방향이 엇나갈 수 있다.&lt;/p&gt; &lt;h3 id=&quot;문서화를-체계적으로-하자&quot;&gt;문서화를 체계적으로 하자&lt;/h3&gt; &lt;p&gt;내가 하는 연구를 follow up 하는 사람은 극소수다. 연구 이후 제품화할 때 조직되는 관련자들은 내가 작성해 놓은 document를 보고 작업을 시작한다. 그래서 진행한 연구와 모델을 제대로 이해시키려면 구조적으로 잘 문서화를 해야 한다.&lt;/p&gt; &lt;h2 id=&quot;지식&quot;&gt;지식&lt;/h2&gt; &lt;h2 id=&quot;tensorflow&quot;&gt;Tensorflow&lt;/h2&gt; &lt;p&gt;나는 지금까지 pytorch를 이용하여 작업을 했다. Tensorflow는 회사 들어와서 거의 처음 쓰게 된 것이다. Tensorflow는 기본적으로 eager mode가 제공되지 않아 코딩하는 것이 힘들었지만 tensorflow와 tensorflow orbit을 익히게 되는 좋은 기회가 되었다.&lt;/p&gt; &lt;h3 id=&quot;논문&quot;&gt;논문&lt;/h3&gt; &lt;p&gt;회사에 들어와서 논문을 진짜 많이 읽었다. 연구에 기반이 되는 논문뿐만 아니라 적용할 만한 최신논문, 기술 리포트, 워크숍 논문 등 다양하게 많이 읽었다. 이를 통해 ViT, active learning, OCR, anomaly detection, super resolution, backbone for edge device, federated learning 등 다양한 분야에 대해 기초지식을 쌓을 수 있었다.&lt;/p&gt; &lt;h3 id=&quot;데이터-분석&quot;&gt;데이터 분석&lt;/h3&gt; &lt;p&gt;기본적으로 데이터가 부족한 상황에서 모델의 성능을 올리는 방법을 고민을 했다. 이 때문에 사용자가 다룰 예상데이터의 특성을 분석하고 데이터에 적합한 방법론을 사용하여 성능을 높일 수 있었다.&lt;/p&gt; &lt;h2 id=&quot;일은-잘했나요&quot;&gt;일은 잘했나요?&lt;/h2&gt; &lt;p&gt;나에 대한 평가가 긍정적인 것을 보면 일을 잘했던 것 같다. 무엇보다도 나랑 같이 인턴을 진행한 분과 잘하는 것이 달라서 서로 시너지가 났던 것 같다. 원래 회사에도 리서치 인턴이 없었는데 인턴 둘이 좋은 선례를 만들어서 앞으로 계속 채용할 예정이다. (사실 면접이 완료되어 다음 리서치 인턴도 정해졌다.)&lt;/p&gt; &lt;h1 id=&quot;앞으로-무엇을-할-것인가요&quot;&gt;앞으로 무엇을 할 것인가요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/todo.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;감사하게도 회사에서 정규직 제안을 받았다. 스타트업 리서치 엔지니어에 가깝지만, 학사 출신으로 연구를 할 수 있다는 것이 흔치 않은 기회이다. 하지만 회사에서 원하는 인재와 내가 가고자 하는 방향이 달랐다. 회사에서 원하는 인재는 generalist이지만 나는 한 분에서 specialist가 되고 싶었다. 분야 또한 일반적인 모델링이 아닌 모델 경량화, hardware optimization, low cost serving 쪽으로 가고 싶다. 그리고 내가 다루어야 하는 target data가 무엇인지 명확하게 정할 수 있는 연구개발을 하고 싶다.&lt;/p&gt; &lt;p&gt;이제 학교에 다시 돌아간다. 8개월 동안 뉴로클 덕분에 좋은 경험을 했고 내 실력도 엄청나게 향상되었다. 이제 4학년 2학기이다. 졸업도 얼마 안 남아서 취업 준비나 대학원 준비를 해야겠지만 DL engineer 쪽 공부도 더욱 열심히 하면서 내가 목표하는 커리어를 만들어 가야겠다.&lt;/p&gt; </description> <pubDate>Tue, 22 Aug 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/startup-research-intern-review/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/startup-research-intern-review/</guid> <category>daily</category> <category>daily</category> </item> <item> <title>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;현재 GPT부터 시작해서 ViT 등 여러 분야에서 attention layer를 사용하고 있다. 하지만 attention layer은 dimension의 제곱에 비례하여 cost가 들어 모델의 병목인 부분이기도 하다. 이에 따라 attention layer를 효율적으로 만드는 시도가 많이 있는데 그 중 하나가 FlashAttention이다. FlashAttention은 tiling과 kernel fusion으로 기존 attention layer대비 2.4배 속도가 향상되었다. 하지만 FlashAttention 또한 기존 GPU의 이론적 성능에 25~40% 정도의 속도밖에 내지 못한다.&lt;/p&gt; &lt;p&gt;저자는 FlashAttention을 분석하던 중 thread block간 work를 partitioning할 때 비효율성을 발견했고, 이로 인해 GPU에서 low-occupancy와 불필요한 memory IO가 일어나는 것을 깨달았다. 따라서 저자는 이를 해결하기 위해 3가지를 제안했다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Output을 바꾸지 않고 non-matmul operation의 FLOPS를 줄인다.&lt;/li&gt; &lt;li&gt;Single head attention일지라도 병렬처리를 하도록 연산 순서를 변경한다.&lt;/li&gt; &lt;li&gt;Thread block내에 warps간 통신을 줄인다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;저자는 위 3가지를 통해 기존 FlashAttention대비 2배 빠른 속도를 달성하고 GPU의 이론적 성능의 50~73%까지 성능을 끌어올렸다.&lt;/p&gt; &lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt; &lt;p&gt;하드웨어 최적화에 관한 논문은 익숙하지 않으니 background까지 꼼꼼하게 읽어보자.&lt;/p&gt; &lt;h2 id=&quot;hardware-characteristics&quot;&gt;Hardware characteristics&lt;/h2&gt; &lt;h3 id=&quot;gpu-performance-cahracteristics&quot;&gt;GPU performance cahracteristics&lt;/h3&gt; &lt;p&gt;GPU는 compute element와 memeory hierarchy를 가지고 있다. Nvidia tensorcore와 같은 최신 GPU compute element는 FP16/BF16과 같은 low-precision에서 matmul operation을 최적화 하고 있다. 반면에 non-matmul operation은 최적화가 되어있지 않아 matmul operation보다 최대 16배가 느리다.&lt;/p&gt; &lt;p&gt;Memeory hierarchy에 관해서는 기본적으로 GPU는 high bandwidth memory (HBM)과 on-chip SRAM (shared memory)를 가지고 있다. A100기준 40~80GB의 HAM은 1.5~2.0TB/s의 bandwidth를 가지고 있고, 108개의 stream multiproccessor는 각각 192KB의 on-chip SRAM을 가지고 있으며 이는 19TB/s의 bandwidth를 가지고 있다. L2 cache도 있으나 이것은 사용자가 컨트롤을 못함으로 논의에서 제외하도록 하자.&lt;/p&gt; &lt;h3 id=&quot;excution-model&quot;&gt;Excution Model&lt;/h3&gt; &lt;p&gt;GPU는 수많은 thread로 구성되어있으며 thread가 모여서 thread block을 구성한다. 이 thread block은 stream multiprocessor (SMs)를 통해 실행된다. Thread block 내에서 thread는 warps이라는 단위로 묶이게 되는데 이 warp들은 공유메모리를 통해 communication을 한다.&lt;/p&gt; &lt;h2 id=&quot;standard-attention-implementation&quot;&gt;Standard Attention Implementation&lt;/h2&gt; &lt;p&gt;기존 attention은 query, key, value들 간의 연산으로 구성된다. Sequence lenght를 N, head dimension을 d라고 하자. Input sequence \(Q, K, V \in \mathbb{R}^{N\times d}\) 에 대해 attention output \(O \in \mathbb{R}^{N \times d}\) 를 계산하기 위해 아래의 식을 이용한다.&lt;/p&gt; \[S=QK^{\intercal}\in \mathbb{R}^{N\times N}\] \[P=\text{softmax}(S)\in\mathbb{R}^{N\times N}\] &lt;p&gt;\(O=PV\in \mathbb{R}^{N\times d}\) 이 때 softmax는 row-wise로 적용하게 된다. Backwardpass는 다음과 같은 과정을 거친다.&lt;/p&gt; \[dV=P^{\intercal}dO\in\mathbb{R}^{N\times d}\] \[dP=dOV^{\intercal}\in\mathbb{R}^{N\times N}\] \[dS=\text{dsoftmax}(dP)\in\mathbb{R}^{N\times N}\] \[dQ=dSK\in\mathbb{R}^{N\times d}\] \[dK=QdS^\intercal\in\mathbb{R}^{N\times d}\] &lt;p&gt;더 자세한 것은 FlashAttention 설명을 참고하면 된다.&lt;/p&gt; &lt;h2 id=&quot;flashattention&quot;&gt;FlashAttention&lt;/h2&gt; &lt;p&gt;자세한 것은 FlashAttention 설명을 참고하기 바란다. &lt;a href=&quot;https://www.wonbeomjang.kr/blog/2023/fastattention/&quot;&gt;FlashAttention 1 포스트&lt;/a&gt;&lt;/p&gt; &lt;h3 id=&quot;forward-pass&quot;&gt;Forward pass&lt;/h3&gt; &lt;p&gt;간단하게 이야기하자면 K,V를 tiling하여 병렬적으로 계산 후 on-line softmax를 통해 병렬적으로 softmax를 적용한다. 이후에 tiling한 Q를 불러와 on-chip연산으로 만든다. 또한 이를 통해 연산을 fusion할 수 있으며 Q, K, V HBM에서 load한 이후 모든 연산을 수행 후 HBM에 저장하게 된다. 연산은 다음과 같고 아래서 표시한 \(S\) 는 \(S=QK^T\) 이다.&lt;/p&gt; \[m^{(1)}=\text{rowmax}(S^{(1)})\in\mathbb{R}^{B_r}\] \[l^{(1)}=\text{rowsum}(e^{S^{(1)}-m^{(1)}})\in\mathbb{R}^{B_r\times B_c}\] \[\tilde{P}^{(1)}=\text{diag}(l^{(1)})^{-1}e^{S^{(1)}-m^{(1)}}\in\mathbb{R}^{B_r\times B_C}\] \[O^{(1)}=\tilde{P}^{(1)}V^{(1)}=\text{diag}(l^{(1)})^{-1}e^{S^{(1)}-m^{(1)}}V^{(1)}\in\mathbb{R}^{B_r\times d}\] \[m^{(2)}=\text{max}(m^{(1)},\text{rowmax}(S^{(2)}))=m\] \[l^{(2)}=e^{m^{(1)}-m^{(2)}}l^{(1)}+\text{rowsum}(e^{S^{(2)}-m})=\text{rowsum}(e^{S^{(1)}-m})+\text{rowsum}(e^{S^{(2)}-m})=l\] \[\tilde{P}^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m^{(2)}}\] \[O^{(2)}=\text{diag}(l^{(1)}/l^{(2)})^{-1}O^{(1)}+\tilde{P}^{(2)}V^{(2)}=\text{diag}(l^{(2)})^{-1}e^{s^{(1)}-m}V^{(1)}+\text{diag}(l^{(2)})^{-1}e^{s^{(2)}-m}V^{(2)}=O\] &lt;p&gt;즉, figure1처럼 vector를 쪼개고, 합치는 과정을 통해 memory IO를 줄여 연산속도를 빠르게 만들었다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h3 id=&quot;backward-pass&quot;&gt;Backward Pass&lt;/h3&gt; &lt;p&gt;Backward pass는 attention 연산을하는 과정에서 \(m, l\) 이 계산되는 되는데 이를 이용하면 다시 연산을 recompute할 수 있다.&lt;/p&gt; &lt;h1 id=&quot;3-flashattention-2&quot;&gt;3. FlashAttention-2&lt;/h1&gt; &lt;p&gt;FlashAttention은 기본적으로 non-matmul FLOPs를 줄인다. 예를들어 Nvidia의 A100 GPU는 FP16/BF16의 matmul 연산은 이론적으로 312 TFLOPs/s의 연산량을 가지지만 non-matmul 연산은 19.5 TFLOPs/s의 연산량을 가진다. 즉 non-matmul 연산이 matmul 연산보다 16배 느려 non-matmul 연산이 전체 연산의 일부를 차지하더라도 이를 최적화 시켜야한다.&lt;/p&gt; &lt;h2 id=&quot;forward-pass-1&quot;&gt;Forward pass&lt;/h2&gt; &lt;p&gt;저자는 FlashAttention에서 on-line softmax를 먼저 주목했다.&lt;/p&gt; &lt;h3 id=&quot;recaling&quot;&gt;Recaling&lt;/h3&gt; &lt;p&gt;기존에는 \(\text{diag}(l^{(2)})^{-1}\) 를 두 항 모두 rescaling 했다.&lt;/p&gt; \[O^{(2)}=\text{diag}(l^{(1)}/l^{(2)})^{-1}O^{(1)}+\tilde{P}^{(2)}V^{(2)}=\text{diag}(l^{(2)})^{-1}e^{s^{(1)}-m}V^{(1)}+\text{diag}(l^{(2)})^{-1}e^{s^{(2)}-m}V^{(2)}=O\] &lt;p&gt;이렇게 한다면 두 텀을 각각 읽어 각각 나눠야되기 때문에 memory IO가 많아진다. 따라서 마지막 결과 \(\tilde{O}^{(last)}\) 를 계산 후에 한꺼번에 \(\text{diag}(l^{(last)})^{-1}\) 으로 rescaling 한다.&lt;/p&gt; \[\tilde{O}^{(2)}=\text{diag}(l^{(1)})^{-1}O^{(1)}+e^{S^{(2)}-m^{(2)}}V^{(2)}\] \[O^{(2)}=\tilde{O}^{(2)}\text{diag}(l^{(2)})^{-1}\] &lt;h3 id=&quot;memorization&quot;&gt;Memorization&lt;/h3&gt; &lt;p&gt;Backward에 사용하기 위해서 \(m, l\) 을 저장한 후 재구성한다고 했다. 각각을 저장하는 대신 \(L^{(j)}=m^{(j)}+\text{log}(l^{(j)})\) 를 저장해도 똑같이 backward를 재구성할 수 있어 \(m, l\) 대신 \(L\) 을 저장하게 된다.&lt;/p&gt; &lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt; &lt;p&gt;결론적으로 flashattention 2에서는 다음과 같은 방법으로 attention을 구현하게 된다.&lt;/p&gt; \[m^{(1)}=\text{rowmax}(S^{(1)})\in\mathbb{R}^{B_r}\] \[l^{(1)}=\text{rowsum}(e^{S^{(1)}-m^{(1)}})\in\mathbb{R}^{B_r\times B_c}\] \[\tilde{O}^{(1)}=e^{S^{(1)}-m^{(1)}}V^{(1)}\in\mathbb{R}^{B_r\times d}\] \[m^{(2)}=\text{max}(m^{(1)},\text{rowmax}(S^{(2)}))=m\] \[l^{(2)}=e^{m^{(1)}-m^{(2)}}l^{(1)}+\text{rowsum}(e^{S^{(2)}-m})=\text{rowsum}(e^{S^{(1)}-m})+\text{rowsum}(e^{S^{(2)}-m})=l\] \[\tilde{P}^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m^{(2)}}\] \[\tilde{O}^{(2)}=\text{diag}(e^{m^{(1)}-m^{(2)}})^{-1}\tilde{O}^{(1)}+e^{S^{(2)}-m^{(2)}}V^{(2)}=e^{S^{(1)-m}}V^{(1)}+e^{S^{(2)}-m}V^{(2)}\] \[O^{(2)}=\text{diag}(l^{(2)})^{-1}\tilde{O}^{(2)}=O\] &lt;p&gt;기존 flashattention과 다르게 term 자체가 줄어들었다. Forward pass에 관한 알고리즘을 정리하자면 다음과 같다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/alg1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;backward&quot;&gt;Backward&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/alg2.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Backward 자체는 \(L\) 을 사용한다는 것 말고는 별 다른 이야기는 없다.&lt;/p&gt; &lt;h2 id=&quot;parallelism&quot;&gt;Parallelism&lt;/h2&gt; &lt;p&gt;기본적으로 GPU는 병렬처리가 가능하다. 각각 gpu thread block마다 1개의 attention module가 들어간다. 따라서 보통 # batch size x # self-attention head로 thread block을 구성하게 되고 이를 stream multiprocessor가 나눠가진다. 그래서 만약 sequence 길이가 길어 small batch size나 small number of self-attention head를 가지게 된다면 병렬처리를 잘 활용하지 못한다. 따라서 저자는 sequence length dimension에 따른 병렬처리를 하게 된다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Forward pass&lt;/strong&gt; 저자는 sequence length dimension으로 병렬처리를 한다. 하지만 이는 한 sequence내에서는 독립적으로 처리되어야함으로 다른 sequence와 통신을 하지 못하도록 구성했다. 물론 이전과 마찬가지로 batch, multi-head간 병렬처리는 유지한다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;backward pass&lt;/strong&gt; &lt;br /&gt; Algorithm 2에 의하면 column block간에 병렬처리만 한다. 위의 경우와 같이 sequence length dimension로도 병렬처리가 가능하여 추가하게 된다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;결과적으로 worker마다 병렬처리가 잘 되게 된다.&lt;/p&gt; &lt;h2 id=&quot;work-partitioning-between-warp&quot;&gt;Work Partitioning Between Warp&lt;/h2&gt; &lt;h3 id=&quot;forward&quot;&gt;Forward&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig3.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;기존의 flashattention은 \(K\) 와 \(V\) 를 각각읜 warp에 K개로 partitioning 했고, \(Q\) 는 모든 warp이 접근 가능하도록 했다. 그리고 이를 “split-K” 라고 한다. 하지만 이러한 방법은 partition된 \(QK^T\) 를 partition된 \(V\) 에 곱하게 된다. 따라서 중간계산결과를 저장하고, 읽고, 동기화를 많이해 IO에서 속도가 느려진다. 따라서 \(Q\) 를 partition하고, \(K, Q\) 를 공유하게 해이런 IO를 줄여 속도를 높이게 된다.&lt;/p&gt; &lt;h2 id=&quot;backward-1&quot;&gt;Backward&lt;/h2&gt; &lt;p&gt;“split-K”를 지양한다라는 것 밖에 이해를 못했다.&lt;/p&gt; &lt;h3 id=&quot;tuning-block-sizes&quot;&gt;Tuning block sizes&lt;/h3&gt; &lt;p&gt;Block size를 늘리면 memory IO의 수가 줄어든다. 하지만 block 수가 많아지면서 registers의 수가 늘어나고, total shared memory 크기가 커져 비효율성이 늘어난다. 많은 registers는 프로그램 속도를 느리게 만들고, total shared memory의 크기가 너무 커지면 GPU memory가 부족하다. 따라서 GPU마다 적절한 block size를 조정한다.&lt;/p&gt; &lt;h1 id=&quot;empirical-validation&quot;&gt;Empirical Validation&lt;/h1&gt; &lt;p&gt;이제 속도를 보자.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig4.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig5.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig6.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig7.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;FlashAttention-2는 기존 FlashAttention, xFormer 대비 2배의 속도를 보여줬고, Triton으로 구현된 FlashAttention보다 1.3~1.5배의 빨라진 속도를 보여줬다. 놀라운 것은 pytorch에서 naive하게 implementation한 것 대비 10배의 속도차이를 보여준다. 이로인해 기존의 large model에서도 더 빠른 연산속도를 보여준다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/table1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; </description> <pubDate>Sun, 06 Aug 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/flashattention-2/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/flashattention-2/</guid> <category>attention</category> <category>hardware-optimization</category> <category>paper</category> <category>attention</category> <category>hardware-optimization</category> <category>paper</category> </item> <item> <title>Fairness-aware Data Valuation for Supervised Learning</title> <description>&lt;h1 id=&quot;들어가기-앞서&quot;&gt;들어가기 앞서&lt;/h1&gt; &lt;p&gt;Active learning과 class imbalance를 찾던 도중 발견한 논문이자. 그래서 FairML 분야는 아는 것이 없고, 이 논문이 좋은지 나쁜지도 판단이 안 된다. 하지만 해당 논문의 개념도 간단하고, 이런 것을 고려하면서 sampling을 하는 것도 좋겠다는 생각에 논문을 정리하고자 한다.&lt;/p&gt; &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;기존의 data valueation 연구는 데이터를 특정한 performace related value로 embedding한다. Active learning에서 “해당 train instance를 추가하면 모델의 성능이 올라가겠다”를 목적으로 entropy를 사용하여 embedding하는 방법도 있다. 하지만 추천시스템에서 해당 value만 사용할 경우 특정 그룹이나 인종에 안 좋은 데이터를 추천해주는 등 여러 안 좋은 점들이 있다. 따라서 fairness도 고려되어야 하는데 기존까지는 performance와 fairness를 동시에 고려하는 연구는 많지 않았다. 저자는 entropy를 기반으로 하여 두 가지 factor를 모두 고려하여 data를 utility value에 embedding하는 framework를 제안하였고 이를 이용하여 더 좋은 data sampling, re-weighting을 할 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;fairness-aware-data-valuation&quot;&gt;Fairness-aware Data Valuation&lt;/h1&gt; &lt;h2 id=&quot;framework&quot;&gt;Framework&lt;/h2&gt; &lt;p&gt;일단 저자는 utility개념을 빌려왔는데, 해당 논문에서 utility는 performance와 fairness를 종합하는 function을 의미한다. Single data instance \(i\) 에 대해 perforamce-related valuation은 \(v_{y_i}\) , protected attribute에 대한 fairness-related valuation은 \(v_{z_i}\) 로 표시한다. Utility function은 다음과 같다.&lt;/p&gt; \[U_i(v_{y_i},v_{z_i})=\alpha(v_{y_i})+(1-\alpha)v_{z_i}\] &lt;p&gt;이 때 \(\alpha \in [0,1]\) 이다. 만약 fairness를 subgroup으로 나눈다면 다음의 식으로 확장할 수 있다.&lt;/p&gt; \[U_i(v_{y_i},v_{z_i})=\alpha(v_{y_i})+\sum_{j=1}^{k}\beta v_{z_{j_i}}\] &lt;h2 id=&quot;entorpy-metric&quot;&gt;Entorpy metric&lt;/h2&gt; &lt;p&gt;먼저 저자는 performace related value를 instance \(i\) 의 prediction \(y_{i}\) 의 entropy로 정의했다.&lt;/p&gt; \[V_{y_i}=E_{y_i}=\hat{y}_i\cdot {log}_2\hat{y}_i+(1-\hat{y}_i)\cdot {log}_2(1-\hat{y}_i)\] &lt;p&gt;해당 수식은 active learning에서 영감을 받았다. Entropy가 높다는 것은 모델이 해당 instance를 잘 예측하지 못한다는 이야기이고, 추후에 이를 집어넣으면 성능이 높아진다는 것을 예상할 수 있다. 하지만 실제 상황에서는 애매한 instance뿐만 아니라 noise 또한 entropy가 높아져서 성능이 더 낮아질 가능성도 있다. 하지만 여러 task에서 해당 방법은 성능이 준수하다는 것으로 나와서 저자는 entropy를 사용했다.&lt;/p&gt; &lt;p&gt;저자는 fairness-related valueation또한 entropy로 정의했다.&lt;/p&gt; \[V_{z_i}=E_{z_i}=\hat{z}_i\cdot {log}_2\hat{z}_i+(1-\hat{z}_i)\cdot {log}_2(1-\hat{z}_i)\] &lt;p&gt;이 수식이 왜 되는지는 이해가 잘 안되지만 저자에 말은 이러하다.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;In the second case, where the variable in question (Z) is not the target for the task at hand, and is seen by the model even at inference time, we prioritize observations where the model had more difficulty in establishing a relationship among X, Y, and Z, leveraging the fact the model has no explicit incentive to draw such relationships. This is directly related to mitigating the base bias condition of the taxonomy of Pombal et al. (2022a) ( P[X, Y] 6= P[X, Y | Z] ), and so related to promoting fairness.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fairness-aware data valuation for supervised learning, Pombal et al, 2023 -&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;p&gt;내가 이해한 대로 적자면 여러 예측 모델이 있는데 이들 모두의 예측값의 entropy가 높다면 bias가 없는 데이터이다. 하지만 특정 모델에서 entropy가 높아 잘 예측한다면 그 데이터는 특정 모델에 적합한 bias가 있는 데이터일 것이다. 따라서 fairness 또한 entropy로 표현할 수 있게 되는 것이다.&lt;/p&gt; &lt;p&gt;이렇게 구한 perforamce related valuation과 fairness related valuation을 종합하여 utility function을 제작하게 된다.&lt;/p&gt; \[U_i=\alpha E_{y_i}+(1-\alpha)E_{z_i}\] &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt; &lt;p&gt;Dataset으로 bank account-opening fraud dataset을 사용했다. 해당 데이터에서 fraud rate는 1%이다. 해당 데이터는 사기계좌를 찾는 것으로 True Positive Rate (TPR)이 높아야한다. 반면에 False Positive Rate (FPR)가 높으면 사용자의 계좌사용이 불편해지기 떄문에 FPR을 낮추는 것을 목표로 하고 있다.&lt;/p&gt; &lt;h1 id=&quot;model&quot;&gt;Model&lt;/h1&gt; &lt;p&gt;Model은 tublar data에서 SOTA를 찍고있는 LightGBM을 사용한다.&lt;/p&gt; &lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt; &lt;p&gt;Data sampling, re-weighting은 다음의 과정을 거친다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;각각의 예측모델 Y, Z에 대해 tranining instance의 value를 계산한다.&lt;/li&gt; &lt;li&gt;각각의 train instance에 대해 utility value를 계산한다.&lt;/li&gt; &lt;li&gt;Utility value를 기반으로 Utility-aware prevalence sampling (UASP) 또는 Utility-aware reweighting (UAR) 수행한다. 이 때 UASP는 under sampling을 이야기하는 것이다.&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/fado/fig1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;결과는 위와 같다. 해당 표를 보는 방법은 모르나 저자말로는 자신이 제안한 framework가 balance를 잘 잡는다고 주장한다.&lt;/p&gt; </description> <pubDate>Wed, 12 Jul 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/fado/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/fado/</guid> <category>data-sampling</category> <category>fair-ml</category> <category>data-sampling</category> <category>fair-ml</category> </item> <item> <title>TinyViT</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;ViT는 많은 발전이 있었지만 edge device에 적용하기에는 모델이 너무 컸다. 하지만 모두들 알다싶이 작은 모델 representation이 작다. 따라서 small dataset에 적합할지 몰라도 large dataset에서는 빠르게 saturation이 되면서 underfitting이 발생하게 되어 잘 사용하지 못한다. 저자는 이에 대하여 고민을 했고 knowledge distillation을 사용해야한다는 결론에 도달하게 된다. 이에따라 small model이 downstream task에 transfer가 잘 되는 것을 확인했다.&lt;br /&gt; 하지만 기존의 knowledge distillation은 train시 teacher network를 메모리에 올리기 때문에 teacher network가 gpu memory를 다수 잡아먹게 되어 batch size 조절이 힘들다. 또한 학습에 필요한 soft-label을 그때그때 제작하기 때문에 학습속도도 느려진다. 따라서 저자는 이를 해결하기 위해 data augmentation과 soft-label을 먼저 저장하여 student model이 학습 시 이를 사용하는 방법을 사용했다. 결론적으로 TinyViT는 21M의 파라미터로 ImageNet에서 84.8% top-1 accuracy를 달성했으며 88M으로 85.8%를 달성한 Swin-B보다 4.2배 적은 파라미터이다. 이미지 크기를 키웠을 때 SOTA를 찍었으며 COCO object detection도 Swin-T 우위에 있다는 것을 확인했다.&lt;/p&gt; &lt;h1 id=&quot;tinyvit&quot;&gt;TinyViT&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;fast-pretraining-distillation&quot;&gt;Fast Pretraining Distillation&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;위 사진에서 볼 수 있듯 small model을 바로 큰 데이터셋에 학습하면 성능이 낮아진다. 따라서 knowledge distillation을 이용하려고 하는데 저자는 downstream task를 위해 finetuning-distillation이 아니라 pretraining distillation에 주목을 한다. 이에 따라 ImageNet-1K에는 distillation없이 finetuning을 이용하여 학습을 진행한다. &lt;br /&gt; 하지만 pretraining distillation은 large teacher model이 많은 데이터셋을 inference 해야 하기 때문에 비용이 많이 들고 비효율적이다. 따라서 Fig.2에서 나오듯 사전에 teacher model에 사용되는 augmentation과 그에 맞는 label을 만들어 스토리지에 저장하게 된다. 이를 통해 student model이 학습 시 teacher model의 forward computation 없이 knoweldge distillation을 할 수 있다.&lt;br /&gt; 수학적으로 표현하자면, input image를 \(x\), RandAugment나 CutMix와 같은 strong augmentation을 \(\mathcal{A}\)라 하자. Teacher 학습시 augmentation \(\mathcal{A}\), techer prediction \(\hat{y}=T(\mathcal{A}(s))\)의 pair인 \((\mathcal{A},\hat{y})\)를 저장한다. 이 때 \(T(\cdot )\)은 teacher model을 의미한다. 그 후 student \(S(\cdot)\)과 cross entropy \(CE(\cdot)\)에 대해 loss 연산을 진행한다.&lt;/p&gt; \[\mathcal{L}=CE(\hat{y},S(\mathcal{A}(x)))\] &lt;h3 id=&quot;sparse-soft-label&quot;&gt;Sparse Soft Label&lt;/h3&gt; &lt;p&gt;Teacher model의 output을 그대로 저장하는 것은 storage를 많이 사용한다. 따라서 저자는 \(\hat{y}\)의 top-K value과 그들의 indices \(\{\mathcal{I}(k)\}_{k=1}^K\)만을 저장하고 나머지는 label smoothing을 이용하여 reconstruct한다. 학습 시 ground truth인 hard label을 사용하지 않고 pseudo label인 soft label만을 사용하여 학습한다.&lt;/p&gt; \[\hat{y}_c = \begin{cases} \hat{y}_{\mathcal{I}(k)} &amp;amp; \text{if} \ c=\mathcal{I}(k)\\ \frac{1-\sum_{k=1}^K\hat{y}_{\mathcal{I}(k)}}{C-K} &amp;amp; \text{otherwise} \end{cases}\] &lt;p&gt;이 때 \(\hat{y}_c\)는recovered teacher logit for student model distillation이라고 하고, \(\hat{y}=[\hat{y}_1, ... ,\hat{y}_c, ... ,\hat{y}C]\)이다. 만약 \(K &amp;lt;&amp;lt; C\)라면 메모리 감소가 크다.&lt;/p&gt; &lt;h3 id=&quot;data-augmentation-encoding&quot;&gt;Data augmentation encoding&lt;/h3&gt; &lt;p&gt;Data augmentation 정보 또한 그대로 저장하면 storage를 많이 사용한다. (rotation degree, crop coordinate 등) 따라서 set of data augmenatation parameter를 \(\mathbf{d}\)라고 하고, encoder \(\mathcal{E}(\cdot)\) 를 사용하여 \(d_0=\mathcal{E}(\mathbf{d})\)로 변환하여 저장한다. 그 후 training process에는 \(\mathcal{E}^{-1}(\cdot)\)을 decoder로사용하여 augmetation을 진행한다.&lt;/p&gt; &lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt; &lt;p&gt;저자는 기본적으로 ViT를 기반으로 모델을 설계했다. 또한 Swin Transformer나 LeViT와 같이 hierarchical한 구조를 채택했다. Patch embedding block은 kernel size 3, stride 2, padding 1인 두 개의 convolution을 사용했다. 하지만 처음부터 끝까지 transformer block을 사용하는 것은 연산적으로 무리가 된다. 따라서 MobileNetV2에서 사용하는 MBConv를 사용하여 stage 1과 downsampling을 구성했다. 또한 MBConv에 residual conntection 또한 적용했다. 마지막 3개 stage 에서는 transformer block을 사용했다. 모든 layer와 block에서 activation function은 요즘 성능이 좋은 GELU를 이용했고, normalization은 conolution layer에는 batch norm, linear layer에는 layer norm을 사용했다.&lt;/p&gt; &lt;h3 id=&quot;contraction-factors&quot;&gt;Contraction factors&lt;/h3&gt; &lt;p&gt;Modern model와 같이 factor를 사용하여 모델의 크기를 조절한다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;\(\gamma_{D_{1-4}}\): embeded demension in four stages&lt;/li&gt; &lt;li&gt;\(\gamma_{N_{1-4}}\): number of block in four stages&lt;/li&gt; &lt;li&gt;\(\gamma_{W_{2-4}}\): window size of last three stages&lt;/li&gt; &lt;li&gt;\(\gamma_{R}\): channel expansion ratio of MBConv&lt;/li&gt; &lt;li&gt;\(\gamma_{M}\): channel expansion MLP of transformer blocks&lt;/li&gt; &lt;li&gt;\(\gamma_{E}\): the dimension of each head in multi-head attention&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;모든 모델이 \(\gamma_{D_{1-4}}\)을 제외하고 같은 factor를 갖는다.&lt;/p&gt; \[\{\gamma_{N_1}, \gamma_{N_1}, \gamma_{N_1}, \gamma_{N_1}\} = \{2, 2, 6, 2\}\] \[\{\gamma_{W_1}, \gamma_{W_1}, \gamma_{W_1}\} = \{7, 14, 7\}\] \[\{\gamma_{R}, \gamma_{M}, \gamma_{E}\} = \{4, 4, 32\}\] &lt;p&gt;Embeded dimensiondm \(\{\gamma_{D_1}, \gamma_{D_2}, \gamma_{D_3}, \gamma_{D_4}\}\)는 TinyViT-21M: {96, 192, 384, 576},TinyViT-11M: {64, 128, 256, 448},TinyViT-5M: {64, 128, 160, 320} 으로 구성했다.&lt;/p&gt; &lt;h1 id=&quot;analysis-and-discussion&quot;&gt;Analysis and Discussion&lt;/h1&gt; &lt;p&gt;작은 모델은 Image21K와 같은 large scale dataset에서는 underfitting이 발생하여 학습이 잘되지 않는다. 그렇다면 다음 두 가지의 의문이 들 수 있다.&lt;/p&gt; &lt;h3 id=&quot;1-어떠한-요소가-small-model의-학습을-방해하는가&quot;&gt;1. 어떠한 요소가 small model의 학습을 방해하는가?&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;이를 확인하려면 IN-21K의 문제점을 알아야한다. Label의 오류와 비슷한 이미지가 다른 label인 경우가 존재한다. 이와 같은 데이터가 전체의 10%로 hard sample이라고 불린다. Large model은 이러한 hard sample을 학습할 수 있자만 small model은 hard sample을 예측하기 어렵고 이는 small model을 large model 보다 낮은 성능으로 이끈다. 이를 증명하기 위해 강력한 모델인 Florence를 이용하여 data마다 florence prediction의 top 5 중에 label이 존재하는지 찾아봤다. 그 결과 ImageNet-21K의 14%에 해당하는 hard sample을 골라냈다. 또한 Florence를 이용하여 TinyViT-21M/Swin-T에 knowledge distllation을 적용했다. 그 결과 small model을 곧바로 ImageNet-21K에 학습하는 것은 성능에서 제약이 존재했고, ImageNet-21K에서 hard sample을 제거하면 1%p 정도 성능향상이 있었다. 놀라운 점은 knowledge distillation이 hard sample의 defact을 줄여줬다.&lt;/p&gt; &lt;h3 id=&quot;2-왜-distillation이-small-model의-large-data-학습에-도움이-되는가&quot;&gt;2. 왜 distillation이 small model의 large data 학습에 도움이 되는가?&lt;/h3&gt; &lt;p&gt;Student model이 teacher model의 지식을 바로 학습할 수 있기 때문이다. Gound truth는 각 물체간 상간관계를 보여주지 못한다. 하지만 teacher model의 inference 값은 그것을 알 수 있다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;위의 그림을 보았을 때도 distillation 없이 TinyViT를 학습했을 때 비슷한 물체간 상관관계가 크지 않았으나 distillation 진행시 상관관계가 크게 나와 teacher model을 제대로 따라할 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;h2 id=&quot;impact-of-pretraining-distillation-on-existing-small-vits&quot;&gt;Impact of pretraining distillation on existing small ViTs&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;앞서 설명을 했 듯 ImageNet-21K로 distillation을 진행한 후 down stream task를 진행했다. 이 학습방법론이 효과가 있을지 다른 가벼운 ViT를 사용하여 실험한 결과 해당 방법론은 효과가 있었다.&lt;/p&gt; &lt;h2 id=&quot;impact-of-pretraining-data-scale&quot;&gt;Impact of pretraining data scale&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Distillation을 pretranining할 때 사용한다. 이 때 사용한 데이터에 따라 성능차이를 확인해보니 데이터가 많아질수록 성능이 좋아졌다.&lt;/p&gt; &lt;h2 id=&quot;impact-of-the-number-of-saved-logits&quot;&gt;Impact of the number of saved logits&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig5.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Soft-label의 용량을 줄이기 위해서 top-K만 저장한다고 했다. K가 늘어날 수록 메모리 사용량은 늘어나나 정확도는 비슷하다. 따라서 이를 밸런스 있게 가져가기 위해서 ImageNet-1K에서는 \(K=10\), ImageNet-21K에서는 \(K=100\)으로 설정했다.&lt;/p&gt; &lt;h3 id=&quot;impact-of-teacher-models&quot;&gt;Impact of teacher models&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Teacher Model을 어떤 것으로 설정하는지에 따라서도 성능차이가 났다. 더 강력하고 좋은 teacher model을 사용하면 성능이 좋아지나 그 만큼 학습시간이 늘어나는 것을 볼 수 있었다.&lt;/p&gt; &lt;h2 id=&quot;result-on-imagenet&quot;&gt;Result on ImageNet&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig6.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;위 표에서 볼 수 있듯 TinyViT는 경량화 ViT 중에서 좋은 성능을 내었다.&lt;/p&gt; &lt;h2 id=&quot;transfer-learning-results&quot;&gt;Transfer Learning Results&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table5.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Self-supervised learning으로 MOCO를 사용하여 Linear probe를 하거나 few shot learning을 할 때도 성능이 좋았다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table6.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;또한 MS COCO를 사용하여 object detection에 사용할 때도 성능이 좋았다.&lt;/p&gt; &lt;h1 id=&quot;comment&quot;&gt;Comment&lt;/h1&gt; &lt;p&gt;사실 TinyViT 자체는 knowedge distillation 내용이라 새로운 것이 별로 없다. 결과들을 미리 저장하는 것 역시 가끔씩 쓰는 트릭이라 크게 중요하지 않다. 해당 논문은 그런 것 보다 small ViT가 ImageNet에서 hard example에 취약하여 이를 골라내는 과정이 중요한 것 같다. 또한 저자는 말하지 않았지만 (몰랐을 수도 있지만) sparse soft label 자체가 teacher model의 noise를 제거하는 역할을 하여 small ViT에 더 좋은 성능을 가져와 주었을 것이다. (비교적 어려운 이미지들은 confidence가 낮은 이미지들의 probability가 noise로 작용하기도 한다.) 끝마치면서 드는 생각은 ImageNet-21K로 pretrain하고 이를 ImageNet-1K로 finetuning하는 것이 workshop에서 맞는 방법이지 않나 싶으면서도 ViT를 생각해보면 그럴수도 있다는 생각이 든다.&lt;/p&gt; </description> <pubDate>Wed, 28 Jun 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/tinyvit/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/tinyvit/</guid> <category>backbone</category> <category>paper</category> <category>cvpr</category> <category>vit</category> <category>efficient-architecture</category> <category>knowledge-distillation</category> <category>backbone</category> <category>paper</category> <category>vit</category> <category>knowledge-distillation</category> </item> <item> <title>EdgeViT</title> <description>&lt;h1 id=&quot;intorduction&quot;&gt;Intorduction&lt;/h1&gt; &lt;p&gt;ViT는 global한 representation을 학습하면서 imagenet banchmark에서 압도적인 성능을 내고있다. 하지만 self-attention이라는 연산의 비용이 커서 inference speed나 power efficiency가 떨어진다. 이를 해결하기 위한 기존의 연구는 크게 다음과 같았다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Spatial Resolution에서 hierarchical architecture을 만들어 연산량을 줄인다.&lt;/li&gt; &lt;li&gt;Locally-grouped self-attention mechanism을 사용한다.&lt;/li&gt; &lt;li&gt;Key, value를 pooling하여 subsampling한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;하지만 이러한 방법은 time cost가 중요한 mobile이나 edge platform에서 충분하지 못하다. 따라서 저자는 다음의 요소를 고려하여 EdgeViT를 설계했다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Inference efficiency&lt;/strong&gt; EdgeViT는 on-device에서 사용할 정도로 가볍고 에너지를 적게 사용해야한다. 기존에는 이를 측정하기위해 FLOPS를 주로 사용했지만 이는 latency과 energy consumption을 제대로 반영하지 못한다. 따라서 FLOPS는 추정치로만 참고하고 실제 mobile device에서 밴치마크를 진행한다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Model size&lt;/strong&gt; 현대 스마트폰은 RAM이 32GB일정도로 메모리량이 충분하다. 따라서 절대적인 모델크기를 고려하는 것은 현대 mobile device에는 적합하지 않아서 이는 크게 고려하지 않는다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Implementation Friendliness&lt;/strong&gt; 현실적으로 implementation을 하기위해서는 ONNX, TensorRT, TorchScript등 기존의 framework와의 호환성을 고려해야한다.&lt;/p&gt; &lt;p&gt;이를 고려하여 저자는 local-global-local bottleneck을 제안했고 이는 energy efficient하고 inference speed도 빠르다.&lt;/p&gt; &lt;h1 id=&quot;local-global-local-bottleneck&quot;&gt;Local-Global-Local-BottleNeck&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/Fig3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;ViT의 성능이 좋은 이유는 global한 representation을 학습하기 유리하기 때문이다. Multi-head self-attention이 이를 가능하게 만들었으니 이를 모사하는 연산을 만드는 것을 필수적이다. 따라서 저자는 factorization을 통해 해당 연산을 3가지로 쪼개 모사했다.&lt;/p&gt; &lt;h2 id=&quot;local-aggregation&quot;&gt;Local Aggregation&lt;/h2&gt; &lt;p&gt;연산을 가볍게 만들기 위해서는 절대적인 연산량을 줄여야 한다. 다행이도 이미지에서는 주변 픽셀은 비슷하다는 inductive bias가 존재한다. 따라서 전체 token에 대하여 attention을 계산하지 않고 주변 픽셀의 정보를 aggregation함으로써 scope를 줄일 수 있다. 따라서 attention 계산에 앞서서 locally proximate tokens의 signals을 integrate한다. 이는 기존 depth-wise separable convolution을 사용한다.&lt;/p&gt; &lt;h2 id=&quot;global-sparse-attention&quot;&gt;Global Sparse Attention&lt;/h2&gt; &lt;p&gt;이제 attention에 대한 scope을 줄였으니 local window를 나타내는 representaion인 delegate token간 attention을 계산하자. 이 때 \(r \times r\) local window의 center값을 나타내는 token을 뽑아서 log-range relation을 계산하기 위해 self-attention을 계산한다. 이때 \(r\) 은 sampling rate을 나타내게 된다.&lt;/p&gt; &lt;h2 id=&quot;local-propagation&quot;&gt;Local Propagation&lt;/h2&gt; &lt;p&gt;이제 global contextual information을 계산했으니 local window에 전파해야한다. 이는 간단하게 transpose convolution으로 구성했다.&lt;/p&gt; &lt;p&gt;위의 3가지 연산을 통해 local-global-local bottleneck을 구성한다. 그리고 이는 다음과 같은 식으로 연결된다. \(X=LocalAgg(Norm(X_{in}))+X_{in}\) \(Y=FFN(Norm(X))+X\) \(Z=LocalProp(GlobalSpaarseAttn(Norm(Y)))+Y\) \(X_{out}=FFN(Norm(Z))+Z\) 이 때 FFN은 fully conntected layer 2개로 구성되어있으며 Normalization은 Layer Normalization을 사용한다.&lt;/p&gt; &lt;h2 id=&quot;model-archtecture&quot;&gt;Model Archtecture&lt;/h2&gt; &lt;p&gt;``&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/Fig2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;전체적인 모델구조는 위와 같다. Spatial resolution을 기준으로 hierachical 구조를 가지고 있다. Downsampling은 1번째만 제외하고 2x2 convolotion을 stride 2로 구성한다. 첫 번째 downsamping은 4x4 convolution은 stride 4로 연산한다. Patch embedding은 요즘 성능이 좋은 relative positional embedding[&lt;a href=&quot;https://arxiv.org/abs/1803.02155&quot;&gt;paper&lt;/a&gt;]을 사용한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/table1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;또한 scalarbility를 위해 3개의 모델 구조로 만든다.&lt;/p&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;p&gt;밴치마크를 위해서 삼성스마트폰을 사용했으며 CPU는 Snapdragon 888을 사용했다. 또한 TorchScript lite를 사용하여 50 step을 기준으로 pytorch에서 제공하는 android benchmarking app을 사용하여 측정했다. 또한 전력을 측정하기 위해서 Monsoon High Voltage Power Monitor울 Snapdragon 888 Hardware Development Kit (HDK8350)과 연결하여 측정했으며, NPU는 범용성이 떨어져서 측정하지 않았다고 한다.&lt;/p&gt; &lt;h2 id=&quot;imagenet&quot;&gt;ImageNet&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/table2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;가벼운 ViT 모델 중에서는 좋은 성능을 보여준다. 하지만 CNN 계열과 비교했을 때는 다소 아쉬운 성능을 보인다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/table3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;실험하면서 정확도가 낮을수록 전력이 줄어든다는 것을 발견했고, EdgeViT는 전력대비 정확도를 측정하면 좋은 결과를 낸다&lt;/p&gt; </description> <pubDate>Wed, 28 Jun 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/edgevit/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/edgevit/</guid> <category>backbone</category> <category>paper</category> <category>cvpr</category> <category>vit</category> <category>efficient-architecture</category> <category>backbone</category> <category>paper</category> <category>vit</category> </item> <item> <title>Integral Neural Network</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;이 논문은 CVPR2023에 accept된 논문이고 award 후보에도 올랐다. 주된 초점은 모델 사이즈별로 성능하락 없이 pruning하는 내용인데, 상당히 아이디어도 괜찮고 앞으로 응용가능성도 있어보인다. 하지만 가벼운 네트워크만 실험을 하여 EfficientNet-L과 같은 무거둔 네크워크나 ViT와 같은 self-attention 메커니즘을 사용한 네트워크의 결과는 없다. 따라서 실제로 사용하려면 이론적토대가 더 필요할 것으로 보인다. 이제 논문을 살표보자.&lt;br /&gt; 기존의 DNN은 많은 분야에서 좋은 성능을 냈다. Kolmogorov superposition theorem과 universal approximation theorem에서 DNN은 어떠한 continuous multivariate function이라도 모사할 수 있다고 이야기한다. 이러한 이론에 따라서 DNN은 발전했고 파라미터 수도 많아졌다. 연구자들을 이를 극복하기 위해 경량화 기법으로 pruning, quantization, NAS를 사용하여 모델의 크기를 줄였다. 하지만 이와 같은 방법은 모델 사이즈가 줄어듬에 따라 성능하락이 발생했고 각각의 사이즈의 모델을 따로 학습시켜야한다는 단점이 있다. 따라서 저자는 neural network에서 사용하는 discrete한 representaiton을 continuous representation으로 바꾸어 inference시 quadrature approximation procedure를 통해 여러 크기의 모델을 만들 수 있도록 제안했다. 따라서 기존에 있는 CNN, FC 연산과 같은 discrete operation을 integral operator로 교체하는 과정을 거치게 된다.&lt;/p&gt; &lt;h1 id=&quot;neural-networks-and-integral-operators&quot;&gt;Neural Networks and Integral Operators&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/fig2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;설명을 하기 앞서 integral operator에 대한 설명을 먼저 하겠다.\(W(x),S(x)\) 가 unitvariate function이라고 하자. 이 때 다음과 같은 식이 성립한다.&lt;/p&gt; \[\int_0^1 W(x)S(x)dx \approx \sum_{i=0}^nq_iW(x_i)S(x_i)=\vec{w_q} \cdot \vec{s}\] &lt;p&gt;이때 다음과 같은 식이 성립한다.&lt;/p&gt; \[\vec{w_q}=(q_0W(x_0),...,q_nW(x_n))\] \[\vec{s}=(S(x_0),...,S(x_n))\] \[\vec{P}^x=(x_0,...,x_n), 0 = x_0 &amp;lt; x_1 &amp;lt; ... &amp;lt; x_{n-1} &amp;lt; x_n = 1\] &lt;p&gt;위 식은 “두 univariate function의 곱의 적분은 수치적분을 이용한 두 벡터의 내적에 근사한다”는 것을 의미한다. 이\((P, q)\) 쌍은 a numerical integration method라고 부른다. 간단하게 생각하자면 고등학교 때 배운 정적분과 부분 적분의 관계를 떠올리면 된다.&lt;/p&gt; &lt;h2 id=&quot;dnns-layers-as-integral-operators&quot;&gt;DNNs layers as integral operators&lt;/h2&gt; &lt;p&gt;기본적인 이론 토대를 만들었으니 이제 어떻게 적용할 수 있는지 보자.&lt;/p&gt; &lt;h3 id=&quot;convolution-or-cross-correlation-layer&quot;&gt;Convolution or cross-correlation layer&lt;/h3&gt; &lt;p&gt;\(\mathbf{x^s}\) 는 dimension을 표현하는 scalar 혹은 vector라고 정의하자. Convolution layer는 multi-channel을 다루기 때문에 이를 반영해야 한다. Convolution의 continuous operation은 integral로 정의되므로 다음과 같은 식을 따른다.\(\lambda\) 는 trainable parameter를 의미한다.&lt;/p&gt; &lt;p&gt;일단 Convolution weight, Input, Output을 다음과 같이 표현하자.&lt;/p&gt; \[F_W(\lambda,x^{out},x^{in}, \mathbf{x^s}), F_I(x^{in}, \mathbf{x^s}), F_O(x^{out}, \mathbf{x^{s^\prime}})\] &lt;p&gt;Convolution operation을 Integral operator로 다음과 같이 표현할 수 있다.&lt;/p&gt; \[F_O(x^{out},x^{s^\prime})=\int_\Omega F_W(\lambda,x^{out},x^{in}, \mathbf{x^s})F_I(x^{in}, \mathbf{x^s}+\mathbf{x^{s^\prime}})dx^{in}d\mathbf{x^s}\] &lt;h3 id=&quot;fully-connected-layer&quot;&gt;Fully-connected layer&lt;/h3&gt; &lt;p&gt;Linear layer는 기본적으로 matrix multiplication 연산으로 이루어져있으며 vector에서 vector로의 변환 연산이다. 또한 이는 1차원 연산이기 때문에 FC weight, input, output을 다음과 같이 정의한다.&lt;/p&gt; \[F_W(\lambda,x^{out},x^{in}), F_I(x^{in}), F_O(x^{out})\] &lt;p&gt;그리고 FC 연산을 다음과 같이 정의한다.&lt;/p&gt; \[F_O(x^{out})=\int_0^1 F_W(\lambda,x^{out},x^{in})F_I(x^{in})dx^{in}\] &lt;h3 id=&quot;pooling-and-activation-functions&quot;&gt;Pooling and Activation Functions&lt;/h3&gt; &lt;p&gt;Pooling 연산은 간단하게 정의된다. Average pooling은 constant function을 이용한 convolution 연산으로 정의되고, max pooling은 signal discretization으로 정의할 수 있다. 또한 activation function은 discrete한 representation에서 적용하면 되는데 그 이유는 다음의 식이 성립하기 때문이다.&lt;/p&gt; \[\mathcal{D}(ActFunction(x),P_x)=ActFunction(\mathcal{D}(x,P_x))\] &lt;p&gt;\(\mathcal{D}\) 는 주어진 partition\(P_x\)에 대해 discretization operation을 말하는 것이다. 즉, Continuous signal의 activate function을 discretizing한 것은 discretized signal에 activation function을 적용한 것과 동일하다는 관계식이 성립한다.&lt;/p&gt; &lt;h3 id=&quot;evaluation-and-backpropagation-through-integration&quot;&gt;Evaluation and backpropagation through integration&lt;/h3&gt; &lt;p&gt;Integral Neural Network (INN)은 빠른 evalution을 위하여 integral kernel을 discretization하는 과정을 거치게 된다. 이를 통해 기존의 conventional layer에 weight을 전달할 수 있고, pytorch와 같은 framework나 GPU와 호환이 된다. Backpropagation은 기존과 같은 chain-rule이 사용된다. 이는 Appendix A에 설명이 들어가있는데 간단하게 lemma만 보자면 다음과 같다.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt; (Neural Integral Lemma) Given that an integral kernel\(F(λ, x)\) is smooth and has continuous partial derivatives\(\frac{\partial(\lambda,x)}{\partial\lambda}\) on the unit cube\([0, 1]^n\) n, any composite quadrature can be represented as a forward pass of the corresponding discrete operator. The backward pass of the discrete operator corresponds to the evaluation of the integral operator with the kernel\(\frac{\partial(\lambda,x)}{\partial\lambda}\) using the same quadrature as in the forward pass.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;continuous-parameters-representation&quot;&gt;Continuous parameters representation&lt;/h2&gt; &lt;p&gt;더 풍부하고 일반화된 continuous parameter representation을 위해서 inference time에 어떠한 해상도(sampling rate)로든 sampling을 하야한다. 따라서 저자는 continuous한 weight을 [0, 1]에서 존재하는 line segment에 interpolation kernel의 linear combination으로 정의한다. 따라서 다음과 같이 나타낼 수 있다.&lt;/p&gt; \[F_W(\lambda,x)=\sum_{i=0}^m\lambda_i u(xm-i)\] &lt;p&gt;여기서\(m$과\)n$$은 interpolation node의 개수와 그들의 값이다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/fig4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;설명이 어렵게 되어있지만 개념은 간단하다. 저자들은 kernel wieght를 각 [0, 1] 사이의 균일한 segment로 저장하고 이를 interpolation을 통해 continuous한 kernel과 representaion을 제작한다. 이때 cubic spline interpolation을 사용하는데 이는 GPU상에서 빠르지만 linear interpolation보다 더 정확한 정보를 담을 수 있기 때문이다. 따라서 fully-connected layer의 weight는 다음과 같이 저장된다.&lt;/p&gt; \[F_W(\lambda,x^{out},x^{in})=\sum_{i,j}\lambda_{i,j}u(x^{out}m^{out}-i)u(x^{in}m^{in}-j)\] &lt;p&gt;또한 evaluation을 위해 discrete하게 export 할 때 다음과 같이 export 하게 된다.&lt;/p&gt; \[W_q[k,l]=q_lW[k,l]=q_lF_W(\lambda,P_k^{out},P_l^{in})\] \[\vec{P}^{out}=\{kh^{out}\}_k, \vec{P}^{in}=\{lh^{in}\}_k\] &lt;h3 id=&quot;trainable-partition&quot;&gt;Trainable partition&lt;/h3&gt; &lt;p&gt;저자는 처음에 fixed sampling step으로 uniform한 partition을 만들 생각이었다. 하지만 non-uniform한 sampling이 partition size를 키우지 않고 numerical integration을 향상시킬 수 있다는 것을 발견했다. 따라서 trainable한 partition을 도입해 자유도를 늘렸으며 이를 통해 좀 더 smooth하고 효율적인 partition을 할 수 있게 되었다. 후술하겠지만 이는 새로운 pruning 방법에 쓰이게된다. 따라서 partition parameterization\(\vec{P}\) 는 다음과 같은 식을 따르게 된다.&lt;/p&gt; \[\vec{\delta}_{norm}=\frac{\vec{\delta}^2}{sum(\vec{\delta}^2)}, \vec{P}=cumsum(\vec{\delta}_{norm})\] &lt;h1 id=&quot;training-integral-neural-networks&quot;&gt;Training Integral Neural Networks&lt;/h1&gt; &lt;p&gt;딥러닝 방법론이 많아지면서 현재는 ResNet과 같은 좋은 network가 존재한다. 따라서 이를 활용한다면 INN에 좋은 initialization이 될 수 있다. 따라서 저자들은 기존 discrete network를 smooth structure로 만들기 위해 weight를 permute하는 방법론을 제시했다.&lt;/p&gt; &lt;h3 id=&quot;conversion-of-dnns-to-inns&quot;&gt;Conversion of DNNs to INNs&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/fig5.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;network를 가능하면 smooth한 structure로 만들기 위해서 weight tensor의 특정방향의 total variation이 최소가 되도록 만들어아햔다. 이 문제는 많이 알려진 Traveling Salesman Problem (TSP)문제로 환원될 수 있다. 이 task에서는\(c^{out}\) dimension에 따라 weight tensor는 city로 대응되고 distance는 total variance로 대응된다. 따라서 optimal permutation은 route로 대응되어 다음 식을 최소화 하는 것으로 문제를 해결하게 된다.&lt;/p&gt; \[min_{\sigma \in S_n}\sum|W[\sigma(i)]-W[\sigma(i+1)]|\] &lt;p&gt;\(W\) 는 weight tensor,\(\sigma\) 는 permutation,\(\sigma(i)\) 는 i-th element의 새로운 위치이다.&lt;/p&gt; &lt;h3 id=&quot;optimization-of-continuous-weights&quot;&gt;Optimization of continuous weights&lt;/h3&gt; &lt;p&gt;INN은 보통의 gradient descent-based method를 사용할 수 있으며 Lemma 1을 사용하여 다음의 학습 알고리즘으로 학습을 진행하게 된다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/algorithm1.png&quot; width=&quot;60%&quot; /&gt; &lt;/p&gt; &lt;p&gt;또한 매 iteration마다 partition size가 달라질 수 있기 때문에 다음과 같은 식을 objective로 설정하여 다른 cube partition간 차이를 최소화한다.&lt;/p&gt; \[|Net(X,P_1)-Net(X,P_2)|\leq|Net(X,P_1)-Y|+|Net(X,P_2)-Y|\] &lt;h1 id=&quot;expertimant&quot;&gt;Expertimant&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/pipline.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;실험 시나리오는 3개로 설정했다.&lt;/p&gt; &lt;h2 id=&quot;pipeline-a-comparison-with-discrete-nns&quot;&gt;Pipeline A. Comparison with discrete NNs&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/table1.png&quot; width=&quot;60%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Discrete 모델을 변환하여 finetuning한 INN이 discrete 모델보다 성능이 비슷하거나 더 좋았다. 하지만 scratch model은 성능이 안 좋았는데 이는 batch normalization을 사용하지 않아서 그렇다고 한다. Super Resolution에서도 비슷한 결과가 나왔다.&lt;/p&gt; &lt;h2 id=&quot;pipeline-b-structured-pruning-without-fine-tuning-through-conversion-to-inn&quot;&gt;Pipeline B. Structured pruning without fine-tuning through conversion to INN&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/table2.png&quot; width=&quot;60%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Section 4에서 partitioning을 finetuning할 수 있다고 했다. 따라서 DNN을 INN으로 변환할 때 partition tuning 유무에 따라 성능비교를 했을 때 partition tuning을 한 모델이 성능이 좋은 것을 알 수 있다.&lt;/p&gt; &lt;h2 id=&quot;pipeline-c-structured-pruning-without-fine-tuning-of-discrete-nns&quot;&gt;Pipeline C. Structured pruning without fine-tuning of discrete NNs&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/fig1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;기존의 pruning 방법과도 비교해봤다. 그 결과 INN을 통해서 pruning하는 것이 성능하락이 적었으며 몇 개의 데이터로만 partition tuning을 했을 때 성능하락이 가장 적은 것을 알 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;comment&quot;&gt;Comment&lt;/h1&gt; &lt;p&gt;합리적이고 흥미로운 논문인 것 같다. Training method를 보았을 때 DNN과 INN의 변환이 계속 일어나 학습시간이 느릴 수 있으나 기존 대형모델을 INN으로 만들어 finetuning한 후 크기별로 export하여 많은 device에 사용할 수 있을 것 같다. 하지만 비교적 가벼운 모델을 위주로 실험하고 ViT 계열의 실험은 안들어가있어 실제로 이를 원래 목적대로 사용할 수 있을지는 의문이다. 이론적 토대가 더 만들어진다면 임팩트가 있는 방법론이 되지 않을까 싶다.&lt;/p&gt; </description> <pubDate>Wed, 21 Jun 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/integral-neural-network/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/integral-neural-network/</guid> <category>backbone</category> <category>paper</category> <category>cvpr</category> <category>vit</category> <category>backbone</category> <category>paper</category> <category>vit</category> </item> <item> <title>Invariant Representation for Unsupervised Image Restoration</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;Image restortation은 대부분 pair한 dataset이 필요하다. 하지만 이를 구하는 것은 어려워서 CycleGAN을 이용하여 Image Restoration을 진행하는 경우가 있다. 하지만 CycleGAN과 같은 Image to Image translation과 DIRT와 같은 unsupervised domain adaptation은 다음과 같은 단점을 가지고 있다.&lt;/p&gt; &lt;h3 id=&quot;indistint-domain-boundary&quot;&gt;Indistint Domain Boundary&lt;/h3&gt; &lt;p&gt;Horse to zebra와 같이 image translation은 분명한 domain boundary가 존재한다. 하지만 image restoration은 noise level과 복잡한 backbond가 domain boundary를 희미하게 만들어서 이미지 퀄리티가 낮아진다.&lt;/p&gt; &lt;h3 id=&quot;weak-representation&quot;&gt;Weak Representation&lt;/h3&gt; &lt;p&gt;Unsupervised Domain Adaptation은 high-level representation만 추출한다. 이는 domain shift problem을 야기하여 low-quality reconstruction을 만들어낸다.&lt;/p&gt; &lt;h3 id=&quot;poor-generalization&quot;&gt;Poor Generalization&lt;/h3&gt; &lt;p&gt;One-to-one image translation은 semantic representation과 texture representation을 분리하여 잡아내기 힘들다.&lt;/p&gt; &lt;p&gt;따라서 이 논문에서 위의 문제를 해결하며 다음의 contribution을 남겼다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Image restoration분야에서 unsuperviesd representation learning method를 제안했다.&lt;/li&gt; &lt;li&gt;Dual domain constraint를 통해 semantic representation과 texture representation; 두 개의 representation을 분리했다.&lt;/li&gt; &lt;li&gt;Domain transfer를 통해 unsupervised image restoration을 제안했다.&lt;/li&gt; &lt;/ol&gt; &lt;h1 id=&quot;the-proposed-method&quot;&gt;The Proposed Method&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;먼저 문제에 대한 정의를 하겠다. \(\mathcal{X}\)를 Noisy Image Domain, \(\mathcal{Y}\)를 Clean Image Domain이라고 하겠다. Encoder는 각각의 도메인을 같은 vector space인 shared-latent space \(\mathcal{Z}\)로 projection 시킨다. 따라서 vector space에 대하여 다음의 식이 성립한다.&lt;/p&gt; \[z=E_\mathcal{X}(x)=E_\mathcal{Y}(y)\] &lt;p&gt;Generator는 shared-latent space \(\mathcal{Z}\)에서 image를 만들어낸다. 따라서 다음과 같은 식이 성립한다.&lt;/p&gt; \[x=G_\mathcal{X}(z),y=G_\mathcal{Y}(z)\] &lt;p&gt;이 때 각각의 도메인에대해 Encoder와 Generator는 \(\{E_\mathcal{X}, G_\mathcal{X}\}, \{E_\mathcal{Y}, G_\mathcal{Y}\}\) 각각 존재한다. 각각의 encoder가 shared-latent space \(\mathcal{Z}\)로 projection을 시킨다고 하더라도 각각의 latent vector는 다르다. 따라서 latent vector를 구분하여 적어주겠다.&lt;/p&gt; \[z_\mathcal{X}=E_\mathcal{X}(x), z_\mathcal{Y}=E_\mathcal{Y}(y)\] &lt;p&gt;따라서 우리가 하고싶어하는 Image restoration과정은 다음과 같다.&lt;/p&gt; \[F^{\mathcal{X}\rightarrow\mathcal{Y}}(x)=G_\mathcal{Y}(z_\mathcal{X})\] &lt;h2 id=&quot;discrete-representation-learning&quot;&gt;Discrete Representation Learning&lt;/h2&gt; &lt;p&gt;먼저 one-to-one image translation의 poor generalization 문제를 해결하기 위해 semantic representation과 texture(noise) representation을 분리시켜야한다. 따라서 저자는 다음 4가지의 방법론을 제시했다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Detangling Representation&lt;/li&gt; &lt;li&gt;Forward Cross Translation&lt;/li&gt; &lt;li&gt;Backward Cross Reconstruction&lt;/li&gt; &lt;li&gt;Adversarial Domain Adaptation&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;detangling-representation&quot;&gt;Detangling Representation&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled%201.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;먼저 extra noise encoder(\(E_\mathcal{X}^N\))을 도입을 한다. \(E_\mathcal{X}^N\)은 noise를 나타내는 texture latent vector를 뽑아내는 역할로 이를 도입해서 semantic representation과 texture representation을 분리했다. 이를 통해 \(z_\mathcal{X}$와\)z_\mathcal{Y}\(는 같은 distribution을 가지게 된다. 만약 noise image를 self-reconstruction하려면\)x=G_\mathcal{X}(z_\mathcal{X}, z_\mathcal{X}^N)$$$을 통하여 같이 reconstruction하면 된다.&lt;/p&gt; &lt;h3 id=&quot;forward-cross-translation&quot;&gt;Forward Cross Translation&lt;/h3&gt; &lt;p&gt;CycleGAN처럼 noise image에서 clean image 변환과 clean image에서 noise이미지의 변환이 되어야한다. 따라서 다음과 같은 방법으로 이미지 변환을 한다. 이 때 \(\mathcal{Y}\)에 \(\mathcal{X}\)의 noise를 추가하기 위해 \(z_\mathcal{X}^N\)를 이용한다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Noise image → clean image: \(\tilde{x}^{\mathcal{X}\rightarrow\mathcal{Y}} =G_\mathcal{Y}(z_\mathcal{X})\)&lt;/li&gt; &lt;li&gt;Clean image → noise image: \(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}} =G_\mathcal{X}(z_\mathcal{Y}\oplus z_\mathcal{X}^N)\)&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;backward-cross-translation&quot;&gt;Backward Cross Translation&lt;/h3&gt; &lt;p&gt;Forward cross translation을 했으니 backward cross translation을 할 수 있다. 이 때 \(\mathcal{X}\)에 \(\mathcal{Y}\)의 noise를 추가하기 위해 \(E_\mathcal{X}^N(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}})\)$를 이용한다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Noise image → clean image: \(\hat{x}=G_\mathcal{X}(E_\mathcal{Y}(\tilde{x}^{\mathcal{X}\rightarrow\mathcal{Y}})\oplus E_\mathcal{X}^N(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}}))\)&lt;/li&gt; &lt;li&gt;Clean image → noise image: \(\hat{y}=G_\mathcal{Y}(E_\mathcal{X}(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}}))\)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Backward cross translatio를 학습하기 위해 loss를 다음과 같이 구성한다.&lt;/p&gt; \[\mathcal{L}_\mathcal{X}^{CC}(G_\mathcal{X},G_\mathcal{Y},E_\mathcal{X},E_\mathcal{Y},E_\mathcal{X}^N)=\mathbb{E}_\mathcal{X}[||G_\mathcal{X}(E_\mathcal{Y}(\tilde{x}^{\mathcal{X}\rightarrow\mathcal{Y}})\oplus E_\mathcal{X}^N(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}}))-x||_1]\] \[\mathcal{L}_\mathcal{Y}^{CC}(G_\mathcal{X},G_\mathcal{Y},E_\mathcal{X},E_\mathcal{Y},E_\mathcal{X}^N)=\mathbb{E}_\mathcal{X}[||G_\mathcal{Y}(E_\mathcal{X}(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}}))-y||_1]\] &lt;h3 id=&quot;adversarial-domain-adaptation&quot;&gt;Adversarial Domain Adaptation&lt;/h3&gt; &lt;p&gt;Semantic representation (\(z_\mathcal{X}, z_\mathcal{Y}\))은 같은 vector space를 사용해야한다. 따라서 이를 강제하기 위해서 reprenentation discriminator \(D_r\)를 사용한다.&lt;/p&gt; \[\mathcal{L}^\mathcal{R}_{adv}(E_\mathcal{X},E_\mathcal{Y},D_\mathcal{R})=\mathbb{E}_\mathcal{X}[\frac{1}{2}logD_\mathcal{R}(z_\mathcal{X}+\frac{1}{2}(1-logD_\mathcal{R}(z_\mathcal{X})))] + \mathbb{E}_\mathcal{Y}[\frac{1}{2}logD_\mathcal{R}(z_\mathcal{Y}+\frac{1}{2}(1-logD_\mathcal{R}(z_\mathcal{Y})))]\] &lt;h2 id=&quot;self-supervised-constraint&quot;&gt;Self-Supervised Constraint&lt;/h2&gt; &lt;h3 id=&quot;background-consistency-loss&quot;&gt;Background Consistency Loss&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled%202.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;복잡한 background의 Indistint Domain Boundary을 해결하기 위해 backgound consistency loss를 제안한다. Noise가 있는 이미지와 깨끗한 이미지는 gaussian blur를 하면 structure 정보만 남기 때문에 background의 비교가 가능하다. 따라서 저자는 다음과 같은 loss를 추가한다.&lt;/p&gt; \[\mathcal{L}_{BC}=\sum_{\sigma=5,9,15} \lambda_\sigma||B_\sigma(\mathcal{X})-B_\sigma(\tilde{\mathcal{X}})||_1\] &lt;h3 id=&quot;semantic-consistency-loss&quot;&gt;Semantic Consistency Loss&lt;/h3&gt; &lt;p&gt;perception loss에서 영감을 받아 pretrained-backbone을 통한 semantic한 정보는 noise가 줄어들 것이라고 기대가 된다. 따라서 VGG19에서 conv5-1 layer와 같이 깊은 layer에서 feature map을 뽑아 비교하여 semantic representations의 consistency를 유지한다.&lt;/p&gt; \[\mathcal{L}_{SC}=||\phi_l(\mathcal{X}-\phi_l(\tilde{\mathcal{X}})||_2^2\] &lt;h2 id=&quot;joint-optimizing&quot;&gt;Joint Optimizing&lt;/h2&gt; &lt;p&gt;좋은 성능을 위하여 다른 여러가지도 추가했다.&lt;/p&gt; &lt;h3 id=&quot;target-domain-adversarial-loss&quot;&gt;Target Domain Adversarial Loss&lt;/h3&gt; &lt;p&gt;Noise Domain과 clean image domain에서 결과물을 더 잘만들기 위해 GAN loss를 추가한다.&lt;/p&gt; \[\mathcal{L}_{adv}^\mathcal{X}=\mathbb{E}_{x\sim P_\mathcal{X}(x)}[logD_\mathcal{X}(x)]+\mathbb{E}_{y\sim P_\mathcal{Y}(y), x \sim P_\mathcal{X}(x)}[log(1-D_\mathcal{X}(G_\mathcal{X}(E_\mathcal{Y}(y), E_\mathcal{X}^N(x))))]\] \[\mathcal{L}_{adv}^\mathcal{Y}=\mathbb{E}_{y\sim P_\mathcal{Y}(y)}[logD_\mathcal{Y}(y)]+\mathbb{E}_{x \sim P_\mathcal{Y}(y)}[log(1-D_\mathcal{Y}(G_\mathcal{Y}(E_\mathcal{X}(x)))]\] &lt;h3 id=&quot;self-reconstruction-loss&quot;&gt;Self Reconstruction Loss&lt;/h3&gt; &lt;p&gt;안정적인 학습 진행을 위해서 self reconstruction loss도 추가해였다.&lt;/p&gt; \[\hat{x}=G_\mathcal{X}(E_\mathcal{X}(x)\oplus E_\mathcal{X}^N(x)), \hat{y}=G_\mathcal{Y}(E_\mathcal{Y}(y))\] \[\mathcal{L}^\mathcal{X}_{rec}=||\hat{x} - x||_1, \mathcal{L}^\mathcal{Y}_{rec}=||\hat{y} - y||_1\] &lt;h3 id=&quot;kl-divergence-loss&quot;&gt;KL Divergence Loss&lt;/h3&gt; &lt;p&gt;Noise는 보통 normal distribution을 따른다. 따라서 이 논문에서도 latent-vector가 normal distribution을 따르도록 KL divergence loss를 추가했다.&lt;/p&gt; \[p(z_\mathcal{X}^N\sim N(0, 1))\] &lt;h2 id=&quot;total-loss&quot;&gt;Total Loss&lt;/h2&gt; &lt;p&gt;모든 loss를 합치면 다음과 같다.&lt;/p&gt; \[\underset{E_\mathcal{X},E_\mathcal{X}^N,E_\mathcal{Y},G_\mathcal{X},G_\mathcal{Y}}{\operatorname{min}} \underset{D_\mathcal{X},D_\mathcal{Y},D_\mathcal{R}}{\operatorname{max}} =\lambda_\mathcal{R}\mathcal{L}^\mathcal{R}_{adv}+\lambda_{adv}\mathcal{L}^{domain}_{adv}+\lambda_{CC}\mathcal{L}^{CC}+\lambda_{rec}\mathcal{L}^{Rec}+\lambda_{bc}\mathcal{L}^{BC}+\lambda_{sc}\mathcal{L}^{SC}+\lambda_{KL}\mathcal{L}^{KL}\] &lt;h2 id=&quot;restoration&quot;&gt;Restoration&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled%203.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;학습이 끝난 후에 noise가 있는 이미지를 보구언하려면 cross encoder-generator $${ E_\mathcal{X}, E_\mathcal{Y}}$ 를 사용하면 된다.&lt;/p&gt; \[\tilde{x}^{\mathcal{X}\rightarrow \mathcal{Y}}=G_\mathcal{Y}(E_\mathcal{X}(x))\] &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;p&gt;성능은 unsupervised방법들 중에선 성능이 좋다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled%204.png&quot; width=&quot;70%&quot; /&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled%205.png&quot; width=&quot;70%&quot; /&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled%206.png&quot; width=&quot;70%&quot; /&gt; &lt;/p&gt; </description> <pubDate>Sat, 29 Apr 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/unsupervised-image-restoration/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/unsupervised-image-restoration/</guid> <category>image-restoration</category> <category>unsupervised-learning</category> </item> <item> <title>DINE: Domain Adaptation from Single and Multiple Black-box Predictors</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;정제된 데이터가 많이 있지만 새로운 도메인에 접근하려면 labeling을 진행하기 힘든 경우가 많다. 따라서 이를 위한 Unsupervised Domain Adaption(UDA) 연구가 진행되고있는다. 하지만 UDA의 문제점이 존재한다.&lt;/p&gt; &lt;h3 id=&quot;개인정보&quot;&gt;개인정보&lt;/h3&gt; &lt;p&gt;의료분야와 같은 개인정보로 인해 Source data에 접근이 힘든 경우가 있어 UDA시 이를 활용할 수 없을 수 있다. 또한 GAN을 이용하는 방법에서는 source data가 재생산 가능하여 또 다시 개인정보 문제가 나타날 수 있다.&lt;/p&gt; &lt;h3 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h3&gt; &lt;p&gt;보통의 UDA는 같은 네트워크 구조에서 진행한다. 하지만 사용자의 환경에 따라 경량화 네트워크를 쓸 수도 있기 때문에 다른 네트워크 구조에서도 작동할 수 있어야한다.&lt;/p&gt; &lt;p&gt;저자는 기존의 UDA는 위의 두 가지 문제점을 가지고 있다고 판단하여 source model이 완전히 black-box인 상황에서도 작동하는 DINE을 제시했다.&lt;/p&gt; &lt;h1 id=&quot;methodology&quot;&gt;Methodology&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/dine/Untitled.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Source Domain: \(n_s\) 개의 data, \(x_s^i \in \mathcal{X}_s, y_s^i \in \mathcal{Y}_s\) 에 대하여 $${x^i_s, y^i_s}^{n_s}_{i=1}$&lt;/li&gt; &lt;li&gt;Target Domain: \(n_t\) 개의 data, \(x_t^i \in \mathcal{X}_t, y_t^i \in \mathcal{Y}_t\) 에 대하여 $${x^i_t, y^i_t}^{n_t}_{i=1}$저저자&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;저자는 \(\mathcal{Y}_s=\mathcal{Y}_t\) 또는 \(\mathcal{Y}_s \supset \mathcal{Y}_t\) 인 경우를 다루고 있다.&lt;/p&gt; &lt;h2 id=&quot;source-domain-model&quot;&gt;Source Domain Model&lt;/h2&gt; &lt;p&gt;Source domain model 은 backbone network에 fc layer를 결합하여 사용한다. Source domain model을 학습시킬때는 일반적인 학습방법으로 label smoothing을 적용하여 학습을 시킨다.&lt;/p&gt; \[\mathcal{L}_s(f_s;\mathcal{X_s},\mathcal{Y}_s)=-\mathbb{E}_{(x_s, y_s)\in \mathcal{X}_s \times \mathcal{Y}_s}(q^s)^Tlogf_s(x_s)\] &lt;h2 id=&quot;target-domain-model&quot;&gt;Target Domain Model&lt;/h2&gt; &lt;p&gt;Target domain model을 학습시키이 위해서는 source model의 noise를 보정해줘야 한다. 따라서 저자는 두 가지 방법을 사용한다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adaptive self-knowledge distillation&lt;/li&gt; &lt;li&gt;Distillation with structural regularizations&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;adaptive-self-knowledge-distillation&quot;&gt;Adaptive self-knowledge distillation&lt;/h3&gt; &lt;p&gt;Target model 또한 backbone network에 fc later를 결합하여 사용한다. 하지만 source model과 backbone 구조는 다를 수 있다.&lt;/p&gt; &lt;p&gt;기존의 kowledge distillation은 다음과 같다.&lt;/p&gt; \[\mathcal{L}_{kd}(f_t;\mathcal{X}_t,f_s)=\mathbb{E}_{x_t\in \mathcal{X}_t} \mathcal{D}_{kl}(f_s(x_t)||f_t(x_t))\] &lt;p&gt;source domain model의 예측값과 target domain model의 kl divergence가 최소가 되도록 학습을 진행한다. 하지만 저자는 source domain과 target domain의 차이로인해 noise가 발생할 것이라고 했고 이를 보정하기 위해 Adaptive Label Smoothing(AdaLS)을 제안한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/dine/Untitled%201.png&quot; width=&quot;60%&quot; /&gt; &lt;/p&gt; &lt;p&gt;즉, top r개의 probability만 가져오고 나머지는 smoothing한 결과를 가져와 K-r개의 예측값은 nosize라 판단하여 무시하는 것이다. 저자는 이를 통해 self-weighted pseudo labeling효과를 낼 수 있다고한다. 만약 Source model이 여러개이면 AdaLS의 평균값을 사용한다.&lt;/p&gt; \[P^T \leftarrow \frac{1}{M}\sum^{M}_{m=1}AdaLS(f_s^{(m)}(x_t))\] &lt;p&gt;논문에서는 추가로 noise 감소를 위해 self-distillation strategy를 사용했다. EMA를 통해서 target domain model의 self-distillation을 하는 것이다.&lt;/p&gt; \[P^T(x_t) \leftarrow \gamma P^T(x_t)+(1-\gamma)f_t(x_t), \forall x_T \in \mathcal{X}_t\] &lt;h2 id=&quot;distillation-with-structural-regularizations&quot;&gt;Distillation with Structural Regularizations&lt;/h2&gt; &lt;p&gt;저자는 AdaLS로는 noise regularization이 부족하다 생각했고 structural regularization을 사용했다.&lt;/p&gt; &lt;h3 id=&quot;mixup&quot;&gt;Mixup&lt;/h3&gt; &lt;p&gt;Target domain network의 예측값으로 mixup loss를 만들어 target domain structural information을 사용했다. 이미지의 linear combination의 예측과과 각각의 예측의 linear combination을 이용하여 target domain에 대한 구조적 정보를 만든 것이다.&lt;/p&gt; \[\mathcal{L}_{mix}(f_t,\mathcal{X}_t)=\mathbb{E}_{x_i^t,x_j^t \in \mathcal{X}_t} \mathbb{E}_{\lambda\in Beta(\alpha,\alpha)}\] \[l_{ce}({Mix}_{\lambda}(f_t^\prime(x_i^t), f_t^\prime(x_j^t)), f_t({Mix}_{\lambda}(x_i^t, x_j^t))\] &lt;h3 id=&quot;mutual-information&quot;&gt;Mutual Information&lt;/h3&gt; &lt;p&gt;Image set을 이용하여 모델의 예측에 대한 엔트로피를 계상하여 global한 구조적 정보를 학습한다. 아래의 수식은 모든 예측값에 대한 엔트로피는 높혀 불명확한 것은 불명확하게, 이미지에 대한 예측값의 엔트로피는 낮춰 명확한 예측값은 명확하게 만든다.&lt;/p&gt; \[\mathcal{L}(f_t; \mathcal{X}_t)=H(\mathcal{Y}_t)-H(\mathcal{Y}_t|\mathcal{X}_t)=h(\mathbb{E}_{x_t\in \mathcal{X}_t}f_t(x_t))-\mathbb{E}_{x_t\in \mathcal{X}_t}h(f_t(x_t))\] &lt;h2 id=&quot;loss&quot;&gt;Loss&lt;/h2&gt; &lt;p&gt;전체적인 Loss는 다음과 같다.&lt;/p&gt; \[\mathcal{L}_t=\mathcal{D}_{kl}(P^T(x_t)||f_t(x_t))+\beta\mathcal{L}_{mix}-\mathcal{L}_{im}\] &lt;h2 id=&quot;finetune&quot;&gt;FineTune&lt;/h2&gt; &lt;p&gt;DIRT-T의 영감을 받아서 secondary training을 진행한다. 이때 loss는 mutual information을 사용한다.&lt;/p&gt; \[\mathcal{L}(f_t; \mathcal{X}_t)=H(\mathcal{Y}_t)-H(\mathcal{Y}_t|\mathcal{X}_t)=h(\mathbb{E}_{x_t\in \mathcal{X}_t}f_t(x_t))-\mathbb{E}_{x_t\in \mathcal{X}_t}h(f_t(x_t))\] &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;p&gt;실험결과로는 다른 Source data를 접근할 수 없는 방법들 중에서는 SOTA를 찍었고, source data를 접근할 수 있는 모델 만큼 성능이 높아졌다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/dine/Untitled%202.png&quot; width=&quot;100%&quot; /&gt; &lt;img src=&quot;/assets/post/image/dine/Untitled%203.png&quot; width=&quot;100%&quot; /&gt; &lt;img src=&quot;/assets/post/image/dine/Untitled%204.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;h1 id=&quot;analysis&quot;&gt;Analysis&lt;/h1&gt; &lt;p&gt;mutual information loss가 학습에서의 영향이 컸고, AdaLS는 top r개를 뽑는 것이 좋았다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/dine/Untitled%205.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; </description> <pubDate>Sun, 16 Apr 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/dine/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/dine/</guid> <category>domain-adaptation</category> </item> <item> <title>MobileOne: An Improved One millisecond Mobile Backbone</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;MobleNet, ShuufleNet등 경량화 네트워크들은 parameter의 수와 FLOPS를 기준으로 모델을 경량화하고 있다. MobileNetV3, MNasNet 등 만이 실제 기기에서 latency를 측정하여 반영하고 있다. 저자는 paramter수와 FLOPS가 줄어들수록 latency줄어드는 의 관계가 항상 일치하지 않는다는 것을 발견하여 실제로 모델의 어떤 부분이 높은 복잡로를 갖는지 평가하여 모델을 제작했다.&lt;/p&gt; &lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt; &lt;h2 id=&quot;metric-correlation&quot;&gt;Metric Correlation&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled.png&quot; width=&quot;50%&quot; /&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%201.png&quot; width=&quot;50%&quot; /&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%202.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;모델의 cost를 비교할 때 parameter의 수와 FLOPS를 기준으로 비교하곤한다. 하지만 실제 모바일환경에서 일치가 안될 경우고 있으므로 iPhone 12를 사용하여 상관관계를 측정했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%203.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;그 결과 모바일 환경에서는 FLOPS가 중간정도의 상관관계를 나타냈고 파라미터의 수는 낮은 상관관계를 나타냈다. 반면에 CPU 환경에서는 관계가 없보인다.&lt;/p&gt; &lt;h2 id=&quot;key-bottlenecks&quot;&gt;Key Bottlenecks&lt;/h2&gt; &lt;h3 id=&quot;activation-function&quot;&gt;Activation Function&lt;/h3&gt; &lt;p&gt;Activation function마다 cost가 다르다. 따라서 이를 비교하기 위해 30 layer의 모델을 선언하였으며 activation layer만 바꾸면서 속도측정을 했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%204.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Dynamic Shift-Max등 여러 강력한 activation function이 있으나 이는 latency가 커서 RELU를 사용하기로 했다.&lt;/p&gt; &lt;h3 id=&quot;architectural-blocks&quot;&gt;Architectural Blocks&lt;/h3&gt; &lt;p&gt;Runtime performace에 영향을 주는 원인은 크게 memory access cost과 degree of parallelism이 있다. 예를 들면 skip connection은 이전 feature map에 대한 정보를 저장하고 읽어와야하기 때문에 memory access cost가 늘어나고 SE-block에서 사용하는 global pooling operation은 동기화를 해야하기 때문에 degree of parallelism이 줄어든다. 따라서 MobileOne에서는 skip connectiond르 제거하고 SE-block의 수는 적당히 조절하였다.&lt;/p&gt; &lt;h2 id=&quot;mobileone-architecture&quot;&gt;MobileOne Architecture&lt;/h2&gt; &lt;h3 id=&quot;mobileone-block&quot;&gt;MobileOne Block&lt;/h3&gt; &lt;p&gt;기본적인 block은 depthwise, pointwise layer로 factorization하였다. Basic block은 MobileNet-V1에서 사용하는 3x3 depthwise convolution과 1x1 pointwise convolution을 사용한다. 그리고 Rep-VGG에서 사용한 re-parameterizable skip connection을 사용한다. 이 때 trivial over-parameterization factor k는 1~5의 값을 사용한다. Memory access cost를 줄이기 위해 skip connection은 inference time에 제거했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%205.png&quot; width=&quot;50%&quot; /&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%206.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Convolution에 대해서는 BachNorm을 Conv layer에 folding하였다. Kernel size \(K\), Input channel dimension \(C_{in}\), output channel dimension \(C_{out}\)에 대해서 weight matix는 \(W^\prime \in \mathbb{R}^{C_{out} \times C_{in} \times K \times K}\), bias는 \(b^\prime \in \mathbb{R}^D\)로 표시할 수 있다. 또한 BatchNorm은 accumulated mean \(\mu\), accumulated standard deviation \(\sigma\), scale \(\gamma\), bias \(\beta\)로 구성되어 있다. Conv와 BN은 모두 linear operation이므로 이를 다느과 같이 합칠 수 있다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weight&lt;/li&gt; &lt;/ul&gt; \[\hat{W}=W^\prime * \frac{\gamma}{\sigma}\] &lt;ul&gt; &lt;li&gt;Bias&lt;/li&gt; &lt;/ul&gt; \[\hat{b}=(b^\prime - \mu) * \frac{\gamma}{\sigma}+\beta\] &lt;p&gt;BN에 대한 skip connection은 1x1 convolutio에 K-1 zero padding으로 folding 할 수 있다. 위 과정을 통해 얻어진 folding은 inferenec time에 사 된다.&lt;/p&gt; &lt;h3 id=&quot;model-scaling&quot;&gt;Model Scaling&lt;/h3&gt; &lt;p&gt;모델의 속도를 위해 block의 수는 resolution마다 다르게설정했다. 이는 resolution이 높은 상단 layer에서는 cost가 높기 때문에 block의 수를 줄이고 하단 layer는 channel 수가 많기 때문에 block를 줄이게 된 것이다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%207.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt; &lt;p&gt;Overfitting을 방지하기 위해 small model에 대해서는 regularization을 적게 해야한다. 또한 cosine learning rate를 decay와 함께 regularization도 deacy를 해준다.&lt;/p&gt; &lt;h1 id=&quot;result&quot;&gt;Result&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%208.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;파라이머의 수가 비슷한 모델중에서는 성능이 제일 높게 나타난다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%209.png&quot; width=&quot;50%&quot; /&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%2010.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Reparameterization도 성능에 좋은 영향을 내는데 모델의 크기가 클수록 성능차이가 좁아진다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%2011.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;다른 모바일 네트워크를 CPU, GPU, Mobile에서 돌렸을 때 성능과 속도가 좋았고&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%2012.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;knowledge distillation을 했을 때도 성능이 제일 좋았다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%2013.png&quot; width=&quot;100%&quot; /&gt; &lt;img src=&quot;/assets/post/image/mobileone/Untitled%2014.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;다른 Task에서도 성능이 좋았다.&lt;/p&gt; </description> <pubDate>Sun, 16 Apr 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/mobileone/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/mobileone/</guid> <category>backbone</category> <category>on-device-ai</category> </item> <item> <title>Proper Reuse of Image Classification Features Improves Object Detection</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;기존 object detection을 학습시킬 때 imagenet backbone을 사용하여 pretrain후 transfer learning을 통해 target dataset에 대해 학습을 했다. 이 상황에서 보통 object detection의 backbone만 pretrain시키고 detection관련 module은 random으로 initalize시킨다. 따라서 이에 관한 연구가 있었는데 크게 두 가지가 있다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pretrain으로 사용한 classiciation data의 량 만큼 object detection에 긍정적 영향을 끼친다.&lt;/li&gt; &lt;li&gt;In-domain dataset으로 학습시간을 길게 가져가면 pretrained model과 scratch model의 성능차이가 없어진다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;저자들은 두 연구를 분석하기위해 backbone network를 classification dataset에 대해 학습 후 paramter를 freeze시켜 object detection model을 학습시켰다.&lt;/p&gt; &lt;h1 id=&quot;methodology&quot;&gt;Methodology&lt;/h1&gt; &lt;p&gt;기존에 사용하는 일반적인 학습방법은 다음과 같다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;ImageNet, JFT-300M과 같은 classification dataset에서 backbone network를 학습시킨다.&lt;/li&gt; &lt;li&gt;Backbone network에 detection specific component를 추가한다. ex) RPN, FPN, NAS-FPN&lt;/li&gt; &lt;li&gt;Target dataset에 대하여 재학습시킨다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;하지만 저자는 다음과 같은 방법을 제안한다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;ImageNet, JFT-300M과 같은 classification dataset에서 backbone network를 학습시킨다.&lt;/li&gt; &lt;li&gt;Backbone network를 freeze시킨 후 detection specific component를 추가한다.&lt;/li&gt; &lt;li&gt;Target dataset에 대하여 재학습시킨다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;결론적으로 backbone을 freeze시키는 것만 달라졌다.&lt;/p&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;p&gt;실험결과를 요약하자면 다음과 같다.&lt;/p&gt; &lt;h2 id=&quot;detectoin-specific-capacity&quot;&gt;Detectoin-Specific capacity&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;FPN, RPN, Detection Cascade 등은 network의 generalzation에 도움을 준다.&lt;/li&gt; &lt;li&gt;해당 요소들의 capacity가 충분하면 backbone freezing이 fine-tuning과 training scratch보다 좋은 성능을 보인다.&lt;/li&gt; &lt;li&gt;Backbone pretraining시 classification dataset의 수가 많아질수록 성능이 높아진다.&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Data Augmentation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instance segmentation 실험에서는 Data augmentation은 다음을 사용했다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Large Scale Jittering&lt;/li&gt; &lt;li&gt;Copy-and-paste augmentation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;첫번째 실험의 model architecture는 다음과 같다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Faster-RCNN&lt;/li&gt; &lt;li&gt;ResNet50&lt;/li&gt; &lt;li&gt;FPN or NAS-FPN&lt;/li&gt; &lt;li&gt;Cascade head&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;두 번째 실험의 model architecutre는 다음과 같다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mask-RCNN&lt;/li&gt; &lt;li&gt;EfficientNet-B7&lt;/li&gt; &lt;li&gt;NAS-FPN&lt;/li&gt; &lt;li&gt;Cascade head&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hyper parameter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;해당실험은 자원이 한정되어있는 상황을 가정했기 때문에 batch size는 64, learning rate는 그에 맞게 0.08로 설정했다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Detection dataset으로 MS-COCO, LVIS를 사용했고, classification dataset으로는 ImageNet, JFT-300M을 사용했다.&lt;/p&gt; &lt;h2 id=&quot;resnet50--faster-rcnn&quot;&gt;ResNet50 + Faster-RCNN&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/proper-reuse-of-image-classification-features-improve-object-detection/Untitled.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;모든 실험서 공통적으로 classification pretrain시 이미지수가 비교적으로 적은 ImageNet보다 JFT-300M의 성능이 좋았다. 또한 FPN을 사용했을 때보다 parameter수가 더 많은 NAS-FPN, Cascade head를 사용했을 때 성능이 좋았다. 이는 backbone network가 다른 domain에서 학습이 되어 이를 generalize하는데 충분한 capacity가 필요하다고 한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/proper-reuse-of-image-classification-features-improve-object-detection/Untitled%201.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;pretraing과 fine-tuning의 관계를 알아보기위해 실험을 진행했다. 첫 번째를 보았을 때 traning schedule이 짧을 경우엔 pretranig에 사용된 classfication dataset의 크기가 커질수록 성능이 좋았따. 두 번째를 보았을 때 training schedule이 길수록 pretran classification dataset에따른 성능차이가 줄어든 것을 보아 pretran은 성능에 도움이 되지 않는다고 한다. 저자는 이를 확장하여 large dataset으로 pretrain하고 backbone을 freeze시켜 long traning schedule으로부터 knowledge를 보호한다. 또한 high-capacity detector component사용하여 domain이 다른 문제를 해결했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/proper-reuse-of-image-classification-features-improve-object-detection/Untitled%202.png&quot; width=&quot;50%&quot; /&gt; &lt;img src=&quot;/assets/post/image/proper-reuse-of-image-classification-features-improve-object-detection/Untitled%203.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Backbone은 ImageNet에서 학습시켰다. 따라서 object detection과 domain gap이 발생하는데 이는 detector component가 해결할 수 있다. FPN을 사용할 경우 generalize하는데 capacity가 부족해 성능감소가 발생했다. 하지만 NAS-FPN과 Cascade head를 추가했을 때 성능에 이득이 있었다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/proper-reuse-of-image-classification-features-improve-object-detection/Untitled%204.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;결론적으로 충분한 capacity의 detector component를 사용하고 backbone network를 freeze 시키면 trainable parameter가 줄어들고 FLOPS도 줄어들어 학습속도와 memory가 줄어든다.&lt;/p&gt; &lt;h2 id=&quot;efficientnet-b7mask-rcnn&quot;&gt;EfficientNet-B7+Mask-RCNN&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/proper-reuse-of-image-classification-features-improve-object-detection/Untitled%205.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;capacity가 충분한 detector component를 추가하고 backbone을 freeze시키는 것이 좋은 성능을 냈으며 Copy-Paste라는 강한 augmentation을 사용했을 떄도 성능향상이 있었다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/proper-reuse-of-image-classification-features-improve-object-detection/Untitled%206.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;small object, midium object 모두 성능에서 향상이 있었다.&lt;/p&gt; &lt;h2 id=&quot;furthurmore&quot;&gt;Furthurmore&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/proper-reuse-of-image-classification-features-improve-object-detection/Untitled%207.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Backbone Freeze가 만병통치약은 아니었다. Freeze시킨 경에서도 residual adapter를 사용하면 더 좋은 성능을 냈다.&lt;/p&gt; </description> <pubDate>Sat, 08 Apr 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/proper-use-classification-features-in-object-detection/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/proper-use-classification-features-in-object-detection/</guid> <category>transfer-learning</category> <category>object-detection</category> </item> <item> <title>Meta Pseudo Labels</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mpl-psudo-label.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Semi-supervised learning 방법은 여러가지 있는데 그 중에 한 가지는 psuedo labeling 방법이다. Psudo labeling은 잘 학습된 teacher network와 student network가 존재하는데 teacher model은 unlabeled data의 psuedo label을 제작하고 student는 그 label을 학습하는 것으로 진행된다.&lt;/p&gt; &lt;p&gt;하지만 이 방법의 문제점은 teacher model의 psuedo label이 정확하지 않다면 student model은 teacher model의 확증편향으로 인하여 잘못된 방향으로 학습을 진행한다. 따라서 저자는 이러한 문제를 해결하기 위해 teacher model이 labeled data에 대한 student model의 성능을 확인하면서 bias를 수정하는 과정을 제안한다.&lt;/p&gt; &lt;h1 id=&quot;meta-psuedo-label&quot;&gt;Meta Psuedo Label&lt;/h1&gt; &lt;p&gt;들어가기 앞서서 notanio에 대하여 설명하겠다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;\(T(x_u,\theta_T)\): Soft prediction of &lt;strong&gt;teacher&lt;/strong&gt; model on unlabeld data&lt;/li&gt; &lt;li&gt;\(S(x_u,\theta_S)\): Soft prediction of &lt;strong&gt;student&lt;/strong&gt; model on &lt;strong&gt;unlabeld&lt;/strong&gt; data&lt;/li&gt; &lt;li&gt;\(S(x_l,\theta_S)\): Soft prediction of student model on &lt;strong&gt;labeled&lt;/strong&gt; data&lt;/li&gt; &lt;li&gt;\(CE(q,p)\): cross-entropy&lt;/li&gt; &lt;/ul&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mpl.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Meta psuedo label은 다음과 같은 과정을 거친다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Teacher model이 unlabeld data의 psuedo label을 생성한다.&lt;/li&gt; &lt;li&gt;Student model이 이를 학습한다.&lt;/li&gt; &lt;li&gt;Student이 labeled data를 이용하여 성능을 평가한다.&lt;/li&gt; &lt;li&gt;Teacher model은 3번을 이용하여 더 좋은 psuedo label을 만들도록 학습한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;더 이해하기쉽게 식으로 살펴보자.&lt;/p&gt; &lt;p&gt;Student의 optimal parameter는 teacher model이 제작한 psuedo-label과 student model의 예측값의 cross entropy가 낮은 parameter이다.&lt;/p&gt; \[\theta_S^{PL}=\underset{\theta_S}{\operatorname{argmin}} \mathbb{E}[CE(T(x_u;\theta_T), S(x_u;\theta_S)]\] &lt;p&gt;un-labeled dataloss는 다음과 같이 정의할 수 있다.&lt;/p&gt; \[\mathbb{E}[CE(T(x_u;\theta_T), S(x_u;\theta_S)] := \mathcal{L}_u(\theta_T, \theta_S)\] &lt;p&gt;그리고 teacher 모델이 사용하는 labeled data loss는 다음과 같이 정의할 수 있다.&lt;/p&gt; \[\mathbb{E}_{x_l,y_l}[CE(y_l,S(x_l;\theta_S^{PL}))] := \mathcal{L}_l(\theta_S^{PL})\] &lt;p&gt;이 때 student model은 teacher model의 psuedo-label을 이용하여 학습하기 때문에 성능은 teacher model에 의존적인 것을 알 수 있다. 이러한 이유로 meta psudo label이라거 명명하였고 이를 나타내기 위해서 Notation을 다음과 같이 작성한다.&lt;/p&gt; \[\theta_S^{PL} \rightarrow \theta_S^{PL}(\theta_T)\] &lt;p&gt;또한 label data loss는 다음과 같이 나타낼 수 있다.&lt;/p&gt; \[\mathcal{L}_l(\theta_S^{PL}) \rightarrow \mathcal{L}_l(\theta_S^{PL}(\theta_T))\] &lt;p&gt;techer model은 확증편향을 수정하기 위해 \(\mathcal{L}_l(\theta_S^{PL}(\theta_T))\)을 이용하여 parameter를 update한다. 하지만 이를 직접 계산하는 것은 힘들기 때문에 approximation을 한다.&lt;/p&gt; \[\theta_S^{PL}(\theta_T) \approx \theta_S - \eta_S \cdot \triangledown_{\theta_S}\mathcal{L}_u(\theta_{T}, \theta_{S})\] &lt;p&gt;따라서 teacher model은 다음을 최소화하는 것을 목표로한다.&lt;/p&gt; \[\underset{\theta_T}{\operatorname{min}} \mathcal{L}_l (\theta_S - \eta_S \cdot \triangledown_{\theta_S}\mathcal{L}_u(\theta_{T}, \theta_{S}))\] &lt;h2 id=&quot;정리&quot;&gt;정리&lt;/h2&gt; &lt;p&gt;Student model은 SGD로 psuedo-label에 대하여 optimize한다.&lt;/p&gt; \[\theta^{\prime}_S = \theta_S - \eta_S \triangledown_{\theta_S} \mathcal{L}(\theta_T, \theta_S)\] &lt;p&gt;Teacher model은 student의 gradient update를 재사용하여 SGD로 optimize한다.&lt;/p&gt; \[\theta^{\prime}_T=\theta_T - \eta_T \triangledown_{\theta_T} \mathcal{L}_l (\theta_S - \eta_S \cdot \triangledown_{\theta_S}\mathcal{L}_u(\theta_{T}, \theta_{S}))\] &lt;h2 id=&quot;auxiliary-loss&quot;&gt;Auxiliary Loss&lt;/h2&gt; &lt;p&gt;이외에도 loss를 추가하면 성능향상에 기여한다. 따라서 teacher model은 두 가지 loss를 추가했다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Supervised: train on label&lt;/li&gt; &lt;li&gt;Semi-supervised: UDA objective on unlabeld data&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Student는 오직 Meta Psuedo label로만 학습을 진행하였다. 이후에는 task에 맞도록 finetunin을 하였다.&lt;/p&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;h2 id=&quot;two-moon-dataset&quot;&gt;Two moon dataset&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mpl-two-moon.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;각각의 class마다 unlabled data 1000개씩, label data 3개씩 추출하였다. Supervised 방법은 label data를 잘 분류하지만 다른 데이터는 오류를 보이고 있다. Supervised 방법으로 학습 된 teacher model을 통하여 pseudo label을 만들었을때는 label data 조차 정확하게 판별을 못했다. 하지만 Meta Psuedo Label에서는 정확하게 두 class를 분리하였다.&lt;/p&gt; &lt;h2 id=&quot;small-model&quot;&gt;Small Model&lt;/h2&gt; &lt;p&gt;EfficientNet과 같은 large model을 실험하기 전에 small model로 실험을 진항하였다. 세 가지 dataset을 사용하였는데 CIFAR-10-4K, SVHN은 WideResNet28-2를 사용하였고 imagenet은 resnet50을 사용하였다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/small-model.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;다른 SOTA method들 보다 더 좋은 성능을 보였다.&lt;/p&gt; &lt;h2 id=&quot;semi-supervised-learning&quot;&gt;Semi-supervised learning&lt;/h2&gt; &lt;h3 id=&quot;다른-기법들&quot;&gt;다른 기법들&lt;/h3&gt; &lt;p&gt;ImageNet supervised learning에서 사용했던 기법들과 비교를 해보았을 때 성능이 좋았다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mpl-supervised.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;imagenet&quot;&gt;ImageNet&lt;/h2&gt; &lt;p&gt;ImageNet을 labeled data, JFT를 unlabeled data로 사용하여 semi-supervised learning을 한 후 imagenet으로 finetuning한 결과 SOTA를 찍었고 supervised learning보다 성능이 좋았다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mlp-imagenet.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; </description> <pubDate>Sat, 01 Apr 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/meta-pseudo-label/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/meta-pseudo-label/</guid> <category>semi-supervised-learning</category> <category>paper</category> </item> <item> <title>MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;Transformer는 NLP에서 좋은 성능을 보였고 vision task에서도 ViT를 통하여 좋은 성능을 보여줬다. 이는 global representation을 학습할 수 있기 때문인데 이와같은 성질은 light weight제작시 단점이 된다. 기존의 CNN에서 light weight 제작은 인접픽셀끼리 높은 상관관계를 가지고 있다라는 inductive bias의 덕을 보았기 때문이다. 따라서 저자는 CNN과 ViT의 장점을 합쳐놓은 MobileViT를 제안하였고, light weight, general-purpose, low latency를 달성하였다.&lt;/p&gt; &lt;h1 id=&quot;mobilevit&quot;&gt;MobileViT&lt;/h1&gt; &lt;h2 id=&quot;vit&quot;&gt;ViT&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mobilevit-vit.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Input \(X \in \mathbb{R}^{H \times W \times C}\)를 flatten patch \(X_f \in \mathbb{R}^{N \times PC}\)로 만든다.&lt;/li&gt; &lt;li&gt;Fixed &lt;em&gt;d&lt;/em&gt;-dimensional space \(X_p \in \mathbb{R}^{N \times d}\)에 projection 시킨다.&lt;/li&gt; &lt;li&gt;&lt;em&gt;L&lt;/em&gt;개의 transformer block을 이용하여 inner-patch representation을 학습한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;ViT의 computational cost는 \(O(N^2d)\)이고, \(P=wh\)이다.&lt;/p&gt; &lt;p&gt;ViT는 spatial inductive bias를 무시하기 때문에 더 많은 파라미터를 요구한다.&lt;/p&gt; &lt;h2 id=&quot;mobilevit-architecture&quot;&gt;MobileVit Architecture&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mobilevit.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;MobileViT block은 위와 같다. MobileViT는 local global featyure를 적은 파라미터를 학습하기 위해 구상되었다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;(Local feature) Input tensor \(X \in \mathbb{R}^{H \times W \times C}\)을 standard \(n \times n\) convolution layer와 point wise convolution을 이용하여 \(X_L \in \mathbb{R}^{X \times W \times d}\) 를 만든다.&lt;/li&gt; &lt;li&gt;\(X_L\)을 non-overlapping patch인 \(X_U \in \mathbb{R}^{P \times N \times d}\)로 unfold한다. 이 때 \(P=wh, N=\frac{HW}{P}\)이고, \(h \leq n, w \leq n\)이다.&lt;/li&gt; &lt;li&gt;(Global feature) 패치 \(p \in \{1, ... ,P\}\)에 대하여 inter-patch relationship을 학습하기 위해 transformer를 사용한 후 \(X_G \in \mathbb{R}^{P \times N \times d}\)를 얻는다.&lt;/li&gt; &lt;/ol&gt; \[X_G(p)=Transformer(X_U(p)), 1 \leq p \leq P\] &lt;ol&gt; &lt;li&gt;이후 point-wise convolution으로 차원을 \(C\)로 만들고 \(X\)와 concatenation연산을 한다.&lt;/li&gt; &lt;li&gt;또 다른 \(n \times n\) convolution을 통하여 concatenation한 결과를 fusion한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;위의 과정을 통해 local information을 \(X_U(p)\)에 encode하고, global information을 \(X_G(p)\)에 encode한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mobilevit-cnn-patch-relationship.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;위의 그림에서 볼 수 있 듯 convolution을 통해 local feature를 encode 한 후 transformer연산을 통해 inter-patch relationship을 encode하여 결과적으로 한 pixel이 다른 모든 pixel을 고려할 수 있게되었다.&lt;/p&gt; &lt;h3 id=&quot;relationship-to-convolution&quot;&gt;Relationship to convolution&lt;/h3&gt; &lt;p&gt;Standard convolution은 다음 3가지 연산의 스택으로 볼 수 있다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Unfolding&lt;/li&gt; &lt;li&gt;Matrix multiplication&lt;/li&gt; &lt;li&gt;Folding&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;이 때 MobileViT block은 matrix multiplication(local processing )에서 transformer(global processing)로 변경되었으므로 &lt;em&gt;transformer as convolution&lt;/em&gt;으로 볼 수 있다.&lt;/p&gt; &lt;h3 id=&quot;light-weight&quot;&gt;Light-weight.&lt;/h3&gt; &lt;p&gt;다른 ViT계열 모델들은 transformer만 사용하여 inter-patch relationship을 계산하여 image-specific inductive bias의 정보를 잃게되었다. 하지만 MobileViT block은 convolution-like한 특성을 가지고 있어 다른 모델보다 경량화가 가능한 것이다.&lt;/p&gt; &lt;h3 id=&quot;computational-cost&quot;&gt;Computational cost&lt;/h3&gt; &lt;p&gt;MobileViT \(O(N^2Pd)\), ViT는 \(O(N^2d)\)이다. MobileViT는 ViT보다 비효율적이지만 실제로는 DeIT보다 2배 더 적은 FLOPs와 1.8%의 성능향상이 되었다.&lt;/p&gt; &lt;h3 id=&quot;mobilevit-architecture-1&quot;&gt;MobileViT architecture&lt;/h3&gt; &lt;p&gt;light-weight CNN을 고려하여 S, XS, XXS 모델을 만들었고, 처음 layer는 3x3 standard convolution layer를 사용하고 다음은 MobileNetv2(MV2) block과 MobileViT block을 사용한다. MobileViT block에서는 3x3 CNN을 사용하였고 \(h=w=2\)를 사용하였다. MV2는 down-sampling의 역할을 수행한다.&lt;/p&gt; &lt;h2 id=&quot;multi-scale-sampler-for-training-efficiency&quot;&gt;MULTI-SCALE SAMPLER FOR TRAINING EFFICIENCY&lt;/h2&gt; &lt;p&gt;일반적인 ViT모델들은 여러 스케일의 모델들을 만든 후 fine-tuning할수밖에 없다. 하지만 MobileViT는 multi-scale traning이 가능하고 이 때 GPU성능을 끌어올리기 위해 batch-size를 resoution마다 유동적이게 관리했다. Resolution set \(S={(H_1, W_1),...,(H_n, W_n)}\)에 대하여 최대 resolution이 \((H_t, W_t) \in S\)일 때, t번째 resolution \((H_t, W_t) \in S\)의 batch size는 \(b_t=\frac{H_nW_nb}{H_tW_t}\)이다.&lt;/p&gt; &lt;h1 id=&quot;experimental-results&quot;&gt;EXPERIMENTAL RESULTS&lt;/h1&gt; &lt;h2 id=&quot;image-classification-on-the-imagenet-1k-dataset&quot;&gt;IMAGE CLASSIFICATION ON THE IMAGENET-1K DATASET&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Nvidia GPU 8개&lt;/li&gt; &lt;li&gt;epoch: 300&lt;/li&gt; &lt;li&gt;batch size: 1024&lt;/li&gt; &lt;li&gt;AdamW optimizer&lt;/li&gt; &lt;li&gt;label smoothing cross-entropy (0.1)&lt;/li&gt; &lt;li&gt;multi-scale sampler (\(S=\{(160,160),(192,192),(256,256),(288,288),(320,320)\}\))&lt;/li&gt; &lt;li&gt;learning rate scheduler: warmup+cosine (0.0002 → 0.002 for 3k, anneal to 0.0002)&lt;/li&gt; &lt;li&gt;L2 weight decay 0.01&lt;/li&gt; &lt;li&gt;Random resized cropping and horizontal flipping&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;comparison-with-cnns&quot;&gt;Comparison with CNNs&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mobilevit-comperision-with-cnn.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h3 id=&quot;comparison-with-vits&quot;&gt;Comparison with ViTs.&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mobilevit-compersition-with-vit.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;ViT계열 모델들은 augmentation에 민감하다 따라서 basic과 advanced로 나누엇다.&lt;/p&gt; &lt;h2 id=&quot;mobile-object-detection&quot;&gt;MOBILE OBJECT DETECTION&lt;/h2&gt; &lt;p&gt;MS-COCO에서 평가하였고 SSD에서 backbone만을 교체하여 실험하였다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mobilevit-comparsition-with-detection.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;mobile-semantic-segmentation&quot;&gt;MOBILE SEMANTIC SEGMENTATION&lt;/h2&gt; &lt;p&gt;DeepLabv3를 사용하였으며 데이터셋은 pascal voc 2012를 사용했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mobilevit-comparsition-segmentation.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;performance-on-mobile-devices&quot;&gt;PERFORMANCE ON MOBILE DEVICES&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mobilevit-comparisoin-using-iphone.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;CorML을 사용하여 iPhone12에서 실험을 진행했을 때 patch size별로 실험을 했을 때 모든 모델들은 real-time에서 동작하였다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/mobilevit-performance-using-iphone.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;하지만 mobilenet과 같은 CNN모델보다는 성능이 안좋았다.&lt;/p&gt; &lt;p&gt;저자는 이를 하드웨어 optimization이 지원되지 않아서라고 추측한다.&lt;/p&gt; </description> <pubDate>Wed, 29 Mar 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/mobile-vit/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/mobile-vit/</guid> <category>transformer</category> <category>mobile-backbone</category> <category>backbone</category> <category>paper</category> </item> <item> <title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;현재 NLP와 Vision분야에서 transformer는 활발히 사용되고 있다. 하지만 transformer는 메로리를 많이 잡아먹는 모듈이였고 이를 해결하기 위해 sparse-approximation, low-rank approximation등을 제안하였다. 하지만 이들은 이론과 달리 computational speed를 증가시켜주지 못하는 경우가 많았다. 저자는 GPU에서 빠른 SRAM으로 연산을 수행할 수 있는 IO-aware algorithm을 제시하였다.&lt;/p&gt; &lt;h2 id=&quot;hardware-performace&quot;&gt;Hardware performace&lt;/h2&gt; &lt;h3 id=&quot;gpu-memory-hierchy&quot;&gt;GPU Memory Hierchy&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/fastattention-gpu-hierchy.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;GPU는 CPU와 마찬가지로 메모리 계층을 가진다. DRAM이 가장 느리고 용량이 크며 SRAM이 가장 빠르고 용량이 작다. GPU는 병렬연산시 데이터를 HBM에서 가져온 후 SRAM에 올려놓고 연산을 한다. 이후 다른 데이터를 읽어드리면 SRAM에 있는 정보는 다시 HBM에 저장된다.&lt;/p&gt; &lt;h3 id=&quot;performance-characteristics&quot;&gt;Performance characteristics&lt;/h3&gt; &lt;p&gt;퍼포먼스를 고려할 때 연산량과 메모리접근의 관점으로 두 가지를 나눌 수 있다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Compute-bound: 연산량이 메모리접근보다 많은 경우이다. ex) MatMul&lt;/li&gt; &lt;li&gt;Memory-bound: 메모리접근이 연산량보다 많은 경우이다. ex) softmax, batchnorm&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;kernel-fusion&quot;&gt;Kernel fusion&lt;/h3&gt; &lt;p&gt;Memory-bound연산을 가속시키는데 많이 사용되는 방법은 kernel fusion이다. 만약 같은 input에 대해 여러 연산을 한다고하면 컴파일러는 자동적으로 많은 elementwise operation을 fusion한다.&lt;/p&gt; &lt;h2 id=&quot;standard-attention-implementation&quot;&gt;Standard Attention Implementation&lt;/h2&gt; &lt;p&gt;Sequnce length \(N\)과 head dimension \(d\)에 대하여 attention는 input sequence \(Q,K,V \in \mathbb{R}^{N \times d}\) 를 이용하여 \(O \in \mathbb{R}^{M \times d}\)를 구한다. 그에대한 식은 다음과 같다.&lt;/p&gt; \[S=QK^\top \in \mathbb{R}^{N \times N}, P=softmax(S) \in \mathbb{R}^{N \times N}, O = PV \in \mathbb{R}^{N \times d}\] &lt;p&gt;이 때 softmax는 row-wise operatio이다. 보통의 attention은 \(O(N^2)\)의 memory cost를 사용하는데 대다수의 경우에는 \(N \gg d\)를 만족한다(GPT-2, N=1024 and d=64).&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/standard-attention-algorithm.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h1 id=&quot;flash-attention&quot;&gt;Flash Attention&lt;/h1&gt; &lt;p&gt;FlashAttention은 &lt;strong&gt;Tiling&lt;/strong&gt;과 &lt;strong&gt;Recomputation&lt;/strong&gt;을 사용하여 Attention을 가속화한다.&lt;/p&gt; &lt;h3 id=&quot;tiling&quot;&gt;Tiling&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/fastattention-tiling.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;기존의 softmax연산은 다음과 같은 과정을 거친다.&lt;/p&gt; \[m(x):=\underset{i}{max}(x_i), f(x):=[e^{x_1-m(x)} ... e^{x_B-m(x)}],\] \[l(x):=\sum_i f(x)_i, softmax(x):= \frac{f(x)}{l(x)}\] &lt;p&gt;vector \(x^{(1)}, x^{(2)} \in \mathbb{R}^B\)일 때 vector의 concatenation \(x=[x^{(1)} x^{(2)}]\) 대해 softmax는 다음과 같이 decomposition을 할 수 있다.&lt;/p&gt; \[m(x)=m([x^{(1)} x{(2)}])=max(m(x^{(1)})),m(x^{(2)}),\] \[f(x):=[e^{m(x^{(1)})-m(x)}f(x^{(1)}) ... e^{x^{(2)}-m(x)}f(x^{(2)})],\] \[l(x)=l([x^{(1)} x{(2)}])=e^{m(x^{(1)})-m(x)}l(x^{(1)}) + e^{x^{(2)}-m(x)}l(x^{(2)}),\] \[softmax(x):= \frac{f(x)}{l(x)}\] &lt;p&gt;즉, softmax를 block단위를 쪼개서 계산할 수 있다는 것이다.&lt;/p&gt; &lt;h3 id=&quot;recomputation&quot;&gt;Recomputation&lt;/h3&gt; &lt;p&gt;저자는 baward때 \(O(N^2)\)의 memory를 저장하지 않기 위해서 softmax normalization statistics \((m,l)\)을 저장한 수 backward때 다시 구성한다. 이로인해 FLOPs는 증가하지만 HBM에서 데이터를 읽는 회수가 줄어들어 속도가 향상된다.&lt;/p&gt; &lt;h3 id=&quot;kernel-fusion-1&quot;&gt;Kernel Fusion&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/fastattention-kernel-fusion.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Tiling을 통해 한 번의 HBM load에서 matrix multiply, softmax, optionally masking and dropout, matrix multiply을 한 후 HBM에 저장할 수 있게 되었다. 이는 반복적인 IO operaion을 줄여준다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/fastattention-algorithm.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;. Algorithm 1 returns \(O=softmax(QL^\top)V\)with \(O(N^2d)\) FLOPs and requires additional memory beyond inputs and output&lt;/p&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;analysis-io-complexity-of-flashattention&quot;&gt;Analysis: IO Complexity of FlashAttention&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/fastattention-coomplexity.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Flash attention은 standard보다 GFLOPs는 많지만 HBM read and write가 적어 runtime이 개선되었다.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Theorem 2.&lt;/strong&gt; Let \(N\) be the sequence length, \(d\) be the head dimension, and \(M\) be size of SRAM with \(d \leq M \leq Nd\). Standard attention (Algorithm 0) requires \(\Theta(Nd+N^2)\) HBM accesses, while FlashAttention (Algorithm 1) requires \(\Theta(N^2d^2M^{-1})\) HBM accesses.&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Proposition 3.&lt;/strong&gt; Let \(N\) be the sequence length, \(d\) be the head dimension, and \(M\) be size of SRAM with \(d \leq M \leq Nd\). There does not exist an algorithm to compute exact attention with \(\Theta(N^2d^2M^{-1})\) HBM accesses for all \(M\) in the range \([d, Nd]\).&lt;/p&gt; &lt;/blockquote&gt; &lt;h1 id=&quot;extension&quot;&gt;Extension&lt;/h1&gt; &lt;p&gt;Block-sparse attention을 응용하여 block-sparse flashattention을 만들기도 했다.&lt;/p&gt; \[S=QK^\top \in \mathbb{R}^{N \times N}, P=softmax(S \odot \mathbb{1}_{\tilde{M}}) \in \mathbb{R}^{N \times N}, O=PV \in \mathbb{R}^{N \times d}\] &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Proposition 4.&lt;/strong&gt; Let \(N\) be the sequence length, \(d\) be the head dimension, and \(M\) be size of SRAM with \(d \leq M \leq Nd\). Block-sparse FlashAttention (Algorithm 5) requires \(\Theta(N^2d^2M^{-1})\) HBM accesses where 𝑠 is the fraction of nonzero blocks in the block-sparsity mask.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;experimant&quot;&gt;Experimant&lt;/h2&gt; &lt;p&gt;FlashAttention은 tiling을 통해 속도가 빠르고 recomputation을 통해 메모리가 줄어들었다. 이를 이용하여 sequnce length를 늘릴 수 있었고 이는 추가적인 성능향상을 가져왔다.&lt;/p&gt; &lt;h3 id=&quot;bert&quot;&gt;Bert&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/fastattention-bert-performance.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Bert 학습 시 MLPerf 1.1기준 학습시간이 15% 개선되었다.&lt;/p&gt; &lt;h3 id=&quot;gpt-2&quot;&gt;GPT-2&lt;/h3&gt; &lt;p&gt;GPT-2는 Huggingface, Megatron-LM과 비교했는데 각각 3배, 1.7배의 speed up이 생겼다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/fastattention-gpt-2-performace.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h3 id=&quot;long-range-arena&quot;&gt;Long-range Arena&lt;/h3&gt; &lt;p&gt;LRA에서도 기존대비 2.4x speed up을 보였으며 다른 attention method보다 성능도 좋았다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/fastattention-long-reange-arena-performace.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;better-models-with-longer-sequences&quot;&gt;Better Models with Longer Sequences&lt;/h2&gt; &lt;h3 id=&quot;language-modeling-with-long-context&quot;&gt;Language Modeling with Long Context.&lt;/h3&gt; &lt;p&gt;Recomputing으로 메모리사용량이 줄어들면서 더 긴 input sequce를 다룰 수 있게 되었다. 이를통해 추가적인 성능향상을 가져왔다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/fastattention-bert-with-long-sequence.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h3 id=&quot;long-document-classification&quot;&gt;Long Document Classification&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/fastattention-long-document-classification.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h3 id=&quot;path-x-and-path-256&quot;&gt;Path-X and Path-256&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/fastattention-path-x.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Path-X와 Path-256은 long context로 기존의 모델들은 random한 결과와 비슷하게 나왔다. FlashAttention은 해당 데이터셋에 random 이상의 결과를 가져온 첫 번째 모델이다.&lt;/p&gt; &lt;h2 id=&quot;benchmarking-attention&quot;&gt;Benchmarking Attention&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/fastattention-banchmarking.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Attention계열(Attention, FlashAttention)은 메모리 사용량이 \(O(N^2)\)이지만 approximate attetion(sparse attention)은 \(O(n)\)이다. 따라서 seqence length를 키우다보면 approximate attention이 더 좋은 결과를 보여주지만 성능에서 우수하다.&lt;/p&gt; &lt;h1 id=&quot;limitation&quot;&gt;Limitation&lt;/h1&gt; &lt;p&gt;FlashAttention은 CUDA kernel를 사용해야하므로 엔지니어링이 필요하다. 그리고 GPU마다 컴파일이 필요하는 등 확장성이 문제가 있다. 그리고 현재는 single gpu를 기준으로 만들어져있어서 multi-GPU를 위한 알고리즘도 제작해야한다.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;추가글…&lt;/strong&gt;&lt;br /&gt; 아앗… 포스트를 올리고 4개월만에 url이 틀렸다는 것을 깨달았다… 하지만 어쩔 수 없다… 그냥 간다… fastattentiondl 아니라 flashattention으로 해야하는데….&lt;/p&gt; </description> <pubDate>Tue, 28 Mar 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/fastattention/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/fastattention/</guid> <category>transformer</category> <category>hardware-optimization</category> <category>paper</category> </item> <item> <title>Cross-Domain Adaptive Teacher for Object Detection</title> <description>&lt;h1 id=&quot;intorduction&quot;&gt;Intorduction&lt;/h1&gt; &lt;p&gt;데이터셋 중에서 label이 있는 데이터도 있지만 label이 없는 데이터도 있다. 이 두개의 domain이 다를 경우 이를 다루는 것은 쉽지 않다. 이 논문은 object detection에서 source domain은 label이 있고 target domain은 label이 없는 상황에 semi-supervised로 doamain adaptation을 하는 방법을 다루고 있다.&lt;/p&gt; &lt;h1 id=&quot;adaptive-teacher&quot;&gt;Adaptive teacher&lt;/h1&gt; &lt;p&gt;논문에서 semi-supervised learning을 위하여 teacher-student 모델을 사용한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/cross-domain-adaptation-for-object-detection.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;학습은 다음과 같이 진행된다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Source domain을 이용하여 object detector를 학습한다.&lt;/li&gt; &lt;li&gt;학습된 object detector를 복사하여 teacher model과 student 모델을 제작한다.&lt;/li&gt; &lt;li&gt;Teacher model은 target domain에 weak augmentation을 적용해 psuedo-label을 만든다.&lt;/li&gt; &lt;li&gt;Student model은 strong augmentation된 source data로 supervised loss, strong augmentation된 target domain으로 unsupervied loss, discriminator로 discriminator loss를 계산한다.&lt;/li&gt; &lt;li&gt;Student model은 gradient update로 학습한다.&lt;/li&gt; &lt;li&gt;Teacher model은 EMA로 학습한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;이 때 psuedo-label은 confidence threshold( \(\delta\) )를 사용하여 양질의 것만 사용한다.&lt;/p&gt; &lt;h1 id=&quot;augmentation&quot;&gt;Augmentation&lt;/h1&gt; &lt;p&gt;기본적으로 teacher model은 source data에서 학습하고 target domain의 psuedo-label을 제작한다. 따라서 positive-false과 같은 오류가 발생할 수 있으므로 더 정확한 psuedo-label을 위해 psuedo-label 제작시 weak augmentation을 적용한다. 하지만 student는 믿을만한 psuedo-label을 가지고 있기 때문에 strong augmentationd르 적용한다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weak augmentation: horizontal flipping, cropping&lt;/li&gt; &lt;li&gt;Strong augmentation: randomly color jittering, grayscaling, Gaussian blurring, and cutting patches&lt;/li&gt; &lt;/ul&gt; &lt;h1 id=&quot;loss&quot;&gt;Loss&lt;/h1&gt; &lt;p&gt;loss는 supervised loss, unsupervised loss, discrimination loss를 사용한다. Psuedo-label에서 사용하는 confidence threshol(\(\delta\))는 class의 confidence만 반영하고 bouding box의 confidence를 반영하지 않는다. 따라서 unsupervised loss에서는 class loss만을 사용한다.&lt;/p&gt; \[\mathcal{L}_{sup}(X_s, B_s, C_s)=\mathcal{L}_{cls}^{rpn}(X_s, B_s, C_s) +\mathcal{L}_{reg}^{rpn}(X_s, B_s, C_s) +\mathcal{L}_{cls}^{roi}(X_s, B_s, C_s) +\mathcal{L}_{reg}^{roi}(X_s, B_s, C_s)\] \[\mathcal{L}_{unsup}(X_s, B_s, C_s)=\mathcal{L}_{cls}^{rpn}(X_s, B_s, \hat{C_s}) +\mathcal{L}_{cls}^{roi}(X_s, B_s, \hat{C_s})\] \[\mathcal{L}_{dis}=-d \times logD(E(X))-(1-d)\times log(1-D(E(X))\] &lt;h1 id=&quot;ema&quot;&gt;EMA&lt;/h1&gt; &lt;p&gt;Teacher model의 update는 EMA를 사용한다.&lt;/p&gt; \[\theta_t \leftarrow \alpha \theta_t + (1-\alpha)\theta_s\] &lt;h1 id=&quot;result&quot;&gt;Result&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/da_result.png&quot; width=&quot;100%&quot; /&gt; &lt;img src=&quot;/assets/post/image/legacy/da_result_2.png&quot; width=&quot;50%&quot; /&gt; &lt;img src=&quot;/assets/post/image/legacy/da_result_3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;AT는 SOTA를 찍긴 했지만 fully supervised learning (Oracle)보다 성능이 높은 것은 주목할만 하다.&lt;/p&gt; </description> <pubDate>Tue, 21 Mar 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/cross-domain-adaptive-teacher-for-object-detection/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/cross-domain-adaptive-teacher-for-object-detection/</guid> <category>domain-adaptation</category> <category>paper</category> </item> <item> <title>Rethinking “Batch” in BatchNorm</title> <description>&lt;h1 id=&quot;intorduction&quot;&gt;Intorduction&lt;/h1&gt; &lt;p&gt;BatchNorm은 layer의 중간에 들어가며 학습을 안정화하여 학습속도를 상승시키고 오버피팅을 방지한다. 하지만 train test domain이 많이 다르거나 batch size가 현저하게 작다면 batch norm은 문제를 일으키기도 한다. 따라서 이 논문에서는 BatchNorm의 함정을 정리하고 권장사항을 제시한다.&lt;/p&gt; &lt;h2 id=&quot;batchnorm&quot;&gt;BatchNorm&lt;/h2&gt; &lt;p&gt;BatchNorm은 다음과 같은 식으로 정의된다.&lt;/p&gt; \[y=\frac{x-\mu}{\sqrt{\sigma^2+\epsilon}}\] &lt;p&gt;보통 평균과 분산은 학습시 mini-batch를 통하여 모평균과 모분산을 추정한다.&lt;/p&gt; \[\mu_{\mathcal{B}}=mean(X,axis=[N,H,W])\] \[\sigma_{\mathcal{B}}^2=var(X,axis=[N,H,W])\] &lt;p&gt;기존의 BatchNorm은 흔히 모평균과 모분산을 예측하기 위해 EMA(Exponential Moving Average)를 사용한다.&lt;/p&gt; \[\mu_{EMA} \leftarrow \lambda \mu_{EMA} + (1-\lambda) \mu_{\mathcal{B}}\] \[\sigma_{EMA}^2 \leftarrow \lambda \sigma_{EMA}^2 + (1-\lambda) \sigma_{\mathcal{B}}^2\] &lt;p&gt;하지만 EMA는 이전에 사용한 값을 대부분 가져가 실제 평균, 분산을 늦게 반영한다. (\(\lambda\)는 보통 0.9로 설정한다.)&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/precisebn_bn_plot.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h1 id=&quot;presizebn&quot;&gt;PresizeBN&lt;/h1&gt; &lt;p&gt;따라서 저자는 PreciseBN을 제안했다. PresizeBN은 mini-batch마다 batch parameter를 update하지 않고 한 epoch이 끝나면 model을 freeze한 후 batch statistics를 aggregation을 하여 update한다.&lt;/p&gt; \[\mu_{pop}=E[\mu_{\mathcal{B}}], \sigma_{pop}^2=E[\mu_{\mathcal{B}}^2+\sigma_M^2]-E[\mu_{\mathcal{B}}^2]\] &lt;h1 id=&quot;ema-vs-precisebn&quot;&gt;EMA vs PreciseBN&lt;/h1&gt; &lt;h2 id=&quot;large-batch-size&quot;&gt;Large Batch Size&lt;/h2&gt; &lt;p&gt;PreciseBN은 EMA보다 안정적이다. 저자는 먼저 batch size가 매우 클 경우에 대하여 실험했다. EMA에서 batch size가 커지면 절대적인 update수가 적어져 validation error의 분산이 커진다. 하지만 PreciseBN은 한 epoch이후에 update하기 때문에 분산이 커지는 일이 없었고 실험적으로 1k~10k의 sample을 aggregation을 하면 모수를 추정하기 충분했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/precisebn_batch_size_plot.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;small-batch-size&quot;&gt;Small Batch Size&lt;/h2&gt; &lt;p&gt;EMA는 mini-batch만 볼 수 있기떄문에 batch size가 작을수록 성능하락이 커 PresiceBN의 성능이 높았다. 하 지만 실험적으로는 EMA는 모델이 충분히 학습하여 수렴했을 때와 Batch size가 충분히 큰 경우 Precise BN과 성능차이가 별로 없었다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/presize_bn_ema_result_with_NBS.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;EMA는 Batchsize가 작아질수록 train-test inconsisitency가 커진다. 따라서 inference에 mini-batch statistic을 이용하면 성능하락이 줄어든다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/presizebn_val_train_generalizationgan.png&quot; width=&quot;50%&quot; /&gt; &lt;img src=&quot;/assets/post/image/legacy/presizebn_pergpu_bn.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;h1 id=&quot;frozenbn&quot;&gt;FrozenBN&lt;/h1&gt; &lt;p&gt;Finetuning할 때 batch norm을 freeze한다. 하지만 일반적인 training에서도 효과를 보는 것을 찾아냈다. 이 때 학습 epoch에서 중간쯤부터 batchnorm을 freeze하면 된다. ImageNet실험에서는 마지막 20 epoch때 frozneBN을 사용하요 train-test-inconsistency를 개선했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/precise_bn_freezebn.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;h1 id=&quot;adaptive-batchnorm&quot;&gt;Adaptive BatchNorm&lt;/h1&gt; &lt;p&gt;Train test에서 큰 domain inconsistency가 존재한다. 따라서 Test set에서 batch norm의 polution statistics를 학습 후 평가했을 때 정확도가 상승했다. 이 때 train과 evalutation의 augmentation 방법이 동일해야한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/presizebn_adaptive_bn.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;h1 id=&quot;batchnorm-in-multi-domain-training&quot;&gt;BatchNorm in multi-domain training&lt;/h1&gt; &lt;p&gt;multi-domain에서 다음의 식이 성립한다.&lt;/p&gt; \[f([X_1, X_2,...,X_n]) \neq [f(X_1, f(X_2), ..., f(X_n)]\] &lt;p&gt;즉, domain 별로 normalize하는 것과 domain을 합쳐 normalize하는 것과 다르다. 이것이 문제를 일으킨다.&lt;/p&gt; &lt;p&gt;저자는 이를 retinanet에 실험을 했다. Retinanet은 size가 다른 featuremap을 공유된 head로 detection을 진행한다. 하지만 의 각각의 feature map은 다른 domain을 볼 수 있다. 따라서 각각의 featuremap을 normaization하는 것이 아닌 합쳐서 normalization을 하면 성능이 높아진다. 이 대 traning과 population statistics과 affine parameter의 환경을 일치하는 것이 중요하다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/presizebn_bn_with_multidomain.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h1 id=&quot;ghost-batchnorm-layernorm&quot;&gt;Ghost BatchNorm, LayerNorm&lt;/h1&gt; &lt;p&gt;Training하다보면 mini-batch안에 같은 class의 image가 들어가는 경우가 있다. 이럴때 모델은 mini-bath에서 class의 hint를 얻을 수 있기 때문에 mini-batch간 dependency가 존재할 수 있고 이는 bias를 유발한다. 따라서 minibatch에서 서로 다른 class 끼리 normalizaion을 진행하면 성능이 올라간다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/presizebn_ghost_bn.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; </description> <pubDate>Tue, 14 Mar 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/rethinking-batch-in-batchnorm/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/rethinking-batch-in-batchnorm/</guid> <category>batch-normalization</category> <category>paper</category> </item> <item> <title>Convolutional Character Network</title> <description>&lt;h1 id=&quot;intorduction&quot;&gt;Intorduction&lt;/h1&gt; &lt;p&gt;대부분 Scene Text Detection은 Text Detection과 Text Recognization으로 나누어져 있다. 하지만 이러한 방법은 representation을 제대로 학습하지 못해 성능이 하락한다. 따라서 저자는 Detection과 Recognization이 합쳐진 CharNet을 제안한다. 또한 CNN과 RNN을 같이 학습하는 것이 어려운 과제이기 때문에 Character 단위로 Text를 예측하는 모델을 제시한다.&lt;/p&gt; &lt;h1 id=&quot;convolutional-character-network&quot;&gt;Convolutional Character Network&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/convolutional_neural_network_architecture.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;모델은 Detection Branch와 Character Branch로 나뉘어져 있다. Character branch는 character detection과 recognization을 위해 만들어졌고 Text detection은 Text의 bbox를 예측한다.&lt;/p&gt; &lt;h2 id=&quot;backbone&quot;&gt;Backbone&lt;/h2&gt; &lt;p&gt;Backbone으로 ResNet50을사용하였고 높은 해상돌르 위하하여 featuremap size를 1/4로만 줄인다. 또한 두 개의 Hour Glass Module을 쌓는다. 이때 Hourglass-104에서 down sampling과 마지막 몇 개의 layer를 제거하여 Hourglass-88을 제작한다.&lt;/p&gt; &lt;h2 id=&quot;character-branch&quot;&gt;Character Branch&lt;/h2&gt; &lt;p&gt;Word 단위로 인식하는 네트워크들은 필연적으로 RNN계열의 모델이 들어간다. 하지만 이로인해 더 많은 데이터와 작업이 필요하므로 word단위가 아닌 character단위로 만드는 것이 성능을 높힐 수 있다. 따라서 저자는 Character branch를 도입했다.&lt;/p&gt; &lt;p&gt;Character branch는 Text instance segmentation, character detection, character recognization으로 이루어져있으며 앞 2가지는 3개(3x3, 3x3, 1x1)의 CNN layer을 가지고 있으며 character recognization은 4개의 CNN layer를 가지고 있다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text instance segmentation: text와 non-text를 구분하는 역할로 2개의 channel을 가지고 있다&lt;/li&gt; &lt;li&gt;Character detection: character의 bbox와 orientation을 예측하는 5개의 channel을 가지고 있다.&lt;/li&gt; &lt;li&gt;Character recognization: 26개의 알파펫, 10개의 숫자, 32개의 특수문자를 분류한다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;이 때 Character의 bbox는 95%이상의 confidence를 갖는 것만 이용한다.&lt;/p&gt; &lt;h2 id=&quot;text-detection-branch&quot;&gt;Text Detection Branch&lt;/h2&gt; &lt;p&gt;Text detection branch는 더 높은 수준의 representation을 학습한다. 또한 word 사이 간격이 가까우면 character로 word를 만들기 어렵기 때문에 text detection을 같이사용한다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-Orientation Text: text, non-text를 나누는 2체널 bbox와 orientation을 학습하는 5체널을 학습한다.&lt;/li&gt; &lt;li&gt;Curved Text: Direction layer를 사용한다.&lt;/li&gt; &lt;li&gt;Generation of Final Results: 만약 Text bbox와 character bbox이 겹치는 정도가 일정 수준을 넘어가면 해당 text에 character가 있다고 판단한다.&lt;/li&gt; &lt;/ul&gt; &lt;h1 id=&quot;iterative-character-detection&quot;&gt;Iterative Character Detection&lt;/h1&gt; &lt;p&gt;저자는 text detection/recognization을 위해 character-level annotation이 필요하나 많은 데이터셋은 word-level annotation을 가지고 있다. 이 때 Synth800k는 두 수준의 annotation을 모두 가지고 있는데 이는 현실데이터와 달라 현실데이터로 fine-tuning하지 않으면 성능이 낮다. 따라서 저자는 다음과 같은 전략을 취한다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Synth800k을 사용하여 pretrained weight를 제작한다.&lt;/li&gt; &lt;li&gt;1번에서 학습시킨 모델과 word-level annotation을 이용하여 weakly-suprevised learning을 한다. 이 때 character bbox와 달리 character class는 label로 사용하지 않는다.&lt;/li&gt; &lt;li&gt;2번 과정에서 만들어진 모델을 가지고 weakly-suprevised learning을 반복하여 성능을 높힌다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Weakly supervised learning은 1번을 학습시킨 모델을 사용하여 word-level annotation에서 character level annotation(label)을 만드는 것인데 만약 예측이 다음과 같은 조건을 만족시키면 “correct”라고 가정한다.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Text instance에 존재하는 character bounding box의 개수와 text instance의 label(word)가 일치하면 해당 pseudo label은 correct하다고 결정한다.&lt;/em&gt;&lt;/p&gt; </description> <pubDate>Tue, 07 Mar 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/convolutional-character-network/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/convolutional-character-network/</guid> <category>scene-text-detection</category> <category>scene-text-recognition</category> <category>paper</category> </item> <item> <title>Jetson Nano Tensorrt 적용</title> <description>&lt;p&gt;이전에 RTX2070에 TenssorRT를 적용했던 것처럼 jetson nano에서도 tensorrt를 적용하고 싶었고 호환성에 애를먹다가 성공했다. python tenosrrt package를 buil하기 귀찮아 ubuntu 18.08을 사용하여 내부에 설치되어있는 것을 이용했다.&lt;/p&gt; &lt;p&gt;Jetson nano 18.04 설치방법은 아래에 나와있다. 여기서 20.04버전이 아닌 18.04 공식 이미지를 사용해야한다.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;http://wonbeomjang.kr/blog/2023/jetson-nano-ubuntu/&quot;&gt;http://wonbeomjang.kr/blog/2023/jetson-nano-ubuntu/&lt;/a&gt;&lt;/p&gt; &lt;h1 id=&quot;1-pytorch-속도-측정&quot;&gt;1. Pytorch 속도 측정&lt;/h1&gt; &lt;p&gt;tensorrt를 사용하기 앞서 pytorch에서 gpu에 올려 돌렸을 때 inference time을 측정해봤다.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mobilenetv3&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_f32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mobilenetv3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mobilenet_v3_small&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_f32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# caching model to gpu &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model_f32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_f32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model_f32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_f32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inference_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_f32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tmp.pth&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getsize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tmp.pth&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e6&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tmp.pth&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inference_time&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; ms / &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_size&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;MB&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;output&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;37.80 ms / 10.3278MB &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;tensorrt에서 float16을 사용할 예정으로 torch.float16으로 type을 변경하여 실험하자.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model_f16&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mobilenetv3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mobilenet_v3_small&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_f16&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# caching model to gpu &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model_f16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_f16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model_f16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_f16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inference_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_f16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tmp.pth&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getsize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tmp.pth&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e6&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tmp.pth&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inference_time&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; ms / &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_size&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;MB&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;output&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;41.11 ms / 5.2162MB &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h1 id=&quot;2-onnx-export&quot;&gt;2. ONNX export&lt;/h1&gt; &lt;p&gt;tensorrt 라이브러리를 이용하기 위해서 pytorch model을 onnx model로 변환해줘야한다. torch.onnx를 통해 mobilenet을 export하자.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;BATCH_SIZE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dummy_input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BATCH_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;onnx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;export&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_f32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dummy_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;mobilnet_f32.onnx&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;그리고 만약 jupyter notebook이라면 tensorrt로 변환하는 동안 pytorch 때문에 gpu 메모리가 부족하여 변환에 실패한다. 따라서 exit를 사용하여 kernel을 종료하자.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;_exit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h1 id=&quot;3-tensorrt-변환&quot;&gt;3. Tensorrt 변환&lt;/h1&gt; &lt;p&gt;trtexec을 통해 mobilenet model을 tensorrt model로 변환하자.&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;trtexec &lt;span class=&quot;nt&quot;&gt;--onnx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mobilnet_f32.onnx &lt;span class=&quot;nt&quot;&gt;--saveEngine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mobilnet_f16_trt.trt &lt;span class=&quot;nt&quot;&gt;--explicitBatch&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--inputIOFormats&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;fp16:chw &lt;span class=&quot;nt&quot;&gt;--outputIOFormats&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;fp16:chw &lt;span class=&quot;nt&quot;&gt;--fp16&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;-bash-trtexec-command-not-found&quot;&gt;-bash: trtexec: command not found&lt;/h3&gt; &lt;p&gt;만약 trtexec을 찾을 수 없다고 오류를 띄운다면 bashrc file을 수정하여 아래의 path를 추가하자.&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; /usr/src/tensorrt/bin/ &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/src/tensorrt/bin:&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h1 id=&quot;4-tensorrt-inference&quot;&gt;4. TensorRT inference&lt;/h1&gt; &lt;p&gt;먼저 trt.Runtime 객체를 만들어 runtime용 객체를 만들고 tensorrt file을 읽어들어와 engine을 context를 선언하자.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorrt&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trt&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pycuda.driver&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pycuda.autoinit&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;resnet_engine_pytorch.trt&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rb&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;runtime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Runtime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WARNING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;runtime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;deserialize_cuda_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;create_execution_context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;cuda를 용하여 intput, output용 메모리를 할당한다. 그후 inference를 위한 stream 객체를 선언한다.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mem_alloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mem_alloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bindings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;이후 numpy array를 cuda memory에 copy하고 모델을 inferecne한 다음 d_output을 output ndarray에 copy한다. 마지막으로 병렬처리를 위한 stream threads을 syncronize하여 마무리한다.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# result gets copied into output &lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# transfer input data to device &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;memcpy_htod_async&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# execute model &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;execute_async_v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bindings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# transfer predictions back &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;memcpy_dtoh_async&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# syncronize threads &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;synchronize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;이제 time을 측정해보자.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# caching model to gpu &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inference_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getsize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;mobilnet_f32_trt.trt&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e6&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inference_time&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; ms / &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_size&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;MB&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;output&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;6.95 ms / 5.8507MB &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h2 id=&quot;결론&quot;&gt;결론&lt;/h2&gt; &lt;p&gt;확실히 속도측면에서 이득을 보고있다. 하지만 최적화 옵션을 안 넣고 단순히 fp16으로 바꿔서 그런지 메모리측면에서는 이득을 보지 못했다. 이전 torch_tensorrt에서는 메모리측면에서도 이득을 보아 더 개선가능성이 있어보인다.&lt;/p&gt; &lt;table align=&quot;center&quot;&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;Inference Time (ms)&lt;/td&gt; &lt;td&gt;Model Parameter (MB)&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;MobileNetV2 float32&lt;/td&gt; &lt;td&gt;37.80&lt;/td&gt; &lt;td&gt;10.3278&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;MobileNetV2 float16&lt;/td&gt; &lt;td&gt;41.11&lt;/td&gt; &lt;td&gt;5.2162&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;TensorRT&lt;/td&gt; &lt;td&gt;6.95&lt;/td&gt; &lt;td&gt;5.8507&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; </description> <pubDate>Tue, 28 Feb 2023 15:00:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/jetson-nano-tensorrt/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/jetson-nano-tensorrt/</guid> <category>jetson-nano</category> <category>tensorrt</category> <category>hardware-optimization</category> </item> <item> <title>error: command &apos;aarch64-linux-gnu-gcc&apos; failed with exit status 1</title> <description>&lt;p&gt;Jetson nano에 pycuda를 설치하다가 다음과 같은 오류를 만났다.&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aarch64-linux-gnu-gcc -pthread -fwrapv -Wall -O3 -DNDEBUG -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DBOOST_ALL_NO_LIB=1 -DBOOST_THREAD_BUILD_DLL=1 -DBOOST_MULTI_INDEX_DISABLE_SERIALIZATION=1 -DBOOST_PYTHON_SOURCE=1 -Dboost=pycudaboost -DBOOST_THREAD_DONT_USE_CHRONO=1 -DPYGPU_PACKAGE=pycuda -DPYGPU_PYCUDA=1 -DHAVE_CURAND=1 -Isrc/cpp -Ibpl-subset/bpl_subset -I/usr/lib/python3/dist-packages/numpy/core/include -I/usr/include/python3.6m -c src/cpp/cuda.cpp -o build/temp.linux-aarch64-3.6/src/cpp/cuda.o In file included from src/cpp/cuda.cpp:4:0: src/cpp/cuda.hpp:14:10: fatal error: cuda.h: No such file or directory #include &amp;lt;cuda.h&amp;gt; ^~~~~~~~ compilation terminated. error: command &apos;aarch64-linux-gnu-gcc&apos; failed with exit status 1 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h2 id=&quot;첫-번째-시도는-흔히하는-cuda-path-설정이다&quot;&gt;첫 번째 시도는 흔히하는 cuda path 설정이다.&lt;/h2&gt; &lt;p&gt;~/.bachrc 에서 아래를 추가해준다.&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if [ -d /usr/local/cuda/ ]; then export PATH=/usr/local/cuda/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH fi &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;하지만 이전에 추가했던 것이었고 내겐 해당사항이 없었다.&lt;/p&gt; &lt;h2 id=&quot;두-번째-시도-cuda-library-incude&quot;&gt;두 번째 시도 cuda library incude&lt;/h2&gt; &lt;p&gt;사실 설치파일에서 cuda.h를 못찾는 것으로 cuda library를 인식하게 만들어주면 된다. 하지만 bashrc에서 어떻게 수정하는지 몰랐고 다음과 같은 명령어를 통해 설치완료했다.&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;pip3 &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--global-option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;build_ext &lt;span class=&quot;nt&quot;&gt;--global-option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;-I/usr/local/cuda/include&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--global-option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;-L/usr/local/cuda/lib64&quot;&lt;/span&gt; pycuda &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;그러자 잘 설치되었다.&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Installing collected packages: pycuda Running setup.py install for pycuda ... done Successfully installed pycuda-2022.1 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; </description> <pubDate>Fri, 24 Feb 2023 09:50:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/jetson-nano-pycud-error/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/jetson-nano-pycud-error/</guid> <category>jetson-nano</category> <category>error</category> </item> <item> <title>Simple Baselines for Image Restoration</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;Image restoration에서 SOTA Network들은 많은 모듈을 추가하여 complexity가 증가하였다. 저자는 이를 inter-block complexity, intra-block complexity로 나누오 생각했고, low inter-block complexity와 low intra-block complexity로 SOTA를 달성하기 위해 여러 실험을 진행하였다.&lt;/p&gt; &lt;p&gt;기본적으로 UNet구조를 따랐고, convolution-relu-shortcut으로 구성된 간단한 plain block으로 시작하였다. 이후 plain block에서 SOTA method 중 필수적이라고 생각하는 것 만을 하나씩 추가하였고 결과적으로 GELU, Channel Attention을 추가하였다. 이후 더 발전사항으로 GELU를 GLU로 대체하 Channel Attanetion을 GLU형태로 변환해 SC(Simple Channel Attention)로 변경하였다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-perforamce.png&quot; width=&quot;80%&quot; /&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-inter-block-architecture.png&quot; width=&quot;80%&quot; /&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-intra-block-architecture.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h1 id=&quot;simple-baseline&quot;&gt;Simple Baseline&lt;/h1&gt; &lt;p&gt;여러 아이디어를 평가하였는데 채택된 아이디어는 bold체에 밑줄로 표현하겠다.&lt;/p&gt; &lt;h2 id=&quot;internal-architecture&quot;&gt;Internal architecture&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Block &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PlainNet: Conv, ReLU, Shortcut&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Transformer 사용 X &lt;ul&gt; &lt;li&gt;SOTA를 달성하는데 필수요소 X&lt;/li&gt; &lt;li&gt;Depth-wise convolution이 더 간단한 표현&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;BatchNorm &lt;ul&gt; &lt;li&gt;BatchNorm: small batch size에 취약&lt;/li&gt; &lt;li&gt;InstanceNorm: manual한 tuning이 필요&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LayerNorm: 다수의 SOTA에서 사용, 성능 향상과 학습 안정성에 기여&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;learning rate 10배 증가 가능&lt;/li&gt; &lt;li&gt;+0.44dB on SSID, +3.39dB on GoPro&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Activation &lt;ul&gt; &lt;li&gt;ReLU: 좋지만 SOTA에서 미미한 사용&lt;/li&gt; &lt;li&gt;GELU: SOTA에서 많이 사용 &lt;ul&gt; &lt;li&gt;SSID에서 차이 X, +0.21dB on GoPro&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;결과&quot;&gt;결과&lt;/h3&gt; &lt;p&gt;Baseline만으로 SOTA 달성하였다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-baseline-intra-block-architecture.png&quot; width=&quot;80%&quot; /&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-baseline-inter-block-architecture.png&quot; width=&quot;80%&quot; /&gt;\ &lt;/p&gt; &lt;h1 id=&quot;nafnet&quot;&gt;NAFNet&lt;/h1&gt; &lt;p&gt;저자는 baseline network의 GELU와 channel attention의 개선점을 찾아 적용했다.&lt;/p&gt; &lt;h2 id=&quot;gelu&quot;&gt;GELU&lt;/h2&gt; &lt;p&gt;GELU는 많은 SOTA network에서 사용하며 수식은 다음과 같다.&lt;/p&gt; \[GELU(x)=x\phi(x)\] &lt;p&gt;이때 \(\phi\)는 cumulative distribution function of the standard normal distribution으로 다음과 같이 근사화시킬 수 있다.&lt;/p&gt; \[GELU \approx 0.5x(1+tanh[\sqrt{2/\pi}(x+0.044715x^3)])\] &lt;h2 id=&quot;glu-gate-linear-unit&quot;&gt;GLU (Gate Linear Unit)&lt;/h2&gt; &lt;p&gt;SOTA network에서 GLU를 사용한다. 따라서 저자는 이 함수가 baseline의 성능을 향상시킬 수 있다고 생각했다.&lt;/p&gt; \[Gate({X},f,g,\sigma)=f({X}) \odot \sigma(g({X}))\] &lt;p&gt;이는 GELU의 generalization한 format이었고 따라서 GELU를 GLU로 대체하였다. 이 때 $\odot$은 element-wise product이다.&lt;/p&gt; &lt;h2 id=&quot;simplegate&quot;&gt;SimpleGate&lt;/h2&gt; &lt;p&gt;GELU에서 element-wise product으로 인해 non-linearity가 발생한다. 따라서 sigmoid를 제거할 수 있다고 생각하였고 다음과 같은 simple gate function을 완성시킨다.&lt;/p&gt; \[SimpleGate({X},{Y})=X \odot Y\] &lt;p&gt;간단히 말하자면 channel을 반으로 쪼개어 element-wise product를 하였다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-sg.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;channel-attention&quot;&gt;Channel Attention&lt;/h2&gt; &lt;p&gt;Channel attention의 수식은 다음과 같다.&lt;/p&gt; \[CA({X})={X}*\sigma(W_2max(0, W_1pool({X})))\] &lt;p&gt;global average pooling을 이용하여 global한 feature를 aggretation하고 channel들간 상관관계를 계산하기 위해 linear layer를 추가한다. 이 때 CA는 channel-attention calculation을 하나의 함수 \(\psi\)로 간주하여 다음과 같이 재정의할 수 있다. 밑의 식에서 \(*\)는 channel-wise product이다.&lt;/p&gt; \[CA({X})={X}*\psi({X})\] &lt;p&gt;이는 이전에 살펴본 GLU와 format이 유사하여 global informaion aggregation, channel information interaction을 남겨 다음과 같이 SCA(Simple Channel Attention)으로 재정의하였다.&lt;/p&gt; \[SCA({X})={X}*Wpool({W})\] &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-sca.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;nafnet-1&quot;&gt;NAFNet&lt;/h2&gt; &lt;p&gt;결과적으로 만들어진 NAFNet의 block은 다음과 같다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-inter-block-architecture.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;전체적인 아키텍쳐는 앞서 언급했듯 UNet을 따른다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-intra-block-architecture.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;16GMACs computational budet&lt;/li&gt; &lt;li&gt;gradient clip&lt;/li&gt; &lt;li&gt;PSNR loss&lt;/li&gt; &lt;li&gt;Adap optimizer \(\beta_1=0.9, \beta_2=0.9\) weight decay = 0&lt;/li&gt; &lt;li&gt;Total iteration 200K&lt;/li&gt; &lt;li&gt;Cosine scheduler: initail learning rate: 1e-3, final learning rate: 1e-6&lt;/li&gt; &lt;li&gt;image size: 256x256&lt;/li&gt; &lt;li&gt;batch size: 32&lt;/li&gt; &lt;li&gt;TLC 사용 (Improving image restoration by revisiting global information aggregation. arXiv preprint arXiv:2112.04491 (2021))&lt;/li&gt; &lt;/ul&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-table-1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;LayerNorm을 추가하면서 learning rate를 높일 수 있게되었고, GELU, CA로 성능이 높아졌다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-table-2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;GELU와 CA를 각각 SG, SCA로 바꾸었을 때 속도 약간 느려졌지만 성능이 좋아졌다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-table3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;block수는 36개까지 성능향상이 컸는데 72개부터는 성능향상보다 complexity가 더 커서 36으로 결정하였다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-table-4-5.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;TLC는 긍정적이었고 SG에서 non-linear function의 영향을 비교했지만 미미하여 Identity를 사용하였다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-table-6.png&quot; width=&quot;80%&quot; /&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-table-7.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Denosing과 Deblur 모두 SOTA를 달성하였다.&lt;/p&gt; &lt;h2 id=&quot;이미지비교&quot;&gt;이미지비교&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-fig6.png&quot; width=&quot;80%&quot; /&gt; &lt;img src=&quot;/assets/post/image/legacy/nafnet-fig7.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; </description> <pubDate>Sat, 18 Feb 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/simple-baselines-for-image-retoration/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/simple-baselines-for-image-retoration/</guid> <category>deblurring</category> <category>denosing</category> <category>paper</category> </item> <item> <title>Jetson nano Ubuntu 20.04 (우분투 20.04) 설치</title> <description>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/218313011-068d00e2-58fe-4cdf-bc2d-25496037be36.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Jetson nano로 프로젝트를 하다가 pytorch 1.10 이상이 필요해서 우분투 20.04 버전을 설치하는 법을 정리했다.&lt;/p&gt; &lt;h2 id=&quot;ubuntu-2004-이미지-다운로드&quot;&gt;Ubuntu 20.04 이미지 다운로드&lt;/h2&gt; &lt;p&gt;먼저 우분투를 설치하기 위해서는 이미지 파일이 필요하다. 하지만 nvidia에서 공식적으로는 18.04 버전만 지원해서 사람들이 만든 버전을 사용해야했다. 물론 nvidia에서는 미지원 버전을 사용하는 것에 대해서 발생하는 문제는 책임을 지지 않는다 했다.&lt;/p&gt; &lt;p&gt;공식버전(18.04)을 받고 싶으면 아래 링크를 통해 다운로드 받으면 된다.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/embedded/downloads&quot;&gt;https://developer.nvidia.com/embedded/downloads&lt;/a&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/218313311-e92e3bf9-32c8-4b90-bcc3-36df2a53e81e.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt; &lt;p&gt;우분투 20.04를 받고 싶으면 아래 링크에서 받으면 후 압축을 풀면 된다. 다운로드 속도 때문에 분리되어있는 압축파일을 다움받는 것을 추천한다.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Qengineering/Jetson-Nano-Ubuntu-20-image&quot;&gt;https://github.com/Qengineering/Jetson-Nano-Ubuntu-20-image&lt;/a&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/218314700-765e1f41-bca4-45f7-8841-4b106a20cafb.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt; &lt;h1 id=&quot;이미지-파일-설치&quot;&gt;이미지 파일 설치&lt;/h1&gt; &lt;p&gt;다음에는 보통 우분투를 설치하는 것 처럼 이미지를 설치하면 된다. 먼저 아래 링크에서 이미지 설치 파일을 다운로드 받자.&lt;/p&gt; &lt;p&gt;https://rufus.ie/ko/&lt;/p&gt; &lt;p&gt;그 후 위에서 받은 이미지 파일을 가지고 우분투를 설치하면 된다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/218315288-55a4e73f-5747-474a-ab20-4e4d36c40409.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt; &lt;p&gt;설치가 끝나고 sd카드를 삽입하면 다음과 같은 화면을 만날 수 있다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/218315593-2f44349e-e15e-4a9e-8f75-496f8f497f2d.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt; &lt;p&gt;비밀번호는 OS에는 다음과 같이 설치되어있다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Password: jetson&lt;/li&gt; &lt;li&gt;sha256sum: 492d6127d816e98fdb916f95f92d90e99ae4d4d7f98f58b0f5690003ce128b34&lt;/li&gt; &lt;li&gt;md5sum: f2181230622b81b6d882d4d696603e04&lt;/li&gt; &lt;/ul&gt; </description> <pubDate>Sun, 12 Feb 2023 09:50:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/jetson-nano-ubuntu/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/jetson-nano-ubuntu/</guid> </item> <item> <title>Bootstrap your own latent</title> <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.07733&quot;&gt;BYOL 논문 링크&lt;/a&gt;&lt;br /&gt; 세상에는 많은 데이터가 존재하지만 이를 annotation하는 것은 많은 비용이 발생한다. Self-supervised learning은 레이블이 지정되지 않은 데이터에서 feature들을 추출하기 위해 학습하는 방법이다. 고전적인 방법으로는 AutoEncoder, GAN이 있지만 contrastive learning이 제시되고 큰 성능향상을 가져왔다. 하지만 contrastive learning은 negative label의 수에 의존적이고 color distortion에 민감하다는 단점이 있다. 먼저 contrastive learning에 대해 살펴보고 이를 BYOL이 어떻게 해결하였는지 살펴보자.&lt;/p&gt; &lt;h1 id=&quot;contrastive-learning&quot;&gt;Contrastive Learning&lt;/h1&gt; &lt;p&gt;Contrastive learning은 self-supervised learning의 한 종류이다. 이미지에서 data augmentation을 통해 image patch를 추출하게 되는데 같은 이미지에서 나온 image patch는 representation vector space에서 distance가 작게, 다른 이미지에서 나온 image patch는 representation vector space에서 distance가 크게 만든다. 이를통해 이미지의 feature를 나타내는 representation vector를 만든다.&lt;/p&gt; &lt;h2 id=&quot;contrastive-learning의-단점&quot;&gt;Contrastive learning의 단점&lt;/h2&gt; &lt;h3 id=&quot;1-data-augmentation에-의존적&quot;&gt;1. Data augmentation에 의존적&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/210832216-783f8cc2-7dc4-43eb-a897-dd38c81be6d1.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;contrastive learning은 한 이미지 내에서 positive sample을 추출한다. 따라서 positive sample들은 비슷한 color histogram을 가지게 된다. 이를 그대로 contrastive learning에 사용할 경우 네트워크는 다른 유용한 정보 대신 color histogram만으로 positive sample과 negative sample을 구분하게 되어 모델의 성능이 낮아지게된다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/210832350-8a36329a-757a-429b-a8a0-8223d7bdbd80.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;위의 그림은 SimCLR의 data augmentation으로 data augmentation의 조합에도 성능이 많이 달라진다.&lt;/p&gt; &lt;h3 id=&quot;2-negative-sample의-수에-의존적&quot;&gt;2. Negative sample의 수에 의존적&lt;/h3&gt; \[\mathcal{L}_q=-log \frac{exp(q \cdot k_{+} / \tau)}{\sum_{i=0}^K exp(q \cdot k_i / \tau)}\] &lt;p&gt;위는 contrastive learning의 loss function인 InfoNCEdlek. 이 때 contrastive learning에서 positive sample만 사용하면 네트워크는 항상 같은 constant vector를 출력하여 &lt;strong&gt;model collapse&lt;/strong&gt;가 발생한다. 따라서 이를 방지하기 위해 negative sample과의 비교도 하여 model collapse를 방지한다.&lt;/p&gt; &lt;center&gt; $$\mathcal{L}_N^{opt}=-\underset{X}{\operatorname{\mathbb{E}}} log \left[ \frac{\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}}{\frac{p(x_{t+k}|c_t)}{p(x_{t+k})} + \sum_{x_j \in X_{neg}} \frac{p(x_{j}|c_t)}{p(x_{j})}} \right]$$ $$= \underset{X}{\operatorname{\mathbb{E}}} log \left[ 1 + \frac{p(x_{t+k})}{p(x_{t+k}|c_t)} \underset{x_j \in X_{neg}}{\operatorname{\sum}} \frac{p(x_{j}|c_t)}{p(x_{j})} \right]$$ $$\approx \underset{X}{\operatorname{\mathbb{E}}} log \left[ 1 + \frac{p(x_{t+k})}{p(x_{t+k}|c_t)} (N-1) \underset{x_j}{\operatorname{\mathbb{E}}} \frac{p(x_{j}|c_t)}{p(x_{j})} \right]$$ $$= \underset{X}{\operatorname{\mathbb{E}}} log \left[ 1 + \frac{p(x_{t+k})}{p(x_{t+k}|c_t)} (N-1) \right]$$ $$\geq \underset{X}{\operatorname{\mathbb{E}}} log \left[ \frac{p(x_{t+k})}{p(x_{t+k}|c_t)} N \right]$$ $$= -I(x_{t+k}, c_t) + log(N)$$ $$I(x_{t+k}, c_t) \geq log(N) - \mathcal{L}_N^{opt}$$ &lt;/center&gt; &lt;p&gt;이는 loss function에서도 볼 수 있다. InfoNCE가 optimal value가 되었을 때 전개해보면 다음과 같다. 맨 아래인 식을 해석하자면 infoNCE는 Mutual information의 lower bound를 제한한다. 이 때 negative sample의 수 또한 lower bound를 제한해 negative sample의 수가 많을수록 성능이 향상되는 것을 알 수 있다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/210817879-6b4e0611-c09a-4ac2-9135-e28470a8097b.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;SimCLR은 batch에 positive sample과 negative sample을 함께 구성하여 negative sample의 수를 확도한다. 이러한 특성으로인해 batch size가 줄어들면 성능이 급격하게 하락하는 것을 볼 수 있다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/210835642-92a9409c-6621-429e-b76e-6a9b0e0e3d11.png&quot; width=&quot;40%&quot; /&gt;&lt;/p&gt; &lt;p&gt;또한 MoCo는 momentum encoder를 사용하여 negative sample queue를 만들어 negative sample를 확보한다. 이러한 특성으로 bath size가 작아도 많은 negative sample을 확보할 수 있다.&lt;/p&gt; &lt;h1 id=&quot;byol&quot;&gt;BYOL&lt;/h1&gt; &lt;p&gt;이 모든 과정을 model collapse를 방지하기 위해 하는 것이다. BYOL의 저자는 이를 해결하기위해 그저 encoder를 random initialization하여 실험을 진행했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/210838495-14ca6edf-a6ba-4d7c-ac2d-d225a27824d0.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;Encoder를 random initialization한 후 parameter를 고정한다. 그리고 linear layer를 하나 붙여 ImageNet에 학습을 시킬 경우 정확도라 1.8%로 낮았다. 하지만 random initialization encoder의 representation vector를 label로 만들어 encoder를 학습시킨 후 위와같은 학습을 하면 정확도가 18.8%로 상당히 상승했다. 저자는 이에 아이디어를 얻어 실험을 진행하였다.&lt;/p&gt; &lt;h2 id=&quot;학습과정&quot;&gt;학습과정&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/210839632-74b56d21-8699-4f53-ab95-8fbdf45f5f41.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;구조가 동일한 online network와 target network를 만든다.&lt;/li&gt; &lt;li&gt;Target network의 역할은 online network가 예측할 representation vector \(f_\mathcal{E}\)를 만든다.&lt;/li&gt; &lt;li&gt;Online network의 역할은 \(f_\mathcal{E}\)를 예측하는 것이다.&lt;/li&gt; &lt;li&gt;Representation vector를 그대로 사용하지 않고 projection layer \(g\)를 사용하여 projection vector \(z\)로 변환한다. (SimCLR 참고)&lt;/li&gt; &lt;li&gt;\(z&apos;_\mathcal{E}\)을 예측하기위해 Online network에 prediction layer \(q_{\theta}\)를 추가하여 asymmetric한 구조를 만들었다.&lt;/li&gt; &lt;li&gt;Data augmentation pool \(\mathcal{T}\) 을 만들고 \(t\), \(t&apos;\)를 뽑아 각각의 네트워크에 통과시킨다.&lt;/li&gt; &lt;/ol&gt; &lt;center&gt; $$\mathcal{L}_{\theta, \mathcal{E}} \triangleq \| \overline{q_{\theta}(z_{\theta})} - \overline{z&apos;_\mathcal{E}} \|^2_2 = 2 - 2 \cdot \frac{\langle q_{\theta}(z_{\theta}), z&apos;_\mathcal{E} \rangle}{\| q_{\theta}(z_{\theta}) \|_2 \cdot \| z&apos;_\mathcal{E} \|_2}$$ $$\mathcal{L}^{BYOL}_{\theta, \mathcal{E}} = \mathcal{L}_{\theta, \mathcal{E}} + \tilde{\mathcal{L}}_{\theta, \mathcal{E}}$$ $$\theta \leftarrow optimizer(\theta, \nabla_{\theta} \mathcal{L}^{BYOL}_{\theta, \mathcal{E}}, \eta)$$ $$\mathcal{E} \leftarrow \tau \mathcal{E} + (1 - \tau) \theta$$ &lt;/center&gt; &lt;ol&gt; &lt;li&gt;online network의 output \(q_{\theta}(z_{\theta})\)와 \(z&apos;_\mathcal{E}\)를 L2 normalize하여 \(\overline{q_{\theta}(z_{\theta})}\)와 \(\overline{z&apos;_\mathcal{E}}\)를 만든다.&lt;/li&gt; &lt;li&gt;\(\overline{q_{\theta}(z_{\theta})}\)와 \(\overline{z&apos;_\mathcal{E}}\)의 L2 loss를 구한다.&lt;/li&gt; &lt;li&gt;Symmetric한 구조를 만들기 위해 \(t\), \(t&apos;\)를 뒤집어 다시 loss를 구한다.&lt;/li&gt; &lt;li&gt;Online network의 parameter는 backpropatation을 통해 update한다.&lt;/li&gt; &lt;li&gt;Target network의 parameter는 online network의 parameter의 exponential moving average(EMA)을 사용하여 update한다.&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;why-it-works&quot;&gt;Why it works?&lt;/h2&gt; &lt;p&gt;단순히 보기만해도 구조와 학습방법이 이상하고 왜 작동하는지 모르겠다. 저자는 “BYOL 의 prediction layer가 optimal 인 경우에는 undesirable equilibria 는 unstable 하다.”라고 한다. 즉, model collapse가 발생하기 어렵다는 것이고 대한 이유로 prediction layer와 EMA를 뽑았다.&lt;/p&gt; &lt;h3 id=&quot;prediction-layer&quot;&gt;Prediction layer&lt;/h3&gt; &lt;p&gt;만약 prediction layer가 optimal solution이 \(p^{\star}\)라고 하자.&lt;/p&gt; &lt;center&gt; $$p^{\star} \triangleq \underset{q}{\operatorname{argmin}} \mathbb{E} \left [\| q_{\theta}(z_{\theta}) - z&apos;_\mathcal{E} \|^2_2 \right ], p^{\star}(z_{\theta})=\mathbb{E}\left [z&apos;_\mathcal{E} | z_{\theta}\right ]$$ &lt;/center&gt; &lt;p&gt;다음 parameter update는 다음과 같다.&lt;/p&gt; &lt;center&gt; $$ \nabla_{\theta} \mathbb{E} \left [\| q^{\star}(z_{\theta}) - z&apos;_{\mathcal{E}} \|^2_2 \right ] = \nabla_{\theta} \mathbb{E} \left [\| \mathbb{E}\left [z&apos;_\mathcal{E} | z_{\theta}\right ] - z&apos;_{\mathcal{E}} \|^2_2 \right ] = \nabla_{\theta} \mathbb{E} \left [\sum_i Var(z&apos;_{\mathcal{E},i}|z_{\theta}) \right ] $$ &lt;/center&gt; &lt;p&gt;만약 online network가 representation을 constant값을 출력한다면 다음과 같은 식이 성립한다.&lt;/p&gt; &lt;center&gt; $$Var(z&apos;_{\mathcal{E}}|z_{\theta}) \leq Var(z&apos;_{\mathcal{E}}|c)$$ &lt;/center&gt; &lt;p&gt;이 때 constant representation vector보다 작은 representation vector가 존재하므로 다음 parameter update가 일어난다.&lt;/p&gt; &lt;h3 id=&quot;ema&quot;&gt;EMA&lt;/h3&gt; &lt;p&gt;Target network를 update할 때 EMA를 사용하지 않고 gradient decent로 update하면 model collapse가 발생한다.&lt;/p&gt; &lt;center&gt; $$Var(c|z_{\theta}) \leq Var(z&apos;_{\mathcal{E},i}|z_{\theta})$$ &lt;/center&gt; &lt;p&gt;위의 식과 같은이유로 model collapse가 발생하면 parameter update가 일어나지 않는다.&lt;/p&gt; &lt;h1 id=&quot;experimental-result&quot;&gt;Experimental Result&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/210858972-fb42de13-7492-47db-9c0a-35f32a92f7df.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Backbone으로 ResNet50을 사용하였다. ImageNet에서 linear evaluation에서 성능이 좋았고 다른 ResNet backbone에서도 성능이 좋았다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/210858995-9d9462c7-1138-4d97-a4c2-c08706bf72e9.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;전체 dataset의 일부만 사용하는 semi-supervised learning에서도 같은 결과를 보였다&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/210859023-4d8d3107-8c13-4e64-adfe-8d817e6ba5c2.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;ImageNet에만 특화되어있는지 알기위해 다른 classification task에 적용을 했을 떄도 대부분의 경우에 성능이 좋았다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/210859042-ac7fb9e5-e936-4d5f-aba6-1156e8dac428.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Image classification뿐만아니라 다른 vision task에서도 성능이 좋았다.&lt;/p&gt; &lt;h1 id=&quot;ablation-study&quot;&gt;Ablation study&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/210859066-567d936c-59af-4db3-af61-f329564f417d.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Batch size, data augmentation의 영향을 받으나 이는 SimCLR보다 더 적은 것으로 나타난다. BYOL은 positive sample만 사용하기 때문에 batch size에 둔감하다는 것이라 추정한다.&lt;/p&gt; &lt;h1 id=&quot;결론&quot;&gt;결론&lt;/h1&gt; &lt;p&gt;BYOL은 self-supervised representation learning에서 SOTA를 달성했다. 이것이 왜 작동되는가는 아직도 많은 논쟁이 있지만 필자는 target network에서 EMA를 사용한 것이 negative sample을 대체했다고 생각한다. 이에대한 자세한 의견은 정리되지 않았으므로 추후에 이야기하겠다.&lt;/p&gt; </description> <pubDate>Thu, 05 Jan 2023 09:50:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/byol/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/byol/</guid> <category>self-supervised-learning</category> <category>paper</category> </item> <item> <title>Quantization과 inference speed</title> <description>&lt;p&gt;Quantization은 precision reduction으로 parameter의 용량을 줄이기위해 나왔다. 하지만 실제로 써봤을 떄 유의미한 속도차이가 있었다. 왜 그런 것일까 궁금해서 몇 까지 측정을했다.&lt;br /&gt; 먼저 3가지 모델을 준비했다. 일반 cpu에서의 MobileNetV2, quatization을 진행한 MobilNetV2, layer fusion과 qutization을 진행한 mobileNetV2이다. 각각 모델에게 image를 5000씩 inference하도록하고 time elapese와 cache miss, intruction per cycle을 비교했다.&lt;br /&gt; &lt;br /&gt; 각각 코드는 다음과 같다.&lt;/p&gt; &lt;h3 id=&quot;pytorch_cpu&quot;&gt;pytorch_cpu&lt;/h3&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision.models.quantization.mobilenetv2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mobilenet_v2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mobilenet_v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_default_qconfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;fbgemm&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# net.fuse_model() &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;prepare_qat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;quantization-with-non-layer-fusion&quot;&gt;quantization with non layer fusion&lt;/h3&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision.models.quantization.mobilenetv2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mobilenet_v2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mobilenet_v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_default_qconfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;fbgemm&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# net.fuse_model() &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;prepare_qat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;quantization-with-payer-fusoin&quot;&gt;quantization with payer fusoin&lt;/h3&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision.models.quantization.mobilenetv2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mobilenet_v2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mobilenet_v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_default_qconfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;fbgemm&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fuse_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;prepare_qat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt; &lt;p&gt;결과는 ubuntu에 perf을 통해 측정했다.&lt;/p&gt; &lt;table align=&quot;center&quot;&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;time elapsed (sec)&lt;/td&gt; &lt;td&gt;cache miss (%)&lt;/td&gt; &lt;td&gt;instruction per cycle&lt;/td&gt; &lt;td&gt;Model Size (MB)&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;MobileNetV2&lt;/td&gt; &lt;td&gt;89.8063&lt;/td&gt; &lt;td&gt;20.774&lt;/td&gt; &lt;td&gt;0.58&lt;/td&gt; &lt;td&gt;14.2605&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;+ quantization&lt;/td&gt; &lt;td&gt;54.4425&lt;/td&gt; &lt;td&gt;5.987&lt;/td&gt; &lt;td&gt;0.68&lt;/td&gt; &lt;td&gt;4.2422&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;+ layer fusion&lt;/td&gt; &lt;td&gt;31.1885&lt;/td&gt; &lt;td&gt;5.004&lt;/td&gt; &lt;td&gt;0.97&lt;/td&gt; &lt;td&gt;3.9436&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;이건 순전히 필자의 추측이다&lt;/strong&gt;&lt;br /&gt; vanilla MobileNetV2와 quantization MobileNetV2을 보면 instruction per cycle 차이보다 cache miss 가 더 유의미하다. (같은 instruction per cycle 에서 MobileNetV2의 time elapsed 는 105.2901이다. 따라서 precision reduction 으로 parameter 가 용량이 적어져 cache miss 가 적어진 것과 parameter data transfer latency 거 적어진 것으로 볼 수 있다. (혹시 아니면 메일을 주면 감사합니다.) 이후 layer fusion 을 통해 time elapsed 가 줄어들었다. 이떄는 cache miss 가 높게 줄어들지 않았으므로 단순히 graph reduction 에 따른 성능향상으로 볼 수 있을 것이다. instruction per cycle 에서 봤을 떄도 낮은 연산떄문에 instruction per cycle 이 높아졌다. 그리고 context switching 도 줄어들었다.&lt;/p&gt; &lt;center&gt; $$ \frac 1 n \sum (x_i - \bar x)(y_i - \bar y) $$ &lt;/center&gt; </description> <pubDate>Wed, 13 Jul 2022 09:50:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2022/quantization-analysis/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2022/quantization-analysis/</guid> <category>pytorch</category> <category>hardware-optimization</category> </item> <item> <title>Pytorch Tensorrt 적용</title> <description>&lt;p&gt;TensorRT는 Deep Learning 모델을 최적화해 GPU에서 inference 속도를 향상시킬 수 있는 최적화 엔진이다. TensorRT는 GPU에서 최적화된 성능을 낼 수 있도록 Network Compression, Netword Optimization을 진행한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://blogs.nvidia.co.kr/2020/02/19/nvidia-tensor-rt/219-%eb%b8%94%eb%a1%9c%ea%b7%b83/&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;quantization--precision-calibration&quot;&gt;Quantization &amp;amp; Precision Calibration&lt;/h3&gt; &lt;p&gt;quantization을 통한 precision reduction은 network의 파라미터의 bit가 작기 떄문에 더 좋은 성능을 발휘할 수 있다. TensorRT는 Symmetric Linear Quantization을 사용하고 있으며, float32 데이터를 float16, int8로 낮출 수 있다. 하지만 int8로 precision을 낮추면 숫자표현이 급격히 줄어들어 성능에서 문제가 생긴다. 따라서 TensorRT는 callibration을 통해 weight과 intermidiate tensor에서의 정보손실을 최소화한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://blogs.nvidia.co.kr/wp-content/uploads/sites/16/2020/02/Figure-5.-Calibration-methodology.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;graph-optimization&quot;&gt;Graph Optimization&lt;/h3&gt; &lt;p&gt;TensorRT는 또한 platform에 최적화된 graph를 위해 Layer Fusion 방식과 Tensor Fusion을 사용한다. 따라서 Vertical Layer Fusion, Horizontal Layer Fusion, Tensor Fusion이 적용되어 graph를 단순하게 만들어 속도를 높힌다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://blogs.nvidia.co.kr/wp-content/uploads/sites/16/2020/02/219-%EB%B8%94%EB%A1%9C%EA%B7%B8-6.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;etc&quot;&gt;Etc&lt;/h3&gt; &lt;p&gt;Kernel Auto-tuning을 통해 GPU의 cuda core 수, 아키텍쳐 등을 고려하여 optimization을 진행하고, Dynamic Tensor Memory &amp;amp; Multi-stream execution을 통해 footprint를 줄여 성능을 높힌다.&lt;/p&gt; &lt;h2 id=&quot;tensorrt설치&quot;&gt;TensorRT설치&lt;/h2&gt; &lt;p&gt;필자 Docker를 사용하므로 Docker를 기준으로 설명하겠다.&lt;/p&gt; &lt;h2 id=&quot;1-도커-설치&quot;&gt;1. 도커 설치&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;https://docs.docker.com/engine/install/ubuntu/&quot;&gt;Ubuntu&lt;/a&gt;, &lt;a href=&quot;https://docs.docker.com/desktop/windows/install/&quot;&gt;Window(WSL2)&lt;/a&gt;, &lt;br /&gt; 이렇게가 설치방법인데 Window는 미리 WSL2를 이용해 Nvidia-Driver를 설치해야한다. &lt;del&gt;(해봤는데 정말 귀찮다)&lt;/del&gt; 그리고 MacOS는 Nvidia GPU를 사용할 수 없으니 사실상 사용을 못한다.&lt;/p&gt; &lt;h2 id=&quot;2-docker-container만들기&quot;&gt;2. Docker container만들기&lt;/h2&gt; &lt;p&gt;먼저 nvidia-smi를 사용하여 cuda version을 확인한다.&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nvidia-smi &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/178482155-6a312bd8-4028-4173-9e19-9cdacb2da2f2.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;위의 사진을 보고 자신의 cuda toolkit version에 맞는 컨테이너 버전을 사용하자. 추가적인 cuda version은 &lt;a href=&quot;https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/rel_22-06.html#rel_22-06&quot;&gt;TensorRT Container Release Notes&lt;/a&gt; 이 링크를 이용하면 된다.&lt;br /&gt; 그 다음 container버전과 python버전을 채워넣어 docker image를 pull하면 된다.&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull nvcr.io/nvidia/tensorrt:&amp;lt;xx.xx&amp;gt;-py&amp;lt;x&amp;gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;예 container version 22.06, python3 사용&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull nvcr.io/nvidia/tensorrt:22.06-py3 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;그 후 container를 만들어 run하면 된다.&lt;br /&gt; &lt;strong&gt;Docker 19.03 또는 그 이후 버전&lt;/strong&gt;을 사용하면&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run &lt;span class=&quot;nt&quot;&gt;--gpus&lt;/span&gt; all &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; tensorrt nvcr.io/nvidia/tensorrt:&amp;lt;xx.xx&amp;gt;-py&amp;lt;x&amp;gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Docker 19.02 또는 그 이전 버전&lt;/strong&gt;을 사용하면&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nvidia-docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; tensorrt nvcr.io/nvidia/tensorrt:&amp;lt;xx.xx&amp;gt;-py&amp;lt;x&amp;gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;을 사용하면 된다.&lt;/p&gt; &lt;h2 id=&quot;3-pytorch-tensorrt설치&quot;&gt;3. Pytorch TensorRT설치&lt;/h2&gt; &lt;p&gt;Torch-TensorRT를 사용하려면 3가지 방법이 있다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;torch_tensorrt &lt;a href=&quot;https://github.com/pytorch/TensorRT&quot;&gt;docker image&lt;/a&gt; 사용&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://pytorch.org/TensorRT/tutorials/installation.html#installation&quot;&gt;torch_tensorrt&lt;/a&gt; 패키지 사용&lt;/li&gt; &lt;li&gt;nvidia iot &lt;a href=&quot;https://github.com/NVIDIA-AI-IOT/torch2trt&quot;&gt;torch2trt&lt;/a&gt; 사용&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;필자는 1번 방법에서는 pytorch버전 오류, 3번에서는 성능최적화가 잘 안돼 2번을 선택했다.&lt;br /&gt; torch_tensorrt는 현재 v1.1.0로 pytorch v1.11.0밖에 지원이 안 된다. v1.11.0으로 설치하자&lt;br /&gt; CUDA 11.3&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;1.11.0+cu113 &lt;span class=&quot;nv&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;0.12.0+cu113 &lt;span class=&quot;nv&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;0.11.0 &lt;span class=&quot;nt&quot;&gt;--extra-index-url&lt;/span&gt; https://download.pytorch.org/whl/cu113 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;CUDA 10.2&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;1.11.0+cu102 &lt;span class=&quot;nv&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;0.12.0+cu102 &lt;span class=&quot;nv&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;0.11.0 &lt;span class=&quot;nt&quot;&gt;--extra-index-url&lt;/span&gt; https://download.pytorch.org/whl/cu102 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;이후 torch-tensorrt를 설치하자&lt;/p&gt; &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;torch-tensorrt &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://github.com/pytorch/TensorRT/releases &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h2 id=&quot;4-tensorrt적용&quot;&gt;4. TensorRT적용&lt;/h2&gt; &lt;p&gt;현재 pytorch는 torch script -&amp;gt; onnx -&amp;gt; tensorrt이렇게 변환한다. 먼제 model을 선언해주자.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision.models.mobilenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mobilenet_v2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch_tensorrt&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mobilenet_v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch_script_module&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trt_ts_module&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch_tensorrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch_script_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch_tensorrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Specify input object with shape and dtype &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;112&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;112&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;448&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;448&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# For static size shape=[1, 3, 224, 224] &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Datatype of input tensor. Allowed options torch.(float|half|int32|bool) &lt;/span&gt; &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;enabled_precisions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Run with FP16 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trt_ts_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;trt_torchscript_module.ts&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# save the TRT embedded Torchscript &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;성능비교를 해보자&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_model_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tmp.pth&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getsize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tmp.pth&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e6&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tmp.pth&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_size&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float32_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float16_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# run inference &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Pytorch time cost: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ms, Mode Size: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_model_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;MB&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;trt_ts_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float16_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# run inference &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;TensorRT time cost: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ms, Model Size: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_model_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trt_ts_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;MB&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt; &lt;table align=&quot;center&quot;&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;Inference Time (ms)&lt;/td&gt; &lt;td&gt;Model Parameter (MB)&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;MobileNetV2&lt;/td&gt; &lt;td&gt;6.1163&lt;/td&gt; &lt;td&gt;14.2630&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;TensorRT&lt;/td&gt; &lt;td&gt;0.3954&lt;/td&gt; &lt;td&gt;0.0005&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;이렇듯 TensorRT를 이용하면 inference time이나 parameter size에서 이득을 볼 수 있다.&lt;/p&gt; </description> <pubDate>Tue, 12 Jul 2022 09:50:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2022/pytorch-tensorrt/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2022/pytorch-tensorrt/</guid> <category>pytorch</category> <category>hardware-optimization</category> </item> <item> <title>Pytorch Quantization 적용</title> <description>&lt;p&gt;딥러닝 모델이 실제 device에 deploy 하는데 2가지 문제점이 있다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;느린 inferfence time&lt;/li&gt; &lt;li&gt;큰 model parameter size&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Pytorch는 float32에서 int8로 데이터 크기를 줄여 연산을 하는 Quantization을 제공한다. 직접 짠 모델에서 quantization을 어떻게 적용하는지 알아보자.&lt;br /&gt; 전체코드는 이 &lt;a href=&quot;https://github.com/wonbeomjang/blog-code/blob/main/resnet-quantization.py&quot;&gt;링크&lt;/a&gt;에 있다.&lt;/p&gt; &lt;h2 id=&quot;work-flow&quot;&gt;Work Flow&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;float32에서 학습시킨 model 혹은 pretrain model을 가져온다.&lt;/li&gt; &lt;li&gt;model을 eveluation으로 변경 후 layer fusion을 적용한다. (Conv + BN + RELU)&lt;/li&gt; &lt;li&gt;forward의 input엔 torch.quantization.QuantStub(), output엔 torch.quantization.DeQuantStub()을 적용한다.&lt;/li&gt; &lt;li&gt;quantization configuration을 적용한다.&lt;/li&gt; &lt;li&gt;layer를 fuse한다.&lt;/li&gt; &lt;li&gt;QAT (quantized aware training) 을 진행한다.&lt;/li&gt; &lt;li&gt;모델을 cpu에 올려놓고 eval mode로 바꾼 후 float32모델을 int8모델로 변환시킨다.&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt; &lt;h3 id=&quot;1-declear-model&quot;&gt;1. Declear Model&lt;/h3&gt; &lt;p&gt;모델은 resnet 사용하기로 한다. 그리고 편의를 위해 학습은 미리 시켰다고 가정한다.&lt;br /&gt; 먼저 resnet의 BottleNeck을 선언하고 resnet18을 구현한다.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.quantization&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fuse_modules&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;BottleNeck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BottleNeck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1x1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;layer1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;layer2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;conv1x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ResNet18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottle_neck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BottleNeck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ResNet18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxpool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;bottle_neck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bottle_neck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;bottle_neck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bottle_neck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;bottle_neck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bottle_neck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;bottle_neck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bottle_neck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avgpool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avgpool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;AdaptiveAvgPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_forward_impl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# See note [TorchScript super()] &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;bn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;maxpool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;layer1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;layer2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;layer3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;layer4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;avgpool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;_forward_impl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;23-deploy-layer-fusion&quot;&gt;2,3. Deploy Layer Fusion&lt;/h3&gt; &lt;p&gt;이후 각 모듈에 layer fusion을 적용한다. layer를 건드리지 않고 상속을 쓰면 결과적으로 parameter가 같기 때문에 QuantizableResNet18은 ResNet18의 파라미터를 쓸 수 있다.&lt;br /&gt; QuantizableBottleNeck에서는 두 tensor를 더하는 연산이 있으므로 기존의 방식이 아닌 FloatFunctional을 이용하야한다.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.nn.quantized&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FloatFunctional&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;QuantizableBottleNeck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BottleNeck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QuantizableBottleNeck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float_functional&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;FloatFunctional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fuse_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fuse_modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fuse_modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;layer1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;layer2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;conv1x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float_functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;QuantizableResNet18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ResNet18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QuantizableResNet18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QuantizableBottleNeck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quant&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;QuantStub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dequant&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DeQuantStub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;quant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;_forward_impl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dequant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fuse_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fuse_modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QuantizableBottleNeck&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fuse_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;45-apply-quantize-configuration-and-fuse-layer&quot;&gt;4,5. Apply Quantize Configuration and Fuse Layer&lt;/h3&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# torch.load resnet18 parameter.... &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_default_qconfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;fbgemm&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fuse_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;67-qat-convert-int8-model&quot;&gt;6,7. QAT, Convert int8 model&lt;/h3&gt; &lt;p&gt;QAT를 진행한 다음에 float32모델을 int8모델로 변환시킨다.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;prepare_qat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt; &lt;p&gt;간단하게 MNIST dataset으로 학습시켰다. epoch 5, image size 224, optimzer Adam을 사용하였다.&lt;/p&gt; &lt;table align=&quot;center&quot;&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;Accuracy (%)&lt;/td&gt; &lt;td&gt;Inference Time (ms)&lt;/td&gt; &lt;td&gt;Model Parameter (MB)&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;ResNet&lt;/td&gt; &lt;td&gt;98.79&lt;/td&gt; &lt;td&gt;66&lt;/td&gt; &lt;td&gt;46.31&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;Quantizable ResNet&lt;/td&gt; &lt;td&gt;96.42&lt;/td&gt; &lt;td&gt;33&lt;/td&gt; &lt;td&gt;11.69&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; </description> <pubDate>Mon, 11 Jul 2022 09:50:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2022/quantization-pytorch/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2022/quantization-pytorch/</guid> <category>pytorch</category> <category>hardware-optimization</category> </item> <item> <title>FitNet</title> <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.6550&quot;&gt;fitnet 논문 링크&lt;/a&gt; &lt;br /&gt; 많이 쓰이는 딥러닝 모델은 inference time에 많은 시간이 소요된다. 그리고 파라미터 수가 많아서 많은 메모리도 필요하다. 이러한 이유로 Knowledge Distillation을 사용한다. 하지만 이전의 연구들은 더 얕은 네트워크에 적용하지 않아 속도면에서 아쉬운 점이 있었다. 따라서 이 논문에서 더 얕은 네트워크를 사용하여 compression하는 방법을 제공한다.&lt;/p&gt; &lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt; &lt;h3 id=&quot;review-of-knowledge-distillation&quot;&gt;Review of Knowledge Distillation&lt;/h3&gt; &lt;p&gt;이전 연구(Hinton &amp;amp; din, 2014)에선 student network가 학습할 때 제공된 label뿐만 아니라 teacher network의 output까지 학습하게 한다. \(P_T\)는 teacher의 output, \(P_S\)는 student의 output이라 하자. 또한 \(P_T\)는 true label과 유사하기 떄문에 τ를 사용하여 soften시킨다.&lt;/p&gt; &lt;center&gt; $$P^{\tau}_T=softmax(a_T/\tau), P^{\tau}_S=softmax(a_S/\tau)$$ &lt;/center&gt; &lt;p&gt;student network는 다음을 최적화하는 것이 목표이다.&lt;/p&gt; &lt;center&gt; $$L_{KD}(W_S)=H(y_{true}, P_S) + \lambda H(P^{\tau}_T, P^{\tau}_S)$$ &lt;/center&gt; &lt;p&gt;H는 cross entropy이고, λ는 두 cross entropy의 균형을 맞추는 hyper parameter이다.&lt;/p&gt; &lt;h3 id=&quot;hint-based-training&quot;&gt;Hint based Training&lt;/h3&gt; &lt;p&gt;저자는 DNN을 학습시키기 위해 hint와 guide layer라는 것을 도입했다. hint는 student의 학습을 도와주기 위한 teacher의 hidden layer이다. 또한 guide layer는 teacher의 hint layer로부터 배우는 student의 hidden layer이다. 저자는 guide layer가 teacher의 hint layer를 학습하도록 목표를 잡았다. 이때 hint layer와 guide layer는 teacher와 studnet의 middle layer로 설정했다. 그리고 guide layer는 hint와 차원이 맞지 않기 떄문에 regression layer를 추가했다.&lt;/p&gt; &lt;center&gt; $$L_{HT}(W_{Guided}, W_r) = 1/2||u_h(x;W_{Hint}) - r(v_g(x;W_{Guided}); W_r)||^2$$ &lt;/center&gt; &lt;p&gt;\(u_h, v_g\)는 각각teacher와 student의 nested funsiton이고 \(W_{Hint}, W_{Guided}\)는 teacher와 student의 parameter이다.&lt;/p&gt; &lt;p&gt;regression layer를 fully connected layer로 설정할 수 있지만 파라미터수가 많아지므로 cnn layer를 사용하여 \(N_{h,1} \times N_{h,2} \times O_{h} \times N_{g,1} \times N_{g,2} \times O_{g}\) 에서 \(k_1 \times k_2 \times O_{h} \times O_{g}\)로 줄일 수 있었다.&lt;/p&gt; &lt;h3 id=&quot;training-method&quot;&gt;Training Method&lt;/h3&gt; &lt;p&gt;FitNet(논문에서 제안한 방법으로 학습된 네트워크)은 teacher가 student를 가르치는 방법으로 다음과 같이 직관적인 학습과정을 거친다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/167399813-49155f46-ad13-47ea-baca-3f4ddcfa7f49.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;학습된 teacher network와 random initialized된 student network를 준비한다.&lt;/li&gt; &lt;li&gt;hint와 guide layer를 가지고 regressor를 학습시킨다.&lt;/li&gt; &lt;li&gt;hint와 regressor를 사용해 guide를 학습시킨다. 이 때 studnet의 학습이 일어난다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;이에대한 알고리즘은 다음과 같다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/167399823-c6670e51-a34b-43a6-835c-81ffddb5bda5.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt; &lt;p&gt;결과는 다음과 같다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/167399991-3448ef8f-7f06-4229-9aba-47198f22a660.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;느낀점&quot;&gt;느낀점&lt;/h3&gt; &lt;p&gt;feature map base로 학습시킨다는 의도는 좋았다. 많은 논문이 이를 따른다는 것에 의미가 있다. 하지만 regressor를 따로 학습시킨다는 것에 의문이 든다. regressor를 학습시킬떄는 teacher는 의미있는 representation이 있지만 student는 의미있는 representation을 가지고 있지 않다. 따라서 각각의 representation space의 변환이 적절하게 되었는지는 의문이다.&lt;/p&gt; </description> <pubDate>Mon, 09 May 2022 09:50:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2022/fitnet/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2022/fitnet/</guid> <category>knowledge-distillation</category> <category>paper</category> </item> <item> <title>[OSAM] 3. 이제 끝나는 건가?</title> <description>&lt;p&gt;해커톤이 끝나간다. 그런데 프론트는 아직 안 끝났고 결과물 제출 일주일 전에 갑자기 인스타그램 연동, 지속적인 학습 및 모델 업데이트, 로그를 넣자고 한다. 나는 반대했지만 우리와 비슷한 아이디어를 가진 다른팀이 있어 다수의 의견대로 진행하기로 했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/86545225/137576790-1e7b5459-fdbd-4cc8-9e3b-d27a3bd3b1b4.jpg&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;그렇게 로그를 추가하고…&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/141277496-ceffd608-0249-485a-8cd0-3118a97544bd.PNG&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;인스타봇을 추가하고…&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/137886632-edd9ca08-831e-4b29-97da-62b6bae0982b.PNG&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;대충 이런 REST API서버를 장고로 만들고…&lt;/p&gt; &lt;p&gt;이제 진짜 끝났다.&lt;/p&gt; &lt;h2 id=&quot;이제-진짜-끝&quot;&gt;이제 진짜 끝&lt;/h2&gt; &lt;table&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/86545225/137917096-372ec2f3-60ab-4e49-ab98-cb87ca96aa88.PNG&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;td&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/86545225/137917134-a9d63375-3663-467a-8ea3-2d5a92950085.PNG&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;td&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/86545225/137917734-1f88e1c0-5f2f-4f2e-a7f5-d3ddb3019b81.png&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/86545225/137917171-afe0567c-4cc5-4bf7-84dd-862c1cec4819.PNG&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;td&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/86545225/137919288-c90a06c7-c843-407f-ba5e-aed914cf3fd5.PNG&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;td&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/86545225/137919350-567523d8-255e-466a-a834-12014eeb4679.PNG&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/86545225/137919337-f2109767-9daa-427d-85f7-2dad831202db.png&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;td&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/86545225/137919583-8a2fd884-c0c3-4bfb-8099-aeb03b7ce081.png&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;td&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/86545225/137919328-6390d7ea-207c-49c9-a0b8-97c4eab44d47.PNG&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;어플 완성!!!!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;제출 2일전에 드디어 완성시키고 그 다음 문서를 작성했다. 주최측에서 &lt;a href=&quot;https://github.com/osamhack2021/AI_APP_WEB_Canary_Canary&quot;&gt;레포지토리&lt;/a&gt; &lt;a href=&quot;https://github.com/osamhack2021/AI_APP_WEB_Canary_Canary&quot;&gt;README&lt;/a&gt;로 개발문서를 만들라고 해서 README에 문서를 떄려 박았다.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/osamhack2021/AI_APP_WEB_Canary_Canary&quot;&gt;github readme&lt;/a&gt;, &lt;a href=&quot;https://docs.google.com/presentation/d/1s4Sa52awVV3G2vbk3Qd4I2jI2PlZm4a3-0RNrLDudvI/edit?usp=sharing&quot;&gt;ppt&lt;/a&gt;&lt;/p&gt; &lt;h2 id=&quot;결과&quot;&gt;결과&lt;/h2&gt; &lt;p&gt;11월 8일 최종결과 발표였는데 국방부 승인이 나지 않아서 11월 11일에 나왔다. 결과는 두구둑두구….&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/141281402-ad7c28a5-3032-4564-a7f8-e7cf00cb7fa0.PNG&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;해군참모총장상을 받았다. 8등 정도 한 것 같았는데 아쉬웠지만 다들 프로젝트를 처음한다고 하니까 다행이라 생각한다. 5주 동안 고생한 나에게 박수를!!!!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.wonbeomjang.kr/blog/2021/osam-1/&quot;&gt;[OSAM] 1. 팀 결정 및 주제&amp;amp;시스템 설계&lt;/a&gt;&lt;br /&gt; &lt;a href=&quot;https://www.wonbeomjang.kr/blog/2021/osam-2/&quot;&gt;[OSAM] 2. computer vision 개발 과정&lt;/a&gt;&lt;br /&gt; &lt;a href=&quot;https://www.wonbeomjang.kr/blog/2021/osam-3/&quot;&gt;[OSAM] 3. 이제 끝나는 건가?&lt;/a&gt;&lt;/p&gt; </description> <pubDate>Thu, 11 Nov 2021 09:50:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2021/osam-3/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2021/osam-3/</guid> <category>OSAM</category> </item> <item> <title>[OSAM] 2. computer vision 개발 과정</title> <description>&lt;p&gt;이 포스트는 이전 포스트를 읽으면 이해하기 더 쉽다.&lt;br /&gt; &lt;a href=&quot;https://wonbeomjang.github.io/2021/10/29/osam-1/&quot;&gt;[OSAM] 1. 팀 결정 및 주제&amp;amp;시스템 설계&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Object Detection 모델을 선택하는데 몇 가지 기준을 세웠다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;model train과 evaluation이 빨라야 한다.&lt;/li&gt; &lt;li&gt;memory를 적게 잡아먹어야 한다.&lt;/li&gt; &lt;li&gt;성능이 나쁘지 않아야 한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;그렇게 SSD, SSDLite, EfficientNet, YOLOv5가 후보에 올랐다. EfficientNet은 학습시간이 너무 오래걸렸고, SSD는 성능이 매우 낮아 YOLOv5를 선택했다.&lt;/p&gt; &lt;h2 id=&quot;dataset준비&quot;&gt;Dataset준비&lt;/h2&gt; &lt;p&gt;우리가 만들 모델은 군사 시설, 장비, 용품 등을 인식하는 문제였다. 데이테셋을 찾아보려고 해도 우리가 원하는 데이터를 찾기 어려웠다. 하지만 다행이 kaggle에 &lt;a href=&quot;https://www.kaggle.com/c/imagenet-object-localization-challenge&quot;&gt;ImageNet Object Localization Challenge&lt;/a&gt;가 있었고, 이 데이터 + 직접 크롤링한 데이터를 이용하기로 했다.&lt;/p&gt; &lt;h2 id=&quot;모델-학습&quot;&gt;모델 학습&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://miro.medium.com/max/568/1*dXqFj2sY7zWXddWdKPuQng.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;일단 다들 알다싶이 precision과 recall은 trade off관계에 있다. 그리고 앞에서 말했 듯 우리가 만들고자 하는 것은 군사보안에 관한 것이다. 물체가 잘못 인식되어서 모자이크가 잘못 쳐지는 것 보다 군관련 사항을 모자이크를 못 하는 것이 치명적인 오류이다. 따라서 평가지표를 recall로 잡고 개발을 진행하기로 했다. 먼저 vanilla yolov5를 이용하여 학습을 진행했고 다음과 같은 결과가 나왔다.&lt;/p&gt; &lt;table align=&quot;center&quot;&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;enhance&lt;/td&gt; &lt;td&gt;model&lt;/td&gt; &lt;td&gt;precision&lt;/td&gt; &lt;td&gt;recall&lt;/td&gt; &lt;td&gt;mAP_0.5&lt;/td&gt; &lt;td&gt;mAP_0.5:0.95&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;Vanilla&lt;/td&gt; &lt;td&gt;yolov5m6&lt;/td&gt; &lt;td&gt;0.602&lt;/td&gt; &lt;td&gt;0.651&lt;/td&gt; &lt;td&gt;0.671&lt;/td&gt; &lt;td&gt;0.535&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;h2 id=&quot;문제점-분석&quot;&gt;문제점 분석&lt;/h2&gt; &lt;h3 id=&quot;데이터&quot;&gt;데이터&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/137607638-124c1622-6bfe-4a45-a16b-519314916436.jpg&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;1차적으로 만든 데이터셋의 특성이고 다음과 같이 분석했다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;bounding box는 한 class당 500개 정도이다.&lt;/li&gt; &lt;li&gt;bounding box 중심의 위치는 대개 정중앙이다.&lt;/li&gt; &lt;li&gt;bounding box가 이미지의 크거나 대부분을 차지한다.&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;모델&quot;&gt;모델&lt;/h3&gt; &lt;p&gt;경량모델의 문제점이 그대로 나타났다. 모델이 가벼워 training set에 overfitting이 잘 되었고, 모델 자체의 성능도 낮았다. yolov5x6와 같은 같은 계열의 무거운 모델을 쓸 수 있지만 그러면 서비스 자체가 느려질 것이었다&lt;/p&gt; &lt;h2 id=&quot;해결방법&quot;&gt;해결방법&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;해결방안 1 - 데이터 추가&lt;/strong&gt;&lt;/p&gt; &lt;table&gt; &lt;tr&gt; &lt;td align=&quot;center&quot;&gt;Orignal Dataset&lt;/td&gt; &lt;td align=&quot;center&quot;&gt;Add more data&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/137607638-124c1622-6bfe-4a45-a16b-519314916436.jpg&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/137607640-9552448f-a39c-4a46-9d50-a523002be0e4.jpg&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;누가 뭐라고 하던 데이터가 많으면 최고다. class당 500개의 box는 말도 안되는 개수라 직접 imgenet dataset에서 annotation을 해서 수를 1200개 이상으로 늘렸다. 그러자 bounding box의 중심도 많이 퍼졌고, small object도 많이 생겨났다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;해결방안 2- augmentation 방법 변경&lt;/strong&gt;&lt;/p&gt; &lt;table&gt; &lt;tr&gt; &lt;td align=&quot;center&quot;&gt;기존&lt;/td&gt; &lt;td align=&quot;center&quot;&gt;변경&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/137607771-6509a1f3-872a-4bfd-ac0f-389e7dcd8fdc.jpeg&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/137607774-68692b66-5324-4184-ba9a-e41151a6a561.jpeg&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;기존 데이터셋의 문제점이 물체의 중심이 이미지 정가운데이고, 물체가 이미지의 대부분을 차지한다 였다. 이 문제점을 해결하기 위해 yolov5에 있는 mosaic augmentation이 적절했으나 이는 부족했다. 따라서 우리는 lagacy code에 있는 masaic_9 augmentatio을 사용하기로 했다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;해결방안 3 - knowledge distillation(&lt;a href=&quot;https://arxiv.org/abs/1906.03609&quot;&gt;paper link&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/136683028-fb1ca2f0-97c0-4581-9b7a-64e26536d7af.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;경량화 기법 중 하나인 knowledge distillation을 사용했다. yolov5x6를 teacher model로 yolov5m6를 student model로 knowledge distillation을 진행하면 overfitting을 막아주고 성능이 높아 질 것이다&lt;/p&gt; &lt;h3 id=&quot;결과&quot;&gt;결과&lt;/h3&gt; &lt;table align=&quot;center&quot;&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;enhance&lt;/td&gt; &lt;td&gt;model&lt;/td&gt; &lt;td&gt;precision&lt;/td&gt; &lt;td&gt;recall&lt;/td&gt; &lt;td&gt;mAP_0.5&lt;/td&gt; &lt;td&gt;mAP_0.5:0.95&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;Before add dataset&lt;/td&gt; &lt;td&gt;yolov5m6&lt;/td&gt; &lt;td&gt;0.602&lt;/td&gt; &lt;td&gt;0.651&lt;/td&gt; &lt;td&gt;0.671&lt;/td&gt; &lt;td&gt;0.535&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;Before add dataset&lt;/td&gt; &lt;td&gt;yolov5m6&lt;/td&gt; &lt;td&gt;0.736&lt;/td&gt; &lt;td&gt;0.779&lt;/td&gt; &lt;td&gt;0.815&lt;/td&gt; &lt;td&gt;0.599&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;None (Add dataset)&lt;/td&gt; &lt;td&gt;yolov5m6&lt;/td&gt; &lt;td&gt;0.736&lt;/td&gt; &lt;td&gt;0.779&lt;/td&gt; &lt;td&gt;0.815&lt;/td&gt; &lt;td&gt;0.599&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;mosaic_9 50%&lt;/td&gt; &lt;td&gt;yolov5m6&lt;/td&gt; &lt;td&gt;0.756&lt;/td&gt; &lt;td&gt;0.775&lt;/td&gt; &lt;td&gt;0.815&lt;/td&gt; &lt;td&gt;0.602&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;mosaic_9 100%&lt;/td&gt; &lt;td&gt;yolov5m6&lt;/td&gt; &lt;td&gt;0.739&lt;/td&gt; &lt;td&gt;0.813&lt;/td&gt; &lt;td&gt;0.806&lt;/td&gt; &lt;td&gt;0.594&lt;/td&gt; &lt;/tr&gt; &lt;tr align=&quot;center&quot;&gt; &lt;td&gt;self-distillation&lt;/td&gt; &lt;td&gt;yolov5m6&lt;/td&gt; &lt;td&gt;0.722&lt;/td&gt; &lt;td&gt;0.822&lt;/td&gt; &lt;td&gt;0.807&lt;/td&gt; &lt;td&gt;0.592&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;table&gt; &lt;tr&gt; &lt;td align=&quot;center&quot;&gt;Original Image&lt;/td&gt; &lt;td align=&quot;center&quot;&gt;Result Image&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/136698553-a00eb618-7783-41d9-bd2c-203dbbd60946.jpg&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/136698552-42c71108-9efc-4c88-a68a-3f5aec8452c6.jpg&quot; width=&quot;80%&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;결과적으로 recall이 17.1%p를 올리는 결과를 냈다. recall을 높이기 위해 precision이 조금 낮아졌다는 것이 아쉬였다. 사용했던 knowledge distillation code는 여기에 있다. &lt;a href=&quot;https://github.com/wonbeomjang/yolov5-knowledge-distillation&quot;&gt;yolov5-knowledge-distillation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.wonbeomjang.kr/blog/2021/osam-1/&quot;&gt;[OSAM] 1. 팀 결정 및 주제&amp;amp;시스템 설계&lt;/a&gt;&lt;br /&gt; &lt;a href=&quot;https://www.wonbeomjang.kr/blog/2021/osam-2/&quot;&gt;[OSAM] 2. computer vision 개발 과정&lt;/a&gt;&lt;br /&gt; &lt;a href=&quot;https://www.wonbeomjang.kr/blog/2021/osam-3/&quot;&gt;[OSAM] 3. 이제 끝나는 건가?&lt;/a&gt;&lt;/p&gt; </description> <pubDate>Fri, 29 Oct 2021 09:50:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2021/osam-2/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2021/osam-2/</guid> <category>OSAM</category> <category>Computer Vision</category> </item> <item> <title>[OSAM] 1. 팀 결정 및 주제&amp;시스템 설계</title> <description>&lt;p&gt;군대에서 재미없는 나날을 보내고 있었는데 동아리형으로부터 OSAM에 꼭 나가보라는 이야기를 들었다. 근데 OSAM이 뭐지??&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/139563256-61f7c62b-e4c9-4d0c-8cf6-7b1efa05699f.PNG&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;오 신기한거다라고 생각하며 참여했다.&lt;/p&gt; &lt;h2 id=&quot;1차-아이디어&quot;&gt;1차 아이디어&lt;/h2&gt; &lt;p&gt;나는 지금까지 computer V\vision을 공부하고 있었기 때문에 관련 주제를 선정했다. 다들 알다싶이 요즘 군대에선 핸드폰을 쓸 수 있다. 하지만 카메라는 예외이다. 나는 항상 그것이 의문스러웠다. &lt;del&gt;병사는 보안을 위반할 보안도 없는데…&lt;/del&gt; 아무튼 병사들도 조금은 자유롭게 카메라를 쓸 수 사람을 제외한 모든 부분은 날려버리는 프로그램을 만드려고 했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/osamhack2021/APP_WEB_AI_AIMS_MOJIRI/blob/main/AI/images/image5_blurred.jpg?raw=true&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;다른 사람이 만든거지만 이런 느낌이랄까? 그렇게 개발계획서를 작성하고 팀을 모집하다가 메일이 하나 왔다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/139563446-f7695d4c-a6e8-41d4-9824-abc5160b1821.PNG&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;처음 메일을 받고 팀을 합칠까 고민을 했고, 비슷한 아이디어라 나중에 평가 받을 때 수상을 못할 확률이 있어서 팀에 합류하기로 했다.&lt;/p&gt; &lt;h2 id=&quot;2차-아이디어&quot;&gt;2차 아이디어&lt;/h2&gt; &lt;p&gt;그렇게 나는 ‘카나리아’팀에 들어갔다. 대충 어플의 컨셉을 말하자면 다음과 같다.&lt;/p&gt; &lt;h3 id=&quot;카나리아--모두를-위한-군사보안-경보-시스템&quot;&gt;🐤카나리아 : 모두를 위한 군사보안 경보 시스템&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/osamhack2021/AI_APP_WEB_Canary_Canary/main/image/canary_2.0.png?token=AJV5HZXFAFMEZ5DCXTQDW5TBRTRPM&quot; alter=&quot;LOGO&quot; /&gt;&lt;br /&gt; &lt;img src=&quot;https://img.shields.io/badge/Version-1.0.0-blue?style=for-the-badge&amp;amp;logo&quot; /&gt; &lt;a href=&quot;https://github.com/osamhack2021/AI_APP_WEB_Canary_Canary/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-GNU GPL v3.0-blue?style=for-the-badge&amp;amp;logo&quot; /&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Canary는 머신러닝을 활용하여 사진 안의 보안 위반 가능성이 있는 요소를 식별하고, 자동 모자이크 처리를 하고, 이를 사용자에게 경고해주는 통합 보안 경보 시스템입니다. Canary App, Canary in Instagram, Admin logweb으로 구성되어 있으며, 앱에서 처리된 사진에는 QR코드가 들어가 처리 여부를 쉽게 식별할 수 있습니다.&lt;/p&gt; &lt;h3 id=&quot;️프로젝트-소개&quot;&gt;🗂️프로젝트 소개&lt;/h3&gt; &lt;p&gt;본 프로젝트는 사진의 보안 내용을 제거하는 기능과 그러한 기능을 가진 카메라를 제공함으로서,&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;군 내에서 카메라를 사용 가능하게 함&lt;/strong&gt;과 동시에,&lt;/li&gt; &lt;li&gt;SNS에 올릴 사진의 보안 위반 가능성을 경고하여 사용자가 &lt;strong&gt;자발적으로&lt;/strong&gt; 보안을 준수 할 수 있게 합니다.&lt;/li&gt; &lt;li&gt;또 현재 SNS올라가 있는 게시물을 검사를 해 &lt;strong&gt;보안에 대한 경각심&lt;/strong&gt;을 일으킬 수 있습니다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;기간은 한 달… 한 달안에 끝낼 수 있을까 모르겠지만 최선을 다하면 되겠지 하면서 시작했다.&lt;/p&gt; &lt;h2 id=&quot;설계&quot;&gt;설계&lt;/h2&gt; &lt;p&gt;예상 사용자 설정과 시스템 설계는 전에 만들어 놓은 문서로 때우겠다. &lt;del&gt;이거면 다 알아보겠지 하면서 말이다&lt;/del&gt;&lt;/p&gt; &lt;h3 id=&quot;페르소나&quot;&gt;페르소나&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/134792500-00226c5c-592b-4298-aeb8-fb155704278f.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;시나리오&quot;&gt;시나리오&lt;/h3&gt; &lt;h4 id=&quot;1&quot;&gt;#1&lt;/h4&gt; &lt;p&gt;&lt;em&gt;막 자대배치를 받은 안준호 이병. 택배로 스마트폰을 받는다.&lt;/em&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;안준호 이병은 처음으로 어플리케이션을 실행한다. &lt;br /&gt; 1-1. 군번, 이름, 계급을 입력하여 자신의 정보를 저장한다.&lt;/li&gt; &lt;li&gt;드디어 스마트폰을 받아 두근대는 마음으로 사진을 찍기 위해 어플리케이션을 켠다.&lt;/li&gt; &lt;li&gt;촬영 모드로 들어가서 카메라를 켠 후 생활관 TV를 배경으로 사진을 찍는다.&lt;/li&gt; &lt;li&gt;잠시 후, TV 모니터가 모자이크 된 사진과 함께 경고 문구가 출력된다.&lt;/li&gt; &lt;li&gt;사진 저장 시 사진에 QR코드가 새겨진다. QR코드에는 안준호 이병의 군번이 암호화되어 들어간다.&lt;/li&gt; &lt;li&gt;모자이크가 된 사진을 SNS에 올려 자랑한다.&lt;/li&gt; &lt;/ol&gt; &lt;h4 id=&quot;2&quot;&gt;#2&lt;/h4&gt; &lt;p&gt;&lt;em&gt;긴 군생활을 끝내고 드디어 전역한 최종훈 병장. 같이 전역하는 동기들과 기념 사진을 찍는다.&lt;/em&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;최종훈 병장과 동기들은 부대 앞에서 기념 사진을 촬영한다.&lt;/li&gt; &lt;li&gt;SNS에 이 글을 게시하기 전, 최종훈 병장은 혹시 사진에 군사보안 위반은 없는지 걱정된다.&lt;/li&gt; &lt;li&gt;어플리케이션을 실행한 후, 방금 전 찍은 사진을 갤러리에서 선택한다.&lt;/li&gt; &lt;li&gt;잠시 후, 부대마크와 군 표지판 부분이 모자이크 된 사진과 함께 경고 문구가 출력된다.&lt;/li&gt; &lt;li&gt;사진 저장 시 사진에 QR코드가 새겨진다. QR코드에는 최종훈 병장의 군번이 암호화되어 들어간다.&lt;/li&gt; &lt;li&gt;최종훈 병장은 안심하면서 SNS에 사진을 업로드 한다.&lt;/li&gt; &lt;/ol&gt; &lt;h4 id=&quot;3&quot;&gt;#3&lt;/h4&gt; &lt;p&gt;&lt;em&gt;예비군 유시진 씨. 인스타그램에 올렸던 군대 사진들을 본다.&lt;/em&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;유시진 씨는 인스타그램에 올렸던 훈련 사진을 본다.&lt;/li&gt; &lt;li&gt;옛날 사진을 보던 중, 한 사진에 탱크가 찍힌 것을 본다.&lt;/li&gt; &lt;li&gt;Canary Instagram bot에 이 사진을 검토해 줄 것을 메시지로 요청한다.&lt;/li&gt; &lt;li&gt;잠시 후, 탱크가 모자이크 된 사진과 함께 경고 문구를 메시지로 받는다.&lt;/li&gt; &lt;li&gt;유시진 씨는 SNS 사진을 수정한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;시스템 흐름도&lt;/strong&gt;&lt;/p&gt; &lt;h3 id=&quot;user-case-diagram&quot;&gt;User-case Diagram&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/134690667-abe8f797-01a8-44db-ae89-ef7809c22d64.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;sequence-diagram&quot;&gt;Sequence Diagram&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/136720501-bbe98072-abbc-4797-a0c2-c66771f7e04a.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/136720255-0456ffd4-4d7d-4d2e-b5c5-09387c5861fa.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;시작&quot;&gt;시작&lt;/h2&gt; &lt;p&gt;두둥… 이제 개발을 시작한다. 한 달 후 어떻 결과물이 나올까 기대된다.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.wonbeomjang.kr/blog/2021/osam-1/&quot;&gt;[OSAM] 1. 팀 결정 및 주제&amp;amp;시스템 설계&lt;/a&gt;&lt;br /&gt; &lt;a href=&quot;https://www.wonbeomjang.kr/blog/2021/osam-2/&quot;&gt;[OSAM] 2. computer vision 개발 과정&lt;/a&gt;&lt;br /&gt; &lt;a href=&quot;https://www.wonbeomjang.kr/blog/2021/osam-3/&quot;&gt;[OSAM] 3. 이제 끝나는 건가?&lt;/a&gt;&lt;/p&gt; </description> <pubDate>Fri, 29 Oct 2021 08:50:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2021/osam-1/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2021/osam-1/</guid> <category>OSAM</category> </item> <item> <title>[AutoML] NASNet</title> <description>&lt;p&gt;2017년에 NASNet이 나온 이후 2018년 부터 AutoML의 시대가 열렸습니다. MnasNet, MobileNet V3, EffientNet등 여러 경량화 네트워크들은 NAS(Neural Architecture Search)을 사용했고, 모바일쪽 네트워크들은 많이 NAS를 사용하고 있습니다. 이번에 소개할 논문은 &lt;a href=&quot;https://arxiv.org/pdf/1611.01578.pdf&quot;&gt;Neural Architecture Search with Reinforcement Learning&lt;/a&gt;를 소개해드리겠습니다.&lt;/p&gt; &lt;p&gt;Neural Architecture Search은 RNN을 사용하여 model description을 생성하고 생성된 네트워크를 학습합니다. 그리고 validation set을 이용하여 accuracy를 구하고 이를 reward로 만듭니다. 후에 이 reward를 갖고 policy gradient를 구해 controller를 업데이트 시킵니다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/105997147-9dbfae80-60ee-11eb-9477-1c820fdb31cb.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;generate-model-description&quot;&gt;Generate Model Description&lt;/h2&gt; &lt;p&gt;NAS에서 controller가 model description을 만듭니다. model description을 갖고 model을 만든 다음 학습을 시킵고 수렴이 되면 validation set으로 accuracy를 측정하게 됩니다. controller RNN의 파라미터 \(\theta_c\)는 validation accuracy의 평균을 이용해 최적화 시킵니다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/106002532-98fdf900-60f4-11eb-9e16-597b22b9371d.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;training-with-reinforce&quot;&gt;Training With REINFORCE&lt;/h2&gt; &lt;p&gt;controller를 학습시키려면 우리는 loss 함수를 정의해야합니다. controller에서 만들어진 네트워크를 child network라고 하겠습니다. 그리고 child network를 구성하기 위한 action들을 \(a_{1:T}\), 학습된 child network의 validation accuracy를 R이라고 할 때 \(J(\theta_c)\)는 다음과 같습니다.&lt;/p&gt; &lt;center&gt; $$J(\theta_c)=E_{P(a_{1:T};\theta_c)}[R]$$ &lt;/center&gt; &lt;p&gt;reward signal인 R은 미분 불가능하기 떄문에 REINFORCE라는 policy로 update를 합니다. 이는 다음과 같은 식으로 만들 수 있고&lt;/p&gt; &lt;center&gt; $$\nabla_{\theta_c} J(\theta_c) = \sum_{t=1}^{T}E_{P(a_1:T; \theta_c)}[\nabla_{\theta_c} logP(a_t | a_{(t-1):1}; theta_c) R]$$ &lt;/center&gt; &lt;p&gt;따라서 위의 식은 다음과 같이 근사될 수 있습니다.&lt;/p&gt; &lt;center&gt; $$\frac{1}{m} \sum_{k=1}^{m} \sum_{t=1}^{T} \nabla_{\theta_c} logP(a_t | a_{(t-1):1}; {\theta_c}) R_k$$ &lt;/center&gt; &lt;p&gt;m은 한 배치에 controller가 만들 child network의 개수이고 k번째 child network의 validation accuracy를 \(R_k\)이다. 위의 식은 불편 추정치이지만 분산이 크기 때문에 분산을 줄이기 위해서 다음과 같은 식을 씁니다.&lt;/p&gt; &lt;center&gt; $$\frac{1}{m} \sum_{k=1}^{m} \sum_{t=1}^{T} \nabla_{\theta_c} logP(a_t | a_{(t-1):1}; {\theta_c}) (R_k-b)$$ &lt;/center&gt; &lt;p&gt;b는 이전 네트워크들의 validation accuracy의 지수이동평균입니다.&lt;/p&gt; &lt;h2 id=&quot;skip-connection&quot;&gt;Skip Connection&lt;/h2&gt; &lt;p&gt;GoogleNet, ResNet같은 네트워크들은 skip connection을 통해 성능을 높힙니다. 이와 같이 NASNet에서도 skip connection을 생성하기 위해 다음과 같이 설정했습니다.&lt;/p&gt; &lt;p&gt;layer N에서 이전의 layer으로 부터 skip connection이 있는지를 결정하는 N-1개의 content-based sigmoid(anchor point)를 추가합니다.&lt;/p&gt; &lt;center&gt; P(Layer j is an input to layer i) = $$sigmoid(v^T tanh(W_{prev} * h_j + W_{curr} * h_i))$$ &lt;/center&gt; &lt;p&gt;\(h_j\)는 j번째 layer의 controller의 hiddenstate이고 그 값은 0 부터 N-1까지 가질 수 있다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/106002715-cb0f5b00-60f4-11eb-81a2-64e564aabcf0.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;하지만 이렇게 연결하다보면 구조가 망가지는 경우가 있기 때문에 다음과 같은 규칙을 정한다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;input layer로 쓴 layer는 다른 layer의 input layer가 되지 않는다.&lt;/li&gt; &lt;li&gt;skip connection이 안되어있는 layer를 다 final layer에 connection을 만든다.&lt;/li&gt; &lt;li&gt;만약 skip connection시 layer의 size가 다르면 zero padding을 한다.&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/40621030/106003536-a49def80-60f5-11eb-8f93-49d8ff7bc92f.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;opinion&quot;&gt;Opinion&lt;/h2&gt; &lt;p&gt;손수 만든 네트워크들에 비해 그렇게까지 정확도가 높다고 말할 수는 없지만 그래도 AutoML로 만들었다는 것에 의의가 있는 것 같습니다. 그리고 실험결과를 볼 때 filter들은 직사각형이 많다라는 말이 있는데 무작위로 뽑아도 직사각형이 될 확률이 높아서 그렇지 않을까 생각합니다.&lt;/p&gt; </description> <pubDate>Wed, 27 Jan 2021 07:41:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2021/NASNet/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2021/NASNet/</guid> <category>backbone</category> <category>nas</category> <category>paper</category> </item> <item> <title>[Python] 우선순위 큐 (heapq vs priority queue)</title> <description>&lt;p&gt;파이썬에서는 유용한 자료구조 라이브러리를 제공합니다. 그 중에 하나는 우선순위 큐(priority queue)입니다.&lt;/p&gt; &lt;h2 id=&quot;우선순위-큐란&quot;&gt;우선순위 큐란?&lt;/h2&gt; &lt;p&gt;우리는 많은 경우에서 우선순위를 만납니다. 응급실의 예를 들어보겠습니다. 다음의 환자들이 있습니다. 스케쥴링을 이야기할 것이 아니기 때문에 치료 시간은 0으로 가정하겠습니다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;5분 이내 치료해야할 사람&lt;/li&gt; &lt;li&gt;10분 이내 치료해야할 사람&lt;/li&gt; &lt;li&gt;2분 이내 치료해야할 사람&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;우리는 위의 환자의 치료 순위를 어떻게 정해야합니까? 3 -&amp;gt; 1 -&amp;gt; 2순으로 치료해야 할 것입니다. 위의 상황을 해결해주는 자료구조가 우선순위 큐입니다.&lt;/p&gt; &lt;p&gt;구현 방법은 이 글에서 다루지 않겠습니다.&lt;/p&gt; &lt;h2 id=&quot;파이썬-라이브러리&quot;&gt;파이썬 라이브러리&lt;/h2&gt; &lt;p&gt;파이썬에서는 heapq, PriorityQueue로 우선순위 큐를 지원합니다. 사용법은 다음과 같습니다.&lt;/p&gt; &lt;h3 id=&quot;heapq&quot;&gt;heapq&lt;/h3&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;heappush&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;heappush&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;heappush&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;heappop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 1 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;heappop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 2 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;heappop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 3 &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;priorityqueue&quot;&gt;PriorityQueue&lt;/h3&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PriorityQueue&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;PriorityQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 1 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 2 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 3 &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;다음과 같은 의문이 들 수 있습니다. &lt;em&gt;똑같은 역할을 하는데 과연 두 라이브러리들은 무엇이 다를까?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;PriorityQueue는 객체이고 heapq는 여러 함수들이 들어있는 파일입니다.&lt;/del&gt; 맞는 말이긴 합니다.&lt;/p&gt; &lt;p&gt;정확하게 말하면 PriorityQueue은 lock을 제공하여 thread-safty class입니다. 반면에 heapq는 list를 사용하기 때문에 thread-safty class가 아닙니다.&lt;/p&gt; &lt;p&gt;그래서 그런지 몰라도 실행시킬 때 실행속도가 차이가 납니다. (제 추측…)&lt;/p&gt; &lt;h2 id=&quot;실행속도-비교&quot;&gt;실행속도 비교&lt;/h2&gt; &lt;p&gt;t2.micro에서 실험을 진행했습니다.&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Duration of PriorityQueue 0.980911 Duration of heapq 0.175374 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;코드&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PriorityQueue&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;priority_queue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;PriorityQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;priority_queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;priority_queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Duration of PriorityQueue &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;heappush&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;heappop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Duration of heapq &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h2 id=&quot;마치며&quot;&gt;마치며&lt;/h2&gt; &lt;p&gt;사실 이 라이브러리는 백준 &lt;a href=&quot;https://www.acmicpc.net/problem/1202&quot;&gt;보석도둑&lt;/a&gt;을 풀면서 알게되었습니다. 알고리즘문제를 풀때 heapq를 써야지 PriorityQueue를 쓰면 시간초과가 결렸습니다.&lt;br /&gt; 다들 저와 같은 실수를 하지 않길 빌면서 이만 가보도록 하겠습니다. &lt;del&gt;알고리즘 어렵다!&lt;/del&gt;&lt;/p&gt; </description> <pubDate>Sun, 10 Jan 2021 06:41:11 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2021/heapq-vs-priority-q/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2021/heapq-vs-priority-q/</guid> <category>Data Structure</category> <category>Python</category> </item> </channel> </rss>