<?xml version="1.0" encoding="UTF-8"?> <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"> <channel> <title> blank </title> <description>개발을 좋아하는 딥러닝 리서쳐 장원범입니다. </description> <link>https://www.wonbeomjang.kr/</link> <atom:link href="https://www.wonbeomjang.kr/feed.xml" rel="self" type="application/rss+xml"/> <pubDate>Mon, 18 Dec 2023 13:31:10 +0000</pubDate> <lastBuildDate>Mon, 18 Dec 2023 13:31:10 +0000</lastBuildDate> <generator>Jekyll v4.3.2</generator> <item> <title>Keras 3.0 설명</title> <description>&lt;h1 id=&quot;why&quot;&gt;Why?&lt;/h1&gt; &lt;p&gt;과거 keras는 Tensorflow, Theano, MXNet등 여러 deep learning backend framework가 있을 때 multi-backend 지원의 강점을 가지며 출시되었다. 하지만 Teano와 MXNet등 여러 framework들은 쇄퇴의 길을 걸었고, tensorflow만 살아남게 되었다. 하지만 당시 tensorflow도 문제점은 가지고 있었다. 그것은 model 선언이 비직관적이라는 것이다. 따라서 tensorflow는 keras를 공식 레포에 집어넣어 keras.layer로 모델을 만들고 tensorflow backend로 학습하는 구조로 발전했다.&lt;/p&gt; &lt;p&gt;하지만 현재는 여러 연구에서 pytorch를 사용하고, pytorch 기반의 huggingface가 등장하면서 keras입장에서는 pytorch가 매력적인 시장으로 보였다. 그리고 tensorflow의 사용자가 줄어가고, 윈도우 네이티브 업데이트 지원을 종료하면서 다시 multi-backend의 강점을 다시 살리기로 했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;그래서 keras는 다음과 같은 기능을 강조하면서 keras3.0을 출시했다.&lt;/p&gt; &lt;h1 id=&quot;주요-기능&quot;&gt;주요 기능&lt;/h1&gt; &lt;p&gt;Keras 3.0을 출시하면서 다음과 같은 중요한 기능을 제시했다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The full Keras API, available for TensorFlow, JAX, and PyTorch&lt;/li&gt; &lt;li&gt;A cross-framework low-level language for deep learning&lt;/li&gt; &lt;li&gt;Seamless integration with native workflows in JAX, PyTorch, and TensorFlow&lt;/li&gt; &lt;li&gt;Support for cross-framework data pipelines with all backends&lt;/li&gt; &lt;li&gt;A new distribution API for large-scale data parallelism and model parallelism&lt;/li&gt; &lt;li&gt;Pretrained models&lt;/li&gt; &lt;li&gt;Progressive disclosure of complexity&lt;/li&gt; &lt;li&gt;A new stateless API for layers, models, metrics, and optimizers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;하나씩 살펴보자&lt;/p&gt; &lt;h2 id=&quot;the-full-keras-api-available-for-tensorflow-jax-and-pytorch&quot;&gt;The full Keras API, available for TensorFlow, JAX, and PyTorch&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;keras로 선언된 모델과 함수들은 tensorflow, jax, pytorch에서 모두 사용가능하다. 즉, 3개의 프레임워크에서 모두 keras 함수를 사용할 수 있다는 것이다. 여기서 재밌는 점은 기존에 tf.keras로 선언된 모델도 jax, pytorch에서 실행 가능하다.&lt;/p&gt; &lt;h2 id=&quot;a-cross-framework-low-level-language-for-deep-learning&quot;&gt;A cross-framework low-level language for deep learning&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;딥러닝 모델을 구성하다보면 matmul, stack 등 기본적인 연산자가 필요할 때 있다. 이럴때는 keras.ops를 사용하여 기본적인 연산자를 구성하면 tensorflow, jax, pytorch에서 모두 사용가능하다. 이 떄 keras는 두 가지를 중심으로 구현했다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Numpy에 관련한 연산자는 모두 구현한다. ex) ops.matmul, ops.sum, ops.stack, ops.einsum&lt;/li&gt; &lt;li&gt;Neural-specific function을 구현한다. ex) ops.softmax, ops.binary_crossentropy, ops.conv&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;seamless-integration-with-native-workflows-in-jax-pytorch-and-tensorflow&quot;&gt;Seamless integration with native workflows in JAX, PyTorch, and TensorFlow&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_5.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Integration하다보면 기존의 training loop 등 workflow를 그대로 유지해야할 경우가 있다. 물론 keras3.0은 이 경우도 지원한다.&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;Write a low-level JAX training loop to train a Keras model using an optax optimizer, jax.grad, jax.jit, jax.pmap.&lt;/li&gt; &lt;li&gt;Write a low-level TensorFlow training loop to train a Keras model using tf.GradientTape and tf.distribute.&lt;/li&gt; &lt;li&gt;Write a low-level PyTorch training loop to train a Keras model using a torch.optim optimizer, a torch loss function, and the torch.nn.parallel.DistributedDataParallel wrapper.&lt;/li&gt; &lt;li&gt;Use Keras layers in a PyTorch Module (because they are Module instances too!)&lt;/li&gt; &lt;li&gt;Use any PyTorch Module in a Keras model as if it were a Keras layer.&lt;/li&gt; &lt;li&gt;etc.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;a-new-distribution-api-for-large-scale-data-parallelism-and-model-parallelism&quot;&gt;A new distribution API for large-scale data parallelism and model parallelism&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_7.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_8.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;keras에서는 여러 data parallelism을 제공한다. 단 두줄 만으로 분산학습이 된다는게 신기하긴 하다.&lt;/p&gt; &lt;h2 id=&quot;support-for-cross-framework-data-pipelines-with-all-backends&quot;&gt;Support for cross-framework data pipelines with all backends&lt;/h2&gt; &lt;p&gt;각 framework별로 다른 dataset 객체를 사용한다. Keras3.0은 이를 모두 지원한다.&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;tf.data.Dataset pipelines: the reference for scalable production ML.&lt;/li&gt; &lt;li&gt;torch.utils.data.DataLoader objects.&lt;/li&gt; &lt;li&gt;NumPy arrays and Pandas dataframes.&lt;/li&gt; &lt;li&gt;Keras’s own keras.utils.PyDataset objects.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;pretrained-models&quot;&gt;Pretrained models&lt;/h2&gt; &lt;p&gt;Keras3.0은 다음과 같은 pretrained model을 지원한다.&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;BERT&lt;/li&gt; &lt;li&gt;OPT&lt;/li&gt; &lt;li&gt;Whisper&lt;/li&gt; &lt;li&gt;T5&lt;/li&gt; &lt;li&gt;StableDiffusion&lt;/li&gt; &lt;li&gt;YOLOv8&lt;/li&gt; &lt;li&gt;SegmentAnything&lt;/li&gt; &lt;li&gt;etc.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;progressive-disclosure-of-complexity&quot;&gt;Progressive disclosure of complexity&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_6.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;개발하다보면 pytorch lightening, pytorch ignite, tensorflow orbit등 disclosure를 위한 툴을 쓴 경험이 있을 것이다. Keras는 이것이 keras api의 핵심 디자인으로 삼았으며 이를 지원한다 한다. &lt;del&gt;그냥 다른거 쓸 것 같긴한데…&lt;/del&gt;&lt;/p&gt; &lt;h2 id=&quot;a-new-stateless-api-for-layers-models-metrics-and-optimizers&quot;&gt;A new stateless API for layers, models, metrics, and optimizers.&lt;/h2&gt; &lt;p&gt;함수형 프로그래밍을 좋아하는 사람을 위해 stateless한 함수들을 만들었다.&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;All layers and models have a stateless_call() method which mirrors &lt;strong&gt;call&lt;/strong&gt;().&lt;/li&gt; &lt;li&gt;All optimizers have a stateless_apply() method which mirrors apply().&lt;/li&gt; &lt;li&gt;All metrics have a stateless_update_state() method which mirrors update_state() and a stateless_result() method which mirrors result().&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h1 id=&quot;example&quot;&gt;Example&lt;/h1&gt; &lt;p&gt;Tensorflow는 기존의 방법과 동일해서 설명을 생략하겠다.&lt;/p&gt; &lt;h3 id=&quot;mnist-with-keras-vgg19-pytorch-beckend&quot;&gt;MNIST with keras vgg19 (Pytorch Beckend)&lt;/h3&gt; &lt;script src=&quot;https://gist.github.com/wonbeomjang/e935128f7f55045ab2d08e091cc2b8e2.js&quot;&gt;&lt;/script&gt; &lt;p&gt;이렇게 하면 기존 tensorflow나 keras vgg를 weight를 포함하여 사용할 수 있다. 여기서 주의할 점은 dataset augmentation 부분에서 CHW를 HWC로 바꿔줘야한다는 것이다.&lt;/p&gt; &lt;h3 id=&quot;declare-pytorch-model-using-keras-application&quot;&gt;Declare Pytorch Model Using Keras Application&lt;/h3&gt; &lt;script src=&quot;https://gist.github.com/wonbeomjang/c76b1da2952d231e209a0d03896c4aef.js&quot;&gt;&lt;/script&gt; &lt;p&gt;재밌는 것은 keras.layer가 torch.nn.Module과 호환이되어 다음과 같이 모델을 선언할 수 있다.&lt;/p&gt; &lt;h1 id=&quot;맺으며&quot;&gt;맺으며&lt;/h1&gt; &lt;p&gt;너무 많은 담기 그래서 이쯤으로 마치고, 더 많은 예제는 다음에 다루기로 하겠다.&lt;/p&gt; </description> <pubDate>Sun, 03 Dec 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/keras-3/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/keras-3/</guid> <category>framework</category> <category>framework</category> </item> <item> <title>What Makes Multi-modal Learning Better than Single (Provably)</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;우리 세상은 많은 modality가 존재한다. 그리고 관념적으로도 여러 modal의 네트워크들을 fusion시키면 uni-modal보다 성능이 더 좋게 나온다. 그렇다면 우리는 이러한 궁금증이 생긴다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; *multi-modal learning이 uni-modal learning보다 좋은 성능을 제공할까?* &lt;/p&gt; &lt;p&gt;저자는 이 궁금증에서 연구를 시작했고, 다음 두 가지를 중점적으로 살펴봤다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;(When) 어떤 상황에서 multi-modal이 uni-modal 보다 성능이 좋은가?&lt;/li&gt; &lt;li&gt;(Why) 무엇이 이런 성능을 유도했는가?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;그리고 연구를 통해서 저자가 한 comtribution은 다음과 같다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-modal learning을 population risk로 설명하고, 이는 latent representation quality의 bound 되어있다는 것을 밝혔다.&lt;/li&gt; &lt;li&gt;전체 modality의 subset으로 훈련시킨 network의 quality의 upper bound를 유도했다.&lt;/li&gt; &lt;li&gt;Modality의 subset으로 학습시키면 성능이 하락한다는 것을 이론적으로 분석했다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;참고로 결론은 다음과 같다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multiple modality는 그 modal의 subset보다 적은 population risk를 갖는다.&lt;/li&gt; &lt;li&gt;이는 multi-modal이 더 정확한 latent space representation을 학습할 수 있다는 것이다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;이제 하나씩 살펴보자.&lt;/p&gt; &lt;h1 id=&quot;the-multi-modal-learning-formulation&quot;&gt;The Multi-modal Learning Formulation&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/figure1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;먼저 수식을 정리하자. K개의 modalities에 대해서 data는 \(\mathbb{x}:=(x^{(1)},\cdots,x^{(K)})\) 으로 표현한다. 이 때 \(x^{(k)} \in \mathcal{X}^{(k)}\) 이다. 우리는 K개의 modalities를 보유하기 때문에 전체 input data space는&lt;/p&gt; \[\mathcal{X}=\mathcal{X}^{1} \times \cdots \times \mathcal{X}^{k}\] &lt;p&gt;로 표현된다. 그리고 target domain을 \(\mathcal{Y}\) , multi-modal의 공통된 latent space를 \(\mathcal{Z}\) 라 하자. 우리는 이제 true mapping을 다음과 같이 쓸 수 있다.&lt;/p&gt; \[g^\star: \mathcal{X} \mapsto \mathcal{Z}, g^\star \in \mathcal{G}\] \[h^\star: \mathcal{Z} \mapsto \mathcal{Y}, h^\star \in \mathcal{H}\] &lt;p&gt;그렇다면 이제 우리는 \(\mathbb{x}\) 의 data distribution을 정의할 수 있다.&lt;/p&gt; \[\mathbb{P}_\mathcal{D}(\mathbb{x},y)\triangleq\mathbb{P}_{y|x}(y|h^\star\circ g^\star(\mathbb{x}))\mathbb{P}_\mathbb{x}(\mathbb{x})\] &lt;p&gt;참고로 \(h^\star\circ g^\star(\mathbb{x})=h^\star(g^\star(\mathbb{x}))\) 로 합성함수를 의미한다.&lt;/p&gt; &lt;p&gt;우리는 일반화를 위해 \(\mathcal{N} \leq \mathcal{M}\) 인 modalitie의 subset에 대해서 살펴볼 것이다. Modality의 superset을 정의하자.&lt;/p&gt; \[\mathcal{X}^\prime := (\mathcal{X}^{(1)}\cup\bot)\times\cdots\times(\mathcal{X}^{(K)}\cup\bot)\] &lt;p&gt;이때, \(\bot\) 은 k번째의 modality는 쓰지 않는다는 것이다. 간단하게 시각화하면 다음과 같다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/img1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;이제 modalities를 선택하는 함수를 정의하자.&lt;/p&gt; \[p_\mathcal{M}(\mathbb{x})^{(k)}= \begin{cases} \mathbb{x}^{(k)} \text{ if } k\in\mathcal{M} \\ \bot \text{ else } \end{cases}\] &lt;p&gt;이때, 우리는 다음과 같은 식을 만들수도 있다. \(p^\prime_\mathcal{M} := \mathcal{X}^\prime\mapsto\mathcal{X}^\prime\)&lt;br /&gt; 우리의 목표는 Empirical Risk Minimization (ERM) principle에 따라서 learning objective를 minimize하는 것이다.&lt;/p&gt; \[\text{min } \hat{r}(h\circ g_\mathcal{M} \triangleq\frac{1}{m}\sum_{i=1}^ml(h\circ g_\mathcal{M}(\mathbb{x}_i),y_i) \text{ s.t. } h \in \mathcal{H}, g_\mathcal{M} \in \mathcal{G}\] &lt;p&gt;여기서 \(l\) 은 loss fuction이고, 최종적으로 정의하는 population risk는 다음과 같다.&lt;/p&gt; \[r(h\circ g_\mathcal{M})=\mathbb{E}_{(\mathbb{x}_i, y_i)\sim\mathcal{D}}[\hat{r}(h\circ g_\mathcal{M})]\] &lt;h1 id=&quot;main-result&quot;&gt;Main Result&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Definition 1.&lt;/strong&gt; Given a data distribution with the form in (1), for any learned latent representation mapping \(g \in \mathcal{G}\) , the &lt;em&gt;latent representation quality&lt;/em&gt; is defined as&lt;/p&gt; \[\eta(g)=\text{inf}_{h\in\mathcal{H}}[r(h\circ g)-r(h(h^*\circ g^*))]\] &lt;/blockquote&gt; &lt;p&gt;즉, \(\eta(g))\) 는 mapping function의 \(g \in \mathcal{G}\) 에 대해서 true latent space와 차이이기 때문에 latent space quality라고 할 수 있다.&lt;/p&gt; &lt;h3 id=&quot;rademacher-complexity&quot;&gt;Rademacher complexity&lt;/h3&gt; &lt;p&gt;이제 model complexity를 측정하는 Rademacher complexity에 대해서 알아보자. \(\mathcal{F}\) 를 \(\mathbb{R}^d \mapsto \mathbb{R}\) 인 vector-valued function으로 정의하자. \(\mathbb{R}^d\) 에서 iid 한 \(Z_1,...,Z_m\) 에 대해 sample를 \(S=(Z_1,...,Z_m)\) 라고 하자. Empirical Rademacher complexity는 다음과 같이 정의된다.&lt;/p&gt; \[\hat{\mathfrak{R}}_S(\mathcal{F}):=\mathbb{E}_\sigma[ \underset{f\in\mathcal{F}}{\text{sup}}\frac{1}{m}\sum_{i=1}^m\sigma_if(Z_i)]\] &lt;p&gt;이 때, \(\sigma=(\sigma_1,...,\sigma_n)^\top\) with \(\sigma_i \sim \text{unif}\{-1, 1\}\) 이다. 전체적인 Rademacher complexity은 다음과 같다.&lt;/p&gt; \[\mathfrak{R}_S(\mathcal{F})=\mathbb{E}[\hat{\mathfrak{R}}_S(\mathcal{F})]\] &lt;p&gt;이해하기 어려우니 다른 블로그의 설명을 인용하겠다.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;Rademacher complexity가 1이라는 것은 모델이 위와 같은 random한 setup에서도 잘 fitting 했다는 것이므로, complexity가 크고 따라서 generalize를 잘 못할 것이라고 이야기 할 수 있다는 개념이다.&lt;br /&gt; &lt;a href=&quot;https://yun905.tistory.com/68&quot;&gt;https://yun905.tistory.com/68&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;connection-to-latent-representation-quality&quot;&gt;Connection to Latent Representation Quality&lt;/h2&gt; &lt;p&gt;이제 latent space quality와 population risk의 관계를 살펴보자.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;. Let \(S = ((x_i,y_i))^m_{i=1}\) be a dataset of m examples drawn i.i.d. according to \(\mathcal{D}\) . Let M, N be two distinct subsets of [ \(K\) ]. Assuming we have produced the empirical risk minimizers \((\hat{h}_\mathcal{M}, \hat{g}_\mathcal{M})\) and \((\hat{h}_\mathcal{N}, \hat{g}_\mathcal{N})\) , training with the \(\mathcal{M}\) and \(\mathcal{N}\) modalities separately. Then, for all \(1 &amp;gt; \delta &amp;gt; 0\) , with probability at least \(1-\frac{\delta}{2}\) :&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;\(r(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}) - r(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}) \leq \gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N})+8L\mathfrak{R}(\mathcal{H} \circ \mathcal{G}_{\mathcal{M}})+\frac{4C}{\sqrt{m}}+2C\sqrt{\frac{2\text{ln}(2/\delta)}{m}}\)$&lt;/p&gt; \[\text{where}, \gamma_S(\mathcal{M},\mathcal{N})\triangleq\eta(\hat{g}_\mathcal{M})-\eta(\hat{g}_\mathcal{N})\] &lt;p&gt;즉, population risk의 차이는 latent space quality 차이와 model complexity에 upper bound가 된다는 것이다. 이는 그대로 사용하지 않고, 추후에 식 정리할 때 사용할 것이다. 여기서 sample size \(m\) 에 대해 \(\mathfrak{R}_S(\mathcal{F})\) 은 보통 \(\sqrt{C(\mathcal{F})/m}\) 에 bound된다. 따라서 우리는 다음과 같이 다시 쓸 수 있다.&lt;/p&gt; \[r(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}) - r(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}) \leq \gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N})+\text{O}(1/m)\] &lt;h2 id=&quot;upper-bound-for-latent-space-exploration&quot;&gt;Upper Bound for Latent Space Exploration&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Theorem 2&lt;/strong&gt;. Let \(S={(x_i, y_i)}^m_{i=1}\) be a dataset of m examples drawn i.i.d. according to D. Let M be a subset of [ \(K\) ]. Assuming we have produced the empirical risk minimizers \((\hat{h}_\mathcal{M}, \hat{g}_\mathcal{M})\) training with the M modalities. Then, for all \(1 &amp;gt; \delta &amp;gt; 0$, with probability at least\)1 − \delta$$ :&lt;/p&gt; &lt;/blockquote&gt; \[\eta(\hat{g}_{\mathcal{M}})\leq 4L\mathfrak{R}(\mathcal{H} \circ \mathcal{G}_{\mathcal{M}})+4L\mathfrak{R}(\mathcal{H} \circ \mathcal{G})+6C\sqrt{\frac{2\text{ln}(2/\delta)}{m}}+\hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S)\] \[\text{where } \hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S) \triangleq \hat{r}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}})-\hat{r}(h^\star\circ g^\star)\] &lt;p&gt;위에서 처럼 Rademacher complexity은 \(O(1/m)\) 이기 때문에&lt;/p&gt; \[\eta(\hat{g}_{\mathcal{M}})\leq \hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S)+\text{O}(1/m)\] &lt;p&gt;이 성립한다. 이 때, assumption 3에 의해&lt;/p&gt; \[\hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S) \leq \hat{L}(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}, S)\] &lt;p&gt;이 성립한다.&lt;/p&gt; &lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt; &lt;p&gt;&lt;em&gt;그렇다면 언제 multi-modal을 사용해야하냐?&lt;/em&gt;&lt;/p&gt; \[\hat{L}(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}, S) - \hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S) \geq \sqrt{\frac{C(\mathcal{H}\circ\mathcal{G}_\mathcal{M})}{m}}-\sqrt{\frac{C(\mathcal{H}\circ\mathcal{G}_\mathcal{N})}{m}}\] &lt;p&gt;저자는 다음과 같이 말한다.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;(i) When the number of sample size m is large, the impact of intrinsic complexity of function classes will be reduced. (ii) Using more modalities can efficiently optimize the empirical risk, hence improve the latent representation quality.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Sample size m이 충분히 클 때 Theorem 1에 적용하면 다음과 같은 식이 성립한다.&lt;/p&gt; &lt;p&gt;\(\gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N})= \eta(\hat{g}_{\mathcal{M}}) - \eta(\hat{g}_{\mathcal{N}})\leq \hat{L}(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}, S) - \hat{L}(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}, S) \leq 0\)$&lt;/p&gt; \[r(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}) \leq r(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}})\] &lt;p&gt;즉, 데이터셋의 크기가 클 때 modality의 수가 많은 것을 사용하는 것이 좋다.&lt;/p&gt; &lt;h2 id=&quot;non-positivity-guarantee&quot;&gt;Non-Positivity Guarantee&lt;/h2&gt; &lt;p&gt;sample size s가 클 때 \(\gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N})\) 이 non-positive라는 것을 증명할 수 있다. 이것의 증명은 여기서 다루지 않겠다.&lt;/p&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;p&gt;이제 실험을 보자. Dataset으로는 Interactive Emotional Dyadic Motion Capture (IEMO- CAP) database을 사용했다. 이 데이터셋에는 여러 모달에 대해서 여러 사람이 대화하는 것이 들어있으며 발화자가 누구인지 맞추는 것이 목표이다. 여기에는 Text, Video, Audio 정보가 들어가있다.&lt;/p&gt; &lt;h2 id=&quot;number-of-modalities&quot;&gt;Number of Modalities&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/table1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Modal이 늘어날 수록 정확도가 상승하는 것을 볼 수 있다.&lt;/p&gt; &lt;h2 id=&quot;number-of-samples&quot;&gt;Number of Samples&lt;/h2&gt; &lt;p&gt;위에서 sample의 수가 클 때 multi-modal이 좋다고 했다. 따라서 이를 살펴보자.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/table2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;여기서 볼 수 있듯, sample의 수가 줄어들면 madality의 수가 적을 때 성능이 좋은 경우가 있다.&lt;/p&gt; &lt;h2 id=&quot;quality-of-latent-spaces&quot;&gt;Quality of Latent Spaces&lt;/h2&gt; &lt;p&gt;multi-modal은 latent space quality가 좋다고 했다. 이를 확인해보자.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/table3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Sample의 수와 modal의 수로 비교해도 같은 결과를 낸다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/figure2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;synthetic-data&quot;&gt;Synthetic Data&lt;/h2&gt; &lt;p&gt;실제 데이터에서 sample의 수가 많을 때 multi-modal이 좋다는 것을 확인했다. 인공데이터는 어떨까?&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/table4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;저자가 만든 인공데이터도 같은 모습을 보였다.&lt;/p&gt; </description> <pubDate>Sun, 19 Nov 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/multimodal-vs-unimodal/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/multimodal-vs-unimodal/</guid> <category>multi-modal</category> <category>paper</category> <category>multi-modal</category> <category>paper</category> </item> <item> <title>스타트업 리서치 인턴 후기</title> <description>&lt;h1 id=&quot;왜-시작했나요&quot;&gt;왜 시작했나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/img.jpeg&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; *&apos;Researcher일까? Engineer일까?&apos;* &lt;/p&gt; &lt;p&gt;나는 대학에 다니면서 항상 그런 고민을 했다. 고등학생 때 우연히 DeepMind에서 발표한 Playing Atari with Deep Reinforcement Learning이라는 논문을 보게되었고 인공지능에 빠져들었다. 대학 와서는 computer vision을 공부하게 되었다. 그저 인공지능이 좋아 backend, frontend 등 다른 분야보다는 인공지능 공부와 개발만 하게 되었다. 그러다 대학을 졸업할 때가 되었고, researcher와 engineer를 선택해야 할 순간이 다가왔다.&lt;/p&gt; &lt;p&gt;불행인지 다행인 건지 중앙대학교에서는 인턴을 해야지 졸업을 할 수 있었고, 관심 있는 두 군데 스타트업에 접촉하여 그중 한 회사인 뉴로클에서 인턴을 진행하게 되었다. 뉴로클을 선택한 이유는 간단했다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Computer Vision을 중점으로 한다.&lt;/li&gt; &lt;li&gt;자체 서비스를 판매하고 있다.&lt;/li&gt; &lt;li&gt;기업매출을 보니 매출도 성장세였고, 흑자를 내기 시작했다.&lt;/li&gt; &lt;li&gt;내가 내 일을 할 수 있고, 주체적으로 일할 수 있는 규모가 작지도 않고 크지도 않는 회사이다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;결론적으로 이러한 예측이 맞았고 성공적으로 인턴을 만들 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;무엇을-했나요&quot;&gt;무엇을 했나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/img.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;기본적으로 리서치 인턴의 역할을 수행했으나 후반에는 리서치 엔지니어의 역할을 하게 되었다. 퍼포먼스가 좋아서 그런지 생각보다 많은 일을 하게 되었다. (외부에 공개적으로 자료가 나간 것들만 포함했다)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Pretrained-OCR: OCR Auto-labeling 기능 추가 및 OCR 모델 성능 향상. 학습 속도 기존의 30%, 정확도 4%p 향상&lt;/li&gt; &lt;li&gt;Smart labeling (segmentation): 기술 검토 및 테스트, 모델 변환&lt;/li&gt; &lt;li&gt;Smart labeling (object detection): 기술 검토 및 모델 변환&lt;/li&gt; &lt;li&gt;회사 블로그 제작&lt;/li&gt; &lt;li&gt;리서치팀 docker 등 개발환경 관리&lt;/li&gt; &lt;li&gt;(방향성만 제시했지만) Neuro-I 성능개선, 다른 사람 연구 해결책 찾기 등등…&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;8개월동안 이걸 다 했다고?? 놀랍게도 그렇다. 개발 base로 computer vision을 공부했다 보니 구현 속도와 실험 속도가 압도적으로 빠른 것 같다.&lt;/p&gt; &lt;h1 id=&quot;무슨-경험이-도움이-되었나요&quot;&gt;무슨 경험이 도움이 되었나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/project.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;동아리에서 공모전에서 프로젝트 경험이 많았고 협업도 많이 진행했다. 그래서 computer vision, 선형대수, 수치해석, 위상수학, 표 본론 등 과 같은 지식뿐만 아니라 tensorrt, onnx, quantization 등 많은 기술, pandas, matplotlib, seaborn과 같은 데이터 시각화, 딥러닝 모델이 제품에 어떻게 탑재해야 하는지에 대한 감도 있었다. 이 모든 경험을 회사에서 다 썼다. (진짜 다 썼다) 이러한 다양한 경험은 여러 기능에 기여를 할 수 있었던 것 같다.&lt;/p&gt; &lt;h1 id=&quot;무엇을-얻었나요&quot;&gt;무엇을 얻었나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/company.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;협업&quot;&gt;협업&lt;/h2&gt; &lt;p&gt;학생 때와 차원이 다른 협업을 하게 되었다. 학교에는 기껏해야 backed, fronted고 인원도 적어서 체계도 없이 작업을 해도 되었다. 하지만 인턴을 하면서 backed, fronted뿐만 아니라 영업, 마케팅, backbend, 기획 등 여러 사람과 협업을 진행했다.&lt;/p&gt; &lt;h3 id=&quot;요구사항을-명확하게-하자&quot;&gt;요구사항을 명확하게 하자&lt;/h3&gt; &lt;p&gt;협업은 기본적으로 background가 완전하게 동일하지 않은 사람들끼리 작업을 한다. 따라서 같은 목표를 바라보고있어도 세부 사항이 다를 수 있다. 만약 이를 조정하지 않고 일을 진행하다 보면 다음에 다시 조정하고 어려울뿐더러 비용 역시 많이 발생한다.&lt;/p&gt; &lt;h3 id=&quot;방향성-설정&quot;&gt;방향성 설정&lt;/h3&gt; &lt;p&gt;하나의 기능이 만들어지기 위해서 다음과 같은 과정을 거친다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;새로운 연구 주제로 연구한다.&lt;/li&gt; &lt;li&gt;기획 및 디자인팀에서 제품 탑재 방향을 결정한다.&lt;/li&gt; &lt;li&gt;개발팀에서 제품을 개발한다.&lt;/li&gt; &lt;li&gt;마케팅팀에서 협력사에 제공할 데이터와 대외 홍보용 자료를 제작한다.&lt;/li&gt; &lt;li&gt;영업을 통해 제품을 판매한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;라이프 사이클에서 연구는 최상단을 차지한다. 따라서 첫 단추가 잘못 채워지면 전체적인 제품 방향이 엇나갈 수 있다.&lt;/p&gt; &lt;h3 id=&quot;문서화를-체계적으로-하자&quot;&gt;문서화를 체계적으로 하자&lt;/h3&gt; &lt;p&gt;내가 하는 연구를 follow up 하는 사람은 극소수다. 연구 이후 제품화할 때 조직되는 관련자들은 내가 작성해 놓은 document를 보고 작업을 시작한다. 그래서 진행한 연구와 모델을 제대로 이해시키려면 구조적으로 잘 문서화를 해야 한다.&lt;/p&gt; &lt;h2 id=&quot;지식&quot;&gt;지식&lt;/h2&gt; &lt;h2 id=&quot;tensorflow&quot;&gt;Tensorflow&lt;/h2&gt; &lt;p&gt;나는 지금까지 pytorch를 이용하여 작업을 했다. Tensorflow는 회사 들어와서 거의 처음 쓰게 된 것이다. Tensorflow는 기본적으로 eager mode가 제공되지 않아 코딩하는 것이 힘들었지만 tensorflow와 tensorflow orbit을 익히게 되는 좋은 기회가 되었다.&lt;/p&gt; &lt;h3 id=&quot;논문&quot;&gt;논문&lt;/h3&gt; &lt;p&gt;회사에 들어와서 논문을 진짜 많이 읽었다. 연구에 기반이 되는 논문뿐만 아니라 적용할 만한 최신논문, 기술 리포트, 워크숍 논문 등 다양하게 많이 읽었다. 이를 통해 ViT, active learning, OCR, anomaly detection, super resolution, backbone for edge device, federated learning 등 다양한 분야에 대해 기초지식을 쌓을 수 있었다.&lt;/p&gt; &lt;h3 id=&quot;데이터-분석&quot;&gt;데이터 분석&lt;/h3&gt; &lt;p&gt;기본적으로 데이터가 부족한 상황에서 모델의 성능을 올리는 방법을 고민을 했다. 이 때문에 사용자가 다룰 예상데이터의 특성을 분석하고 데이터에 적합한 방법론을 사용하여 성능을 높일 수 있었다.&lt;/p&gt; &lt;h2 id=&quot;일은-잘했나요&quot;&gt;일은 잘했나요?&lt;/h2&gt; &lt;p&gt;나에 대한 평가가 긍정적인 것을 보면 일을 잘했던 것 같다. 무엇보다도 나랑 같이 인턴을 진행한 분과 잘하는 것이 달라서 서로 시너지가 났던 것 같다. 원래 회사에도 리서치 인턴이 없었는데 인턴 둘이 좋은 선례를 만들어서 앞으로 계속 채용할 예정이다. (사실 면접이 완료되어 다음 리서치 인턴도 정해졌다.)&lt;/p&gt; &lt;h1 id=&quot;앞으로-무엇을-할-것인가요&quot;&gt;앞으로 무엇을 할 것인가요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/todo.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;감사하게도 회사에서 정규직 제안을 받았다. 스타트업 리서치 엔지니어에 가깝지만, 학사 출신으로 연구를 할 수 있다는 것이 흔치 않은 기회이다. 하지만 회사에서 원하는 인재와 내가 가고자 하는 방향이 달랐다. 회사에서 원하는 인재는 generalist이지만 나는 한 분에서 specialist가 되고 싶었다. 분야 또한 일반적인 모델링이 아닌 모델 경량화, hardware optimization, low cost serving 쪽으로 가고 싶다. 그리고 내가 다루어야 하는 target data가 무엇인지 명확하게 정할 수 있는 연구개발을 하고 싶다.&lt;/p&gt; &lt;p&gt;이제 학교에 다시 돌아간다. 8개월 동안 뉴로클 덕분에 좋은 경험을 했고 내 실력도 엄청나게 향상되었다. 이제 4학년 2학기이다. 졸업도 얼마 안 남아서 취업 준비나 대학원 준비를 해야겠지만 DL engineer 쪽 공부도 더욱 열심히 하면서 내가 목표하는 커리어를 만들어 가야겠다.&lt;/p&gt; </description> <pubDate>Tue, 22 Aug 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/startup-research-intern-review/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/startup-research-intern-review/</guid> <category>daily</category> <category>daily</category> </item> <item> <title>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;현재 GPT부터 시작해서 ViT 등 여러 분야에서 attention layer를 사용하고 있다. 하지만 attention layer은 dimension의 제곱에 비례하여 cost가 들어 모델의 병목인 부분이기도 하다. 이에 따라 attention layer를 효율적으로 만드는 시도가 많이 있는데 그 중 하나가 FlashAttention이다. FlashAttention은 tiling과 kernel fusion으로 기존 attention layer대비 2.4배 속도가 향상되었다. 하지만 FlashAttention 또한 기존 GPU의 이론적 성능에 25~40% 정도의 속도밖에 내지 못한다.&lt;/p&gt; &lt;p&gt;저자는 FlashAttention을 분석하던 중 thread block간 work를 partitioning할 때 비효율성을 발견했고, 이로 인해 GPU에서 low-occupancy와 불필요한 memory IO가 일어나는 것을 깨달았다. 따라서 저자는 이를 해결하기 위해 3가지를 제안했다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Output을 바꾸지 않고 non-matmul operation의 FLOPS를 줄인다.&lt;/li&gt; &lt;li&gt;Single head attention일지라도 병렬처리를 하도록 연산 순서를 변경한다.&lt;/li&gt; &lt;li&gt;Thread block내에 warps간 통신을 줄인다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;저자는 위 3가지를 통해 기존 FlashAttention대비 2배 빠른 속도를 달성하고 GPU의 이론적 성능의 50~73%까지 성능을 끌어올렸다.&lt;/p&gt; &lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt; &lt;p&gt;하드웨어 최적화에 관한 논문은 익숙하지 않으니 background까지 꼼꼼하게 읽어보자.&lt;/p&gt; &lt;h2 id=&quot;hardware-characteristics&quot;&gt;Hardware characteristics&lt;/h2&gt; &lt;h3 id=&quot;gpu-performance-cahracteristics&quot;&gt;GPU performance cahracteristics&lt;/h3&gt; &lt;p&gt;GPU는 compute element와 memeory hierarchy를 가지고 있다. Nvidia tensorcore와 같은 최신 GPU compute element는 FP16/BF16과 같은 low-precision에서 matmul operation을 최적화 하고 있다. 반면에 non-matmul operation은 최적화가 되어있지 않아 matmul operation보다 최대 16배가 느리다.&lt;/p&gt; &lt;p&gt;Memeory hierarchy에 관해서는 기본적으로 GPU는 high bandwidth memory (HBM)과 on-chip SRAM (shared memory)를 가지고 있다. A100기준 40~80GB의 HAM은 1.5~2.0TB/s의 bandwidth를 가지고 있고, 108개의 stream multiproccessor는 각각 192KB의 on-chip SRAM을 가지고 있으며 이는 19TB/s의 bandwidth를 가지고 있다. L2 cache도 있으나 이것은 사용자가 컨트롤을 못함으로 논의에서 제외하도록 하자.&lt;/p&gt; &lt;h3 id=&quot;excution-model&quot;&gt;Excution Model&lt;/h3&gt; &lt;p&gt;GPU는 수많은 thread로 구성되어있으며 thread가 모여서 thread block을 구성한다. 이 thread block은 stream multiprocessor (SMs)를 통해 실행된다. Thread block 내에서 thread는 warps이라는 단위로 묶이게 되는데 이 warp들은 공유메모리를 통해 communication을 한다.&lt;/p&gt; &lt;h2 id=&quot;standard-attention-implementation&quot;&gt;Standard Attention Implementation&lt;/h2&gt; &lt;p&gt;기존 attention은 query, key, value들 간의 연산으로 구성된다. Sequence lenght를 N, head dimension을 d라고 하자. Input sequence \(Q, K, V \in \mathbb{R}^{N\times d}\) 에 대해 attention output \(O \in \mathbb{R}^{N \times d}\) 를 계산하기 위해 아래의 식을 이용한다.&lt;/p&gt; \[S=QK^{\intercal}\in \mathbb{R}^{N\times N}\] \[P=\text{softmax}(S)\in\mathbb{R}^{N\times N}\] &lt;p&gt;\(O=PV\in \mathbb{R}^{N\times d}\) 이 때 softmax는 row-wise로 적용하게 된다. Backwardpass는 다음과 같은 과정을 거친다.&lt;/p&gt; \[dV=P^{\intercal}dO\in\mathbb{R}^{N\times d}\] \[dP=dOV^{\intercal}\in\mathbb{R}^{N\times N}\] \[dS=\text{dsoftmax}(dP)\in\mathbb{R}^{N\times N}\] \[dQ=dSK\in\mathbb{R}^{N\times d}\] \[dK=QdS^\intercal\in\mathbb{R}^{N\times d}\] &lt;p&gt;더 자세한 것은 FlashAttention 설명을 참고하면 된다.&lt;/p&gt; &lt;h2 id=&quot;flashattention&quot;&gt;FlashAttention&lt;/h2&gt; &lt;p&gt;자세한 것은 FlashAttention 설명을 참고하기 바란다. &lt;a href=&quot;https://www.wonbeomjang.kr/blog/2023/fastattention/&quot;&gt;FlashAttention 1 포스트&lt;/a&gt;&lt;/p&gt; &lt;h3 id=&quot;forward-pass&quot;&gt;Forward pass&lt;/h3&gt; &lt;p&gt;간단하게 이야기하자면 K,V를 tiling하여 병렬적으로 계산 후 on-line softmax를 통해 병렬적으로 softmax를 적용한다. 이후에 tiling한 Q를 불러와 on-chip연산으로 만든다. 또한 이를 통해 연산을 fusion할 수 있으며 Q, K, V HBM에서 load한 이후 모든 연산을 수행 후 HBM에 저장하게 된다. 연산은 다음과 같고 아래서 표시한 \(S\) 는 \(S=QK^T\) 이다.&lt;/p&gt; \[m^{(1)}=\text{rowmax}(S^{(1)})\in\mathbb{R}^{B_r}\] \[l^{(1)}=\text{rowsum}(e^{S^{(1)}-m^{(1)}})\in\mathbb{R}^{B_r\times B_c}\] \[\tilde{P}^{(1)}=\text{diag}(l^{(1)})^{-1}e^{S^{(1)}-m^{(1)}}\in\mathbb{R}^{B_r\times B_C}\] \[O^{(1)}=\tilde{P}^{(1)}V^{(1)}=\text{diag}(l^{(1)})^{-1}e^{S^{(1)}-m^{(1)}}V^{(1)}\in\mathbb{R}^{B_r\times d}\] \[m^{(2)}=\text{max}(m^{(1)},\text{rowmax}(S^{(2)}))=m\] \[l^{(2)}=e^{m^{(1)}-m^{(2)}}l^{(1)}+\text{rowsum}(e^{S^{(2)}-m})=\text{rowsum}(e^{S^{(1)}-m})+\text{rowsum}(e^{S^{(2)}-m})=l\] \[\tilde{P}^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m^{(2)}}\] \[O^{(2)}=\text{diag}(l^{(1)}/l^{(2)})^{-1}O^{(1)}+\tilde{P}^{(2)}V^{(2)}=\text{diag}(l^{(2)})^{-1}e^{s^{(1)}-m}V^{(1)}+\text{diag}(l^{(2)})^{-1}e^{s^{(2)}-m}V^{(2)}=O\] &lt;p&gt;즉, figure1처럼 vector를 쪼개고, 합치는 과정을 통해 memory IO를 줄여 연산속도를 빠르게 만들었다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h3 id=&quot;backward-pass&quot;&gt;Backward Pass&lt;/h3&gt; &lt;p&gt;Backward pass는 attention 연산을하는 과정에서 \(m, l\) 이 계산되는 되는데 이를 이용하면 다시 연산을 recompute할 수 있다.&lt;/p&gt; &lt;h1 id=&quot;3-flashattention-2&quot;&gt;3. FlashAttention-2&lt;/h1&gt; &lt;p&gt;FlashAttention은 기본적으로 non-matmul FLOPs를 줄인다. 예를들어 Nvidia의 A100 GPU는 FP16/BF16의 matmul 연산은 이론적으로 312 TFLOPs/s의 연산량을 가지지만 non-matmul 연산은 19.5 TFLOPs/s의 연산량을 가진다. 즉 non-matmul 연산이 matmul 연산보다 16배 느려 non-matmul 연산이 전체 연산의 일부를 차지하더라도 이를 최적화 시켜야한다.&lt;/p&gt; &lt;h2 id=&quot;forward-pass-1&quot;&gt;Forward pass&lt;/h2&gt; &lt;p&gt;저자는 FlashAttention에서 on-line softmax를 먼저 주목했다.&lt;/p&gt; &lt;h3 id=&quot;recaling&quot;&gt;Recaling&lt;/h3&gt; &lt;p&gt;기존에는 \(\text{diag}(l^{(2)})^{-1}\) 를 두 항 모두 rescaling 했다.&lt;/p&gt; \[O^{(2)}=\text{diag}(l^{(1)}/l^{(2)})^{-1}O^{(1)}+\tilde{P}^{(2)}V^{(2)}=\text{diag}(l^{(2)})^{-1}e^{s^{(1)}-m}V^{(1)}+\text{diag}(l^{(2)})^{-1}e^{s^{(2)}-m}V^{(2)}=O\] &lt;p&gt;이렇게 한다면 두 텀을 각각 읽어 각각 나눠야되기 때문에 memory IO가 많아진다. 따라서 마지막 결과 \(\tilde{O}^{(last)}\) 를 계산 후에 한꺼번에 \(\text{diag}(l^{(last)})^{-1}\) 으로 rescaling 한다.&lt;/p&gt; \[\tilde{O}^{(2)}=\text{diag}(l^{(1)})^{-1}O^{(1)}+e^{S^{(2)}-m^{(2)}}V^{(2)}\] \[O^{(2)}=\tilde{O}^{(2)}\text{diag}(l^{(2)})^{-1}\] &lt;h3 id=&quot;memorization&quot;&gt;Memorization&lt;/h3&gt; &lt;p&gt;Backward에 사용하기 위해서 \(m, l\) 을 저장한 후 재구성한다고 했다. 각각을 저장하는 대신 \(L^{(j)}=m^{(j)}+\text{log}(l^{(j)})\) 를 저장해도 똑같이 backward를 재구성할 수 있어 \(m, l\) 대신 \(L\) 을 저장하게 된다.&lt;/p&gt; &lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt; &lt;p&gt;결론적으로 flashattention 2에서는 다음과 같은 방법으로 attention을 구현하게 된다.&lt;/p&gt; \[m^{(1)}=\text{rowmax}(S^{(1)})\in\mathbb{R}^{B_r}\] \[l^{(1)}=\text{rowsum}(e^{S^{(1)}-m^{(1)}})\in\mathbb{R}^{B_r\times B_c}\] \[\tilde{O}^{(1)}=e^{S^{(1)}-m^{(1)}}V^{(1)}\in\mathbb{R}^{B_r\times d}\] \[m^{(2)}=\text{max}(m^{(1)},\text{rowmax}(S^{(2)}))=m\] \[l^{(2)}=e^{m^{(1)}-m^{(2)}}l^{(1)}+\text{rowsum}(e^{S^{(2)}-m})=\text{rowsum}(e^{S^{(1)}-m})+\text{rowsum}(e^{S^{(2)}-m})=l\] \[\tilde{P}^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m^{(2)}}\] \[\tilde{O}^{(2)}=\text{diag}(e^{m^{(1)}-m^{(2)}})^{-1}\tilde{O}^{(1)}+e^{S^{(2)}-m^{(2)}}V^{(2)}=e^{S^{(1)-m}}V^{(1)}+e^{S^{(2)}-m}V^{(2)}\] \[O^{(2)}=\text{diag}(l^{(2)})^{-1}\tilde{O}^{(2)}=O\] &lt;p&gt;기존 flashattention과 다르게 term 자체가 줄어들었다. Forward pass에 관한 알고리즘을 정리하자면 다음과 같다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/alg1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;backward&quot;&gt;Backward&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/alg2.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Backward 자체는 \(L\) 을 사용한다는 것 말고는 별 다른 이야기는 없다.&lt;/p&gt; &lt;h2 id=&quot;parallelism&quot;&gt;Parallelism&lt;/h2&gt; &lt;p&gt;기본적으로 GPU는 병렬처리가 가능하다. 각각 gpu thread block마다 1개의 attention module가 들어간다. 따라서 보통 # batch size x # self-attention head로 thread block을 구성하게 되고 이를 stream multiprocessor가 나눠가진다. 그래서 만약 sequence 길이가 길어 small batch size나 small number of self-attention head를 가지게 된다면 병렬처리를 잘 활용하지 못한다. 따라서 저자는 sequence length dimension에 따른 병렬처리를 하게 된다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Forward pass&lt;/strong&gt; 저자는 sequence length dimension으로 병렬처리를 한다. 하지만 이는 한 sequence내에서는 독립적으로 처리되어야함으로 다른 sequence와 통신을 하지 못하도록 구성했다. 물론 이전과 마찬가지로 batch, multi-head간 병렬처리는 유지한다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;backward pass&lt;/strong&gt; &lt;br /&gt; Algorithm 2에 의하면 column block간에 병렬처리만 한다. 위의 경우와 같이 sequence length dimension로도 병렬처리가 가능하여 추가하게 된다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;결과적으로 worker마다 병렬처리가 잘 되게 된다.&lt;/p&gt; &lt;h2 id=&quot;work-partitioning-between-warp&quot;&gt;Work Partitioning Between Warp&lt;/h2&gt; &lt;h3 id=&quot;forward&quot;&gt;Forward&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig3.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;기존의 flashattention은 \(K\) 와 \(V\) 를 각각읜 warp에 K개로 partitioning 했고, \(Q\) 는 모든 warp이 접근 가능하도록 했다. 그리고 이를 “split-K” 라고 한다. 하지만 이러한 방법은 partition된 \(QK^T\) 를 partition된 \(V\) 에 곱하게 된다. 따라서 중간계산결과를 저장하고, 읽고, 동기화를 많이해 IO에서 속도가 느려진다. 따라서 \(Q\) 를 partition하고, \(K, Q\) 를 공유하게 해이런 IO를 줄여 속도를 높이게 된다.&lt;/p&gt; &lt;h2 id=&quot;backward-1&quot;&gt;Backward&lt;/h2&gt; &lt;p&gt;“split-K”를 지양한다라는 것 밖에 이해를 못했다.&lt;/p&gt; &lt;h3 id=&quot;tuning-block-sizes&quot;&gt;Tuning block sizes&lt;/h3&gt; &lt;p&gt;Block size를 늘리면 memory IO의 수가 줄어든다. 하지만 block 수가 많아지면서 registers의 수가 늘어나고, total shared memory 크기가 커져 비효율성이 늘어난다. 많은 registers는 프로그램 속도를 느리게 만들고, total shared memory의 크기가 너무 커지면 GPU memory가 부족하다. 따라서 GPU마다 적절한 block size를 조정한다.&lt;/p&gt; &lt;h1 id=&quot;empirical-validation&quot;&gt;Empirical Validation&lt;/h1&gt; &lt;p&gt;이제 속도를 보자.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig4.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig5.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig6.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig7.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;FlashAttention-2는 기존 FlashAttention, xFormer 대비 2배의 속도를 보여줬고, Triton으로 구현된 FlashAttention보다 1.3~1.5배의 빨라진 속도를 보여줬다. 놀라운 것은 pytorch에서 naive하게 implementation한 것 대비 10배의 속도차이를 보여준다. 이로인해 기존의 large model에서도 더 빠른 연산속도를 보여준다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/table1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; </description> <pubDate>Sun, 06 Aug 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/flashattention-2/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/flashattention-2/</guid> <category>attention</category> <category>hardware-optimization</category> <category>paper</category> <category>attention</category> <category>hardware-optimization</category> <category>paper</category> </item> <item> <title>Fairness-aware Data Valuation for Supervised Learning</title> <description>&lt;h1 id=&quot;들어가기-앞서&quot;&gt;들어가기 앞서&lt;/h1&gt; &lt;p&gt;Active learning과 class imbalance를 찾던 도중 발견한 논문이자. 그래서 FairML 분야는 아는 것이 없고, 이 논문이 좋은지 나쁜지도 판단이 안 된다. 하지만 해당 논문의 개념도 간단하고, 이런 것을 고려하면서 sampling을 하는 것도 좋겠다는 생각에 논문을 정리하고자 한다.&lt;/p&gt; &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;기존의 data valueation 연구는 데이터를 특정한 performace related value로 embedding한다. Active learning에서 “해당 train instance를 추가하면 모델의 성능이 올라가겠다”를 목적으로 entropy를 사용하여 embedding하는 방법도 있다. 하지만 추천시스템에서 해당 value만 사용할 경우 특정 그룹이나 인종에 안 좋은 데이터를 추천해주는 등 여러 안 좋은 점들이 있다. 따라서 fairness도 고려되어야 하는데 기존까지는 performance와 fairness를 동시에 고려하는 연구는 많지 않았다. 저자는 entropy를 기반으로 하여 두 가지 factor를 모두 고려하여 data를 utility value에 embedding하는 framework를 제안하였고 이를 이용하여 더 좋은 data sampling, re-weighting을 할 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;fairness-aware-data-valuation&quot;&gt;Fairness-aware Data Valuation&lt;/h1&gt; &lt;h2 id=&quot;framework&quot;&gt;Framework&lt;/h2&gt; &lt;p&gt;일단 저자는 utility개념을 빌려왔는데, 해당 논문에서 utility는 performance와 fairness를 종합하는 function을 의미한다. Single data instance \(i\) 에 대해 perforamce-related valuation은 \(v_{y_i}\) , protected attribute에 대한 fairness-related valuation은 \(v_{z_i}\) 로 표시한다. Utility function은 다음과 같다.&lt;/p&gt; \[U_i(v_{y_i},v_{z_i})=\alpha(v_{y_i})+(1-\alpha)v_{z_i}\] &lt;p&gt;이 때 \(\alpha \in [0,1]\) 이다. 만약 fairness를 subgroup으로 나눈다면 다음의 식으로 확장할 수 있다.&lt;/p&gt; \[U_i(v_{y_i},v_{z_i})=\alpha(v_{y_i})+\sum_{j=1}^{k}\beta v_{z_{j_i}}\] &lt;h2 id=&quot;entorpy-metric&quot;&gt;Entorpy metric&lt;/h2&gt; &lt;p&gt;먼저 저자는 performace related value를 instance \(i\) 의 prediction \(y_{i}\) 의 entropy로 정의했다.&lt;/p&gt; \[V_{y_i}=E_{y_i}=\hat{y}_i\cdot {log}_2\hat{y}_i+(1-\hat{y}_i)\cdot {log}_2(1-\hat{y}_i)\] &lt;p&gt;해당 수식은 active learning에서 영감을 받았다. Entropy가 높다는 것은 모델이 해당 instance를 잘 예측하지 못한다는 이야기이고, 추후에 이를 집어넣으면 성능이 높아진다는 것을 예상할 수 있다. 하지만 실제 상황에서는 애매한 instance뿐만 아니라 noise 또한 entropy가 높아져서 성능이 더 낮아질 가능성도 있다. 하지만 여러 task에서 해당 방법은 성능이 준수하다는 것으로 나와서 저자는 entropy를 사용했다.&lt;/p&gt; &lt;p&gt;저자는 fairness-related valueation또한 entropy로 정의했다.&lt;/p&gt; \[V_{z_i}=E_{z_i}=\hat{z}_i\cdot {log}_2\hat{z}_i+(1-\hat{z}_i)\cdot {log}_2(1-\hat{z}_i)\] &lt;p&gt;이 수식이 왜 되는지는 이해가 잘 안되지만 저자에 말은 이러하다.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;In the second case, where the variable in question (Z) is not the target for the task at hand, and is seen by the model even at inference time, we prioritize observations where the model had more difficulty in establishing a relationship among X, Y, and Z, leveraging the fact the model has no explicit incentive to draw such relationships. This is directly related to mitigating the base bias condition of the taxonomy of Pombal et al. (2022a) ( P[X, Y] 6= P[X, Y | Z] ), and so related to promoting fairness.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fairness-aware data valuation for supervised learning, Pombal et al, 2023 -&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;p&gt;내가 이해한 대로 적자면 여러 예측 모델이 있는데 이들 모두의 예측값의 entropy가 높다면 bias가 없는 데이터이다. 하지만 특정 모델에서 entropy가 높아 잘 예측한다면 그 데이터는 특정 모델에 적합한 bias가 있는 데이터일 것이다. 따라서 fairness 또한 entropy로 표현할 수 있게 되는 것이다.&lt;/p&gt; &lt;p&gt;이렇게 구한 perforamce related valuation과 fairness related valuation을 종합하여 utility function을 제작하게 된다.&lt;/p&gt; \[U_i=\alpha E_{y_i}+(1-\alpha)E_{z_i}\] &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt; &lt;p&gt;Dataset으로 bank account-opening fraud dataset을 사용했다. 해당 데이터에서 fraud rate는 1%이다. 해당 데이터는 사기계좌를 찾는 것으로 True Positive Rate (TPR)이 높아야한다. 반면에 False Positive Rate (FPR)가 높으면 사용자의 계좌사용이 불편해지기 떄문에 FPR을 낮추는 것을 목표로 하고 있다.&lt;/p&gt; &lt;h1 id=&quot;model&quot;&gt;Model&lt;/h1&gt; &lt;p&gt;Model은 tublar data에서 SOTA를 찍고있는 LightGBM을 사용한다.&lt;/p&gt; &lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt; &lt;p&gt;Data sampling, re-weighting은 다음의 과정을 거친다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;각각의 예측모델 Y, Z에 대해 tranining instance의 value를 계산한다.&lt;/li&gt; &lt;li&gt;각각의 train instance에 대해 utility value를 계산한다.&lt;/li&gt; &lt;li&gt;Utility value를 기반으로 Utility-aware prevalence sampling (UASP) 또는 Utility-aware reweighting (UAR) 수행한다. 이 때 UASP는 under sampling을 이야기하는 것이다.&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/fado/fig1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;결과는 위와 같다. 해당 표를 보는 방법은 모르나 저자말로는 자신이 제안한 framework가 balance를 잘 잡는다고 주장한다.&lt;/p&gt; </description> <pubDate>Wed, 12 Jul 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/fado/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/fado/</guid> <category>data-sampling</category> <category>fair-ml</category> <category>data-sampling</category> <category>fair-ml</category> </item> <item> <title>TinyViT</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;ViT는 많은 발전이 있었지만 edge device에 적용하기에는 모델이 너무 컸다. 하지만 모두들 알다싶이 작은 모델 representation이 작다. 따라서 small dataset에 적합할지 몰라도 large dataset에서는 빠르게 saturation이 되면서 underfitting이 발생하게 되어 잘 사용하지 못한다. 저자는 이에 대하여 고민을 했고 knowledge distillation을 사용해야한다는 결론에 도달하게 된다. 이에따라 small model이 downstream task에 transfer가 잘 되는 것을 확인했다.&lt;br /&gt; 하지만 기존의 knowledge distillation은 train시 teacher network를 메모리에 올리기 때문에 teacher network가 gpu memory를 다수 잡아먹게 되어 batch size 조절이 힘들다. 또한 학습에 필요한 soft-label을 그때그때 제작하기 때문에 학습속도도 느려진다. 따라서 저자는 이를 해결하기 위해 data augmentation과 soft-label을 먼저 저장하여 student model이 학습 시 이를 사용하는 방법을 사용했다. 결론적으로 TinyViT는 21M의 파라미터로 ImageNet에서 84.8% top-1 accuracy를 달성했으며 88M으로 85.8%를 달성한 Swin-B보다 4.2배 적은 파라미터이다. 이미지 크기를 키웠을 때 SOTA를 찍었으며 COCO object detection도 Swin-T 우위에 있다는 것을 확인했다.&lt;/p&gt; &lt;h1 id=&quot;tinyvit&quot;&gt;TinyViT&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;fast-pretraining-distillation&quot;&gt;Fast Pretraining Distillation&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;위 사진에서 볼 수 있듯 small model을 바로 큰 데이터셋에 학습하면 성능이 낮아진다. 따라서 knowledge distillation을 이용하려고 하는데 저자는 downstream task를 위해 finetuning-distillation이 아니라 pretraining distillation에 주목을 한다. 이에 따라 ImageNet-1K에는 distillation없이 finetuning을 이용하여 학습을 진행한다. &lt;br /&gt; 하지만 pretraining distillation은 large teacher model이 많은 데이터셋을 inference 해야 하기 때문에 비용이 많이 들고 비효율적이다. 따라서 Fig.2에서 나오듯 사전에 teacher model에 사용되는 augmentation과 그에 맞는 label을 만들어 스토리지에 저장하게 된다. 이를 통해 student model이 학습 시 teacher model의 forward computation 없이 knoweldge distillation을 할 수 있다.&lt;br /&gt; 수학적으로 표현하자면, input image를 \(x\), RandAugment나 CutMix와 같은 strong augmentation을 \(\mathcal{A}\)라 하자. Teacher 학습시 augmentation \(\mathcal{A}\), techer prediction \(\hat{y}=T(\mathcal{A}(s))\)의 pair인 \((\mathcal{A},\hat{y})\)를 저장한다. 이 때 \(T(\cdot )\)은 teacher model을 의미한다. 그 후 student \(S(\cdot)\)과 cross entropy \(CE(\cdot)\)에 대해 loss 연산을 진행한다.&lt;/p&gt; \[\mathcal{L}=CE(\hat{y},S(\mathcal{A}(x)))\] &lt;h3 id=&quot;sparse-soft-label&quot;&gt;Sparse Soft Label&lt;/h3&gt; &lt;p&gt;Teacher model의 output을 그대로 저장하는 것은 storage를 많이 사용한다. 따라서 저자는 \(\hat{y}\)의 top-K value과 그들의 indices \(\{\mathcal{I}(k)\}_{k=1}^K\)만을 저장하고 나머지는 label smoothing을 이용하여 reconstruct한다. 학습 시 ground truth인 hard label을 사용하지 않고 pseudo label인 soft label만을 사용하여 학습한다.&lt;/p&gt; \[\hat{y}_c = \begin{cases} \hat{y}_{\mathcal{I}(k)} &amp;amp; \text{if} \ c=\mathcal{I}(k)\\ \frac{1-\sum_{k=1}^K\hat{y}_{\mathcal{I}(k)}}{C-K} &amp;amp; \text{otherwise} \end{cases}\] &lt;p&gt;이 때 \(\hat{y}_c\)는recovered teacher logit for student model distillation이라고 하고, \(\hat{y}=[\hat{y}_1, ... ,\hat{y}_c, ... ,\hat{y}C]\)이다. 만약 \(K &amp;lt;&amp;lt; C\)라면 메모리 감소가 크다.&lt;/p&gt; &lt;h3 id=&quot;data-augmentation-encoding&quot;&gt;Data augmentation encoding&lt;/h3&gt; &lt;p&gt;Data augmentation 정보 또한 그대로 저장하면 storage를 많이 사용한다. (rotation degree, crop coordinate 등) 따라서 set of data augmenatation parameter를 \(\mathbf{d}\)라고 하고, encoder \(\mathcal{E}(\cdot)\) 를 사용하여 \(d_0=\mathcal{E}(\mathbf{d})\)로 변환하여 저장한다. 그 후 training process에는 \(\mathcal{E}^{-1}(\cdot)\)을 decoder로사용하여 augmetation을 진행한다.&lt;/p&gt; &lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt; &lt;p&gt;저자는 기본적으로 ViT를 기반으로 모델을 설계했다. 또한 Swin Transformer나 LeViT와 같이 hierarchical한 구조를 채택했다. Patch embedding block은 kernel size 3, stride 2, padding 1인 두 개의 convolution을 사용했다. 하지만 처음부터 끝까지 transformer block을 사용하는 것은 연산적으로 무리가 된다. 따라서 MobileNetV2에서 사용하는 MBConv를 사용하여 stage 1과 downsampling을 구성했다. 또한 MBConv에 residual conntection 또한 적용했다. 마지막 3개 stage 에서는 transformer block을 사용했다. 모든 layer와 block에서 activation function은 요즘 성능이 좋은 GELU를 이용했고, normalization은 conolution layer에는 batch norm, linear layer에는 layer norm을 사용했다.&lt;/p&gt; &lt;h3 id=&quot;contraction-factors&quot;&gt;Contraction factors&lt;/h3&gt; &lt;p&gt;Modern model와 같이 factor를 사용하여 모델의 크기를 조절한다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;\(\gamma_{D_{1-4}}\): embeded demension in four stages&lt;/li&gt; &lt;li&gt;\(\gamma_{N_{1-4}}\): number of block in four stages&lt;/li&gt; &lt;li&gt;\(\gamma_{W_{2-4}}\): window size of last three stages&lt;/li&gt; &lt;li&gt;\(\gamma_{R}\): channel expansion ratio of MBConv&lt;/li&gt; &lt;li&gt;\(\gamma_{M}\): channel expansion MLP of transformer blocks&lt;/li&gt; &lt;li&gt;\(\gamma_{E}\): the dimension of each head in multi-head attention&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;모든 모델이 \(\gamma_{D_{1-4}}\)을 제외하고 같은 factor를 갖는다.&lt;/p&gt; \[\{\gamma_{N_1}, \gamma_{N_1}, \gamma_{N_1}, \gamma_{N_1}\} = \{2, 2, 6, 2\}\] \[\{\gamma_{W_1}, \gamma_{W_1}, \gamma_{W_1}\} = \{7, 14, 7\}\] \[\{\gamma_{R}, \gamma_{M}, \gamma_{E}\} = \{4, 4, 32\}\] &lt;p&gt;Embeded dimensiondm \(\{\gamma_{D_1}, \gamma_{D_2}, \gamma_{D_3}, \gamma_{D_4}\}\)는 TinyViT-21M: {96, 192, 384, 576},TinyViT-11M: {64, 128, 256, 448},TinyViT-5M: {64, 128, 160, 320} 으로 구성했다.&lt;/p&gt; &lt;h1 id=&quot;analysis-and-discussion&quot;&gt;Analysis and Discussion&lt;/h1&gt; &lt;p&gt;작은 모델은 Image21K와 같은 large scale dataset에서는 underfitting이 발생하여 학습이 잘되지 않는다. 그렇다면 다음 두 가지의 의문이 들 수 있다.&lt;/p&gt; &lt;h3 id=&quot;1-어떠한-요소가-small-model의-학습을-방해하는가&quot;&gt;1. 어떠한 요소가 small model의 학습을 방해하는가?&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;이를 확인하려면 IN-21K의 문제점을 알아야한다. Label의 오류와 비슷한 이미지가 다른 label인 경우가 존재한다. 이와 같은 데이터가 전체의 10%로 hard sample이라고 불린다. Large model은 이러한 hard sample을 학습할 수 있자만 small model은 hard sample을 예측하기 어렵고 이는 small model을 large model 보다 낮은 성능으로 이끈다. 이를 증명하기 위해 강력한 모델인 Florence를 이용하여 data마다 florence prediction의 top 5 중에 label이 존재하는지 찾아봤다. 그 결과 ImageNet-21K의 14%에 해당하는 hard sample을 골라냈다. 또한 Florence를 이용하여 TinyViT-21M/Swin-T에 knowledge distllation을 적용했다. 그 결과 small model을 곧바로 ImageNet-21K에 학습하는 것은 성능에서 제약이 존재했고, ImageNet-21K에서 hard sample을 제거하면 1%p 정도 성능향상이 있었다. 놀라운 점은 knowledge distillation이 hard sample의 defact을 줄여줬다.&lt;/p&gt; &lt;h3 id=&quot;2-왜-distillation이-small-model의-large-data-학습에-도움이-되는가&quot;&gt;2. 왜 distillation이 small model의 large data 학습에 도움이 되는가?&lt;/h3&gt; &lt;p&gt;Student model이 teacher model의 지식을 바로 학습할 수 있기 때문이다. Gound truth는 각 물체간 상간관계를 보여주지 못한다. 하지만 teacher model의 inference 값은 그것을 알 수 있다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;위의 그림을 보았을 때도 distillation 없이 TinyViT를 학습했을 때 비슷한 물체간 상관관계가 크지 않았으나 distillation 진행시 상관관계가 크게 나와 teacher model을 제대로 따라할 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;h2 id=&quot;impact-of-pretraining-distillation-on-existing-small-vits&quot;&gt;Impact of pretraining distillation on existing small ViTs&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;앞서 설명을 했 듯 ImageNet-21K로 distillation을 진행한 후 down stream task를 진행했다. 이 학습방법론이 효과가 있을지 다른 가벼운 ViT를 사용하여 실험한 결과 해당 방법론은 효과가 있었다.&lt;/p&gt; &lt;h2 id=&quot;impact-of-pretraining-data-scale&quot;&gt;Impact of pretraining data scale&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Distillation을 pretranining할 때 사용한다. 이 때 사용한 데이터에 따라 성능차이를 확인해보니 데이터가 많아질수록 성능이 좋아졌다.&lt;/p&gt; &lt;h2 id=&quot;impact-of-the-number-of-saved-logits&quot;&gt;Impact of the number of saved logits&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig5.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Soft-label의 용량을 줄이기 위해서 top-K만 저장한다고 했다. K가 늘어날 수록 메모리 사용량은 늘어나나 정확도는 비슷하다. 따라서 이를 밸런스 있게 가져가기 위해서 ImageNet-1K에서는 \(K=10\), ImageNet-21K에서는 \(K=100\)으로 설정했다.&lt;/p&gt; &lt;h3 id=&quot;impact-of-teacher-models&quot;&gt;Impact of teacher models&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Teacher Model을 어떤 것으로 설정하는지에 따라서도 성능차이가 났다. 더 강력하고 좋은 teacher model을 사용하면 성능이 좋아지나 그 만큼 학습시간이 늘어나는 것을 볼 수 있었다.&lt;/p&gt; &lt;h2 id=&quot;result-on-imagenet&quot;&gt;Result on ImageNet&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/fig6.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;위 표에서 볼 수 있듯 TinyViT는 경량화 ViT 중에서 좋은 성능을 내었다.&lt;/p&gt; &lt;h2 id=&quot;transfer-learning-results&quot;&gt;Transfer Learning Results&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table5.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Self-supervised learning으로 MOCO를 사용하여 Linear probe를 하거나 few shot learning을 할 때도 성능이 좋았다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/tinyvit/table6.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;또한 MS COCO를 사용하여 object detection에 사용할 때도 성능이 좋았다.&lt;/p&gt; &lt;h1 id=&quot;comment&quot;&gt;Comment&lt;/h1&gt; &lt;p&gt;사실 TinyViT 자체는 knowedge distillation 내용이라 새로운 것이 별로 없다. 결과들을 미리 저장하는 것 역시 가끔씩 쓰는 트릭이라 크게 중요하지 않다. 해당 논문은 그런 것 보다 small ViT가 ImageNet에서 hard example에 취약하여 이를 골라내는 과정이 중요한 것 같다. 또한 저자는 말하지 않았지만 (몰랐을 수도 있지만) sparse soft label 자체가 teacher model의 noise를 제거하는 역할을 하여 small ViT에 더 좋은 성능을 가져와 주었을 것이다. (비교적 어려운 이미지들은 confidence가 낮은 이미지들의 probability가 noise로 작용하기도 한다.) 끝마치면서 드는 생각은 ImageNet-21K로 pretrain하고 이를 ImageNet-1K로 finetuning하는 것이 workshop에서 맞는 방법이지 않나 싶으면서도 ViT를 생각해보면 그럴수도 있다는 생각이 든다.&lt;/p&gt; </description> <pubDate>Wed, 28 Jun 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/tinyvit/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/tinyvit/</guid> <category>backbone</category> <category>paper</category> <category>cvpr</category> <category>vit</category> <category>efficient-architecture</category> <category>knowledge-distillation</category> <category>backbone</category> <category>paper</category> <category>vit</category> <category>knowledge-distillation</category> </item> <item> <title>EdgeViT</title> <description>&lt;h1 id=&quot;intorduction&quot;&gt;Intorduction&lt;/h1&gt; &lt;p&gt;ViT는 global한 representation을 학습하면서 imagenet banchmark에서 압도적인 성능을 내고있다. 하지만 self-attention이라는 연산의 비용이 커서 inference speed나 power efficiency가 떨어진다. 이를 해결하기 위한 기존의 연구는 크게 다음과 같았다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Spatial Resolution에서 hierarchical architecture을 만들어 연산량을 줄인다.&lt;/li&gt; &lt;li&gt;Locally-grouped self-attention mechanism을 사용한다.&lt;/li&gt; &lt;li&gt;Key, value를 pooling하여 subsampling한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;하지만 이러한 방법은 time cost가 중요한 mobile이나 edge platform에서 충분하지 못하다. 따라서 저자는 다음의 요소를 고려하여 EdgeViT를 설계했다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Inference efficiency&lt;/strong&gt; EdgeViT는 on-device에서 사용할 정도로 가볍고 에너지를 적게 사용해야한다. 기존에는 이를 측정하기위해 FLOPS를 주로 사용했지만 이는 latency과 energy consumption을 제대로 반영하지 못한다. 따라서 FLOPS는 추정치로만 참고하고 실제 mobile device에서 밴치마크를 진행한다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Model size&lt;/strong&gt; 현대 스마트폰은 RAM이 32GB일정도로 메모리량이 충분하다. 따라서 절대적인 모델크기를 고려하는 것은 현대 mobile device에는 적합하지 않아서 이는 크게 고려하지 않는다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Implementation Friendliness&lt;/strong&gt; 현실적으로 implementation을 하기위해서는 ONNX, TensorRT, TorchScript등 기존의 framework와의 호환성을 고려해야한다.&lt;/p&gt; &lt;p&gt;이를 고려하여 저자는 local-global-local bottleneck을 제안했고 이는 energy efficient하고 inference speed도 빠르다.&lt;/p&gt; &lt;h1 id=&quot;local-global-local-bottleneck&quot;&gt;Local-Global-Local-BottleNeck&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/Fig3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;ViT의 성능이 좋은 이유는 global한 representation을 학습하기 유리하기 때문이다. Multi-head self-attention이 이를 가능하게 만들었으니 이를 모사하는 연산을 만드는 것을 필수적이다. 따라서 저자는 factorization을 통해 해당 연산을 3가지로 쪼개 모사했다.&lt;/p&gt; &lt;h2 id=&quot;local-aggregation&quot;&gt;Local Aggregation&lt;/h2&gt; &lt;p&gt;연산을 가볍게 만들기 위해서는 절대적인 연산량을 줄여야 한다. 다행이도 이미지에서는 주변 픽셀은 비슷하다는 inductive bias가 존재한다. 따라서 전체 token에 대하여 attention을 계산하지 않고 주변 픽셀의 정보를 aggregation함으로써 scope를 줄일 수 있다. 따라서 attention 계산에 앞서서 locally proximate tokens의 signals을 integrate한다. 이는 기존 depth-wise separable convolution을 사용한다.&lt;/p&gt; &lt;h2 id=&quot;global-sparse-attention&quot;&gt;Global Sparse Attention&lt;/h2&gt; &lt;p&gt;이제 attention에 대한 scope을 줄였으니 local window를 나타내는 representaion인 delegate token간 attention을 계산하자. 이 때 \(r \times r\) local window의 center값을 나타내는 token을 뽑아서 log-range relation을 계산하기 위해 self-attention을 계산한다. 이때 \(r\) 은 sampling rate을 나타내게 된다.&lt;/p&gt; &lt;h2 id=&quot;local-propagation&quot;&gt;Local Propagation&lt;/h2&gt; &lt;p&gt;이제 global contextual information을 계산했으니 local window에 전파해야한다. 이는 간단하게 transpose convolution으로 구성했다.&lt;/p&gt; &lt;p&gt;위의 3가지 연산을 통해 local-global-local bottleneck을 구성한다. 그리고 이는 다음과 같은 식으로 연결된다. \(X=LocalAgg(Norm(X_{in}))+X_{in}\) \(Y=FFN(Norm(X))+X\) \(Z=LocalProp(GlobalSpaarseAttn(Norm(Y)))+Y\) \(X_{out}=FFN(Norm(Z))+Z\) 이 때 FFN은 fully conntected layer 2개로 구성되어있으며 Normalization은 Layer Normalization을 사용한다.&lt;/p&gt; &lt;h2 id=&quot;model-archtecture&quot;&gt;Model Archtecture&lt;/h2&gt; &lt;p&gt;``&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/Fig2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;전체적인 모델구조는 위와 같다. Spatial resolution을 기준으로 hierachical 구조를 가지고 있다. Downsampling은 1번째만 제외하고 2x2 convolotion을 stride 2로 구성한다. 첫 번째 downsamping은 4x4 convolution은 stride 4로 연산한다. Patch embedding은 요즘 성능이 좋은 relative positional embedding[&lt;a href=&quot;https://arxiv.org/abs/1803.02155&quot;&gt;paper&lt;/a&gt;]을 사용한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/table1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;또한 scalarbility를 위해 3개의 모델 구조로 만든다.&lt;/p&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;p&gt;밴치마크를 위해서 삼성스마트폰을 사용했으며 CPU는 Snapdragon 888을 사용했다. 또한 TorchScript lite를 사용하여 50 step을 기준으로 pytorch에서 제공하는 android benchmarking app을 사용하여 측정했다. 또한 전력을 측정하기 위해서 Monsoon High Voltage Power Monitor울 Snapdragon 888 Hardware Development Kit (HDK8350)과 연결하여 측정했으며, NPU는 범용성이 떨어져서 측정하지 않았다고 한다.&lt;/p&gt; &lt;h2 id=&quot;imagenet&quot;&gt;ImageNet&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/table2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;가벼운 ViT 모델 중에서는 좋은 성능을 보여준다. 하지만 CNN 계열과 비교했을 때는 다소 아쉬운 성능을 보인다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/edgevit/table3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;실험하면서 정확도가 낮을수록 전력이 줄어든다는 것을 발견했고, EdgeViT는 전력대비 정확도를 측정하면 좋은 결과를 낸다&lt;/p&gt; </description> <pubDate>Wed, 28 Jun 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/edgevit/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/edgevit/</guid> <category>backbone</category> <category>paper</category> <category>cvpr</category> <category>vit</category> <category>efficient-architecture</category> <category>backbone</category> <category>paper</category> <category>vit</category> </item> <item> <title>Integral Neural Network</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;이 논문은 CVPR2023에 accept된 논문이고 award 후보에도 올랐다. 주된 초점은 모델 사이즈별로 성능하락 없이 pruning하는 내용인데, 상당히 아이디어도 괜찮고 앞으로 응용가능성도 있어보인다. 하지만 가벼운 네트워크만 실험을 하여 EfficientNet-L과 같은 무거둔 네크워크나 ViT와 같은 self-attention 메커니즘을 사용한 네트워크의 결과는 없다. 따라서 실제로 사용하려면 이론적토대가 더 필요할 것으로 보인다. 이제 논문을 살표보자.&lt;br /&gt; 기존의 DNN은 많은 분야에서 좋은 성능을 냈다. Kolmogorov superposition theorem과 universal approximation theorem에서 DNN은 어떠한 continuous multivariate function이라도 모사할 수 있다고 이야기한다. 이러한 이론에 따라서 DNN은 발전했고 파라미터 수도 많아졌다. 연구자들을 이를 극복하기 위해 경량화 기법으로 pruning, quantization, NAS를 사용하여 모델의 크기를 줄였다. 하지만 이와 같은 방법은 모델 사이즈가 줄어듬에 따라 성능하락이 발생했고 각각의 사이즈의 모델을 따로 학습시켜야한다는 단점이 있다. 따라서 저자는 neural network에서 사용하는 discrete한 representaiton을 continuous representation으로 바꾸어 inference시 quadrature approximation procedure를 통해 여러 크기의 모델을 만들 수 있도록 제안했다. 따라서 기존에 있는 CNN, FC 연산과 같은 discrete operation을 integral operator로 교체하는 과정을 거치게 된다.&lt;/p&gt; &lt;h1 id=&quot;neural-networks-and-integral-operators&quot;&gt;Neural Networks and Integral Operators&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/fig2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;설명을 하기 앞서 integral operator에 대한 설명을 먼저 하겠다.\(W(x),S(x)\) 가 unitvariate function이라고 하자. 이 때 다음과 같은 식이 성립한다.&lt;/p&gt; \[\int_0^1 W(x)S(x)dx \approx \sum_{i=0}^nq_iW(x_i)S(x_i)=\vec{w_q} \cdot \vec{s}\] &lt;p&gt;이때 다음과 같은 식이 성립한다.&lt;/p&gt; \[\vec{w_q}=(q_0W(x_0),...,q_nW(x_n))\] \[\vec{s}=(S(x_0),...,S(x_n))\] \[\vec{P}^x=(x_0,...,x_n), 0 = x_0 &amp;lt; x_1 &amp;lt; ... &amp;lt; x_{n-1} &amp;lt; x_n = 1\] &lt;p&gt;위 식은 “두 univariate function의 곱의 적분은 수치적분을 이용한 두 벡터의 내적에 근사한다”는 것을 의미한다. 이\((P, q)\) 쌍은 a numerical integration method라고 부른다. 간단하게 생각하자면 고등학교 때 배운 정적분과 부분 적분의 관계를 떠올리면 된다.&lt;/p&gt; &lt;h2 id=&quot;dnns-layers-as-integral-operators&quot;&gt;DNNs layers as integral operators&lt;/h2&gt; &lt;p&gt;기본적인 이론 토대를 만들었으니 이제 어떻게 적용할 수 있는지 보자.&lt;/p&gt; &lt;h3 id=&quot;convolution-or-cross-correlation-layer&quot;&gt;Convolution or cross-correlation layer&lt;/h3&gt; &lt;p&gt;\(\mathbf{x^s}\) 는 dimension을 표현하는 scalar 혹은 vector라고 정의하자. Convolution layer는 multi-channel을 다루기 때문에 이를 반영해야 한다. Convolution의 continuous operation은 integral로 정의되므로 다음과 같은 식을 따른다.\(\lambda\) 는 trainable parameter를 의미한다.&lt;/p&gt; &lt;p&gt;일단 Convolution weight, Input, Output을 다음과 같이 표현하자.&lt;/p&gt; \[F_W(\lambda,x^{out},x^{in}, \mathbf{x^s}), F_I(x^{in}, \mathbf{x^s}), F_O(x^{out}, \mathbf{x^{s^\prime}})\] &lt;p&gt;Convolution operation을 Integral operator로 다음과 같이 표현할 수 있다.&lt;/p&gt; \[F_O(x^{out},x^{s^\prime})=\int_\Omega F_W(\lambda,x^{out},x^{in}, \mathbf{x^s})F_I(x^{in}, \mathbf{x^s}+\mathbf{x^{s^\prime}})dx^{in}d\mathbf{x^s}\] &lt;h3 id=&quot;fully-connected-layer&quot;&gt;Fully-connected layer&lt;/h3&gt; &lt;p&gt;Linear layer는 기본적으로 matrix multiplication 연산으로 이루어져있으며 vector에서 vector로의 변환 연산이다. 또한 이는 1차원 연산이기 때문에 FC weight, input, output을 다음과 같이 정의한다.&lt;/p&gt; \[F_W(\lambda,x^{out},x^{in}), F_I(x^{in}), F_O(x^{out})\] &lt;p&gt;그리고 FC 연산을 다음과 같이 정의한다.&lt;/p&gt; \[F_O(x^{out})=\int_0^1 F_W(\lambda,x^{out},x^{in})F_I(x^{in})dx^{in}\] &lt;h3 id=&quot;pooling-and-activation-functions&quot;&gt;Pooling and Activation Functions&lt;/h3&gt; &lt;p&gt;Pooling 연산은 간단하게 정의된다. Average pooling은 constant function을 이용한 convolution 연산으로 정의되고, max pooling은 signal discretization으로 정의할 수 있다. 또한 activation function은 discrete한 representation에서 적용하면 되는데 그 이유는 다음의 식이 성립하기 때문이다.&lt;/p&gt; \[\mathcal{D}(ActFunction(x),P_x)=ActFunction(\mathcal{D}(x,P_x))\] &lt;p&gt;\(\mathcal{D}\) 는 주어진 partition\(P_x\)에 대해 discretization operation을 말하는 것이다. 즉, Continuous signal의 activate function을 discretizing한 것은 discretized signal에 activation function을 적용한 것과 동일하다는 관계식이 성립한다.&lt;/p&gt; &lt;h3 id=&quot;evaluation-and-backpropagation-through-integration&quot;&gt;Evaluation and backpropagation through integration&lt;/h3&gt; &lt;p&gt;Integral Neural Network (INN)은 빠른 evalution을 위하여 integral kernel을 discretization하는 과정을 거치게 된다. 이를 통해 기존의 conventional layer에 weight을 전달할 수 있고, pytorch와 같은 framework나 GPU와 호환이 된다. Backpropagation은 기존과 같은 chain-rule이 사용된다. 이는 Appendix A에 설명이 들어가있는데 간단하게 lemma만 보자면 다음과 같다.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt; (Neural Integral Lemma) Given that an integral kernel\(F(λ, x)\) is smooth and has continuous partial derivatives\(\frac{\partial(\lambda,x)}{\partial\lambda}\) on the unit cube\([0, 1]^n\) n, any composite quadrature can be represented as a forward pass of the corresponding discrete operator. The backward pass of the discrete operator corresponds to the evaluation of the integral operator with the kernel\(\frac{\partial(\lambda,x)}{\partial\lambda}\) using the same quadrature as in the forward pass.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;continuous-parameters-representation&quot;&gt;Continuous parameters representation&lt;/h2&gt; &lt;p&gt;더 풍부하고 일반화된 continuous parameter representation을 위해서 inference time에 어떠한 해상도(sampling rate)로든 sampling을 하야한다. 따라서 저자는 continuous한 weight을 [0, 1]에서 존재하는 line segment에 interpolation kernel의 linear combination으로 정의한다. 따라서 다음과 같이 나타낼 수 있다.&lt;/p&gt; \[F_W(\lambda,x)=\sum_{i=0}^m\lambda_i u(xm-i)\] &lt;p&gt;여기서\(m$과\)n$$은 interpolation node의 개수와 그들의 값이다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/fig4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;설명이 어렵게 되어있지만 개념은 간단하다. 저자들은 kernel wieght를 각 [0, 1] 사이의 균일한 segment로 저장하고 이를 interpolation을 통해 continuous한 kernel과 representaion을 제작한다. 이때 cubic spline interpolation을 사용하는데 이는 GPU상에서 빠르지만 linear interpolation보다 더 정확한 정보를 담을 수 있기 때문이다. 따라서 fully-connected layer의 weight는 다음과 같이 저장된다.&lt;/p&gt; \[F_W(\lambda,x^{out},x^{in})=\sum_{i,j}\lambda_{i,j}u(x^{out}m^{out}-i)u(x^{in}m^{in}-j)\] &lt;p&gt;또한 evaluation을 위해 discrete하게 export 할 때 다음과 같이 export 하게 된다.&lt;/p&gt; \[W_q[k,l]=q_lW[k,l]=q_lF_W(\lambda,P_k^{out},P_l^{in})\] \[\vec{P}^{out}=\{kh^{out}\}_k, \vec{P}^{in}=\{lh^{in}\}_k\] &lt;h3 id=&quot;trainable-partition&quot;&gt;Trainable partition&lt;/h3&gt; &lt;p&gt;저자는 처음에 fixed sampling step으로 uniform한 partition을 만들 생각이었다. 하지만 non-uniform한 sampling이 partition size를 키우지 않고 numerical integration을 향상시킬 수 있다는 것을 발견했다. 따라서 trainable한 partition을 도입해 자유도를 늘렸으며 이를 통해 좀 더 smooth하고 효율적인 partition을 할 수 있게 되었다. 후술하겠지만 이는 새로운 pruning 방법에 쓰이게된다. 따라서 partition parameterization\(\vec{P}\) 는 다음과 같은 식을 따르게 된다.&lt;/p&gt; \[\vec{\delta}_{norm}=\frac{\vec{\delta}^2}{sum(\vec{\delta}^2)}, \vec{P}=cumsum(\vec{\delta}_{norm})\] &lt;h1 id=&quot;training-integral-neural-networks&quot;&gt;Training Integral Neural Networks&lt;/h1&gt; &lt;p&gt;딥러닝 방법론이 많아지면서 현재는 ResNet과 같은 좋은 network가 존재한다. 따라서 이를 활용한다면 INN에 좋은 initialization이 될 수 있다. 따라서 저자들은 기존 discrete network를 smooth structure로 만들기 위해 weight를 permute하는 방법론을 제시했다.&lt;/p&gt; &lt;h3 id=&quot;conversion-of-dnns-to-inns&quot;&gt;Conversion of DNNs to INNs&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/fig5.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;network를 가능하면 smooth한 structure로 만들기 위해서 weight tensor의 특정방향의 total variation이 최소가 되도록 만들어아햔다. 이 문제는 많이 알려진 Traveling Salesman Problem (TSP)문제로 환원될 수 있다. 이 task에서는\(c^{out}\) dimension에 따라 weight tensor는 city로 대응되고 distance는 total variance로 대응된다. 따라서 optimal permutation은 route로 대응되어 다음 식을 최소화 하는 것으로 문제를 해결하게 된다.&lt;/p&gt; \[min_{\sigma \in S_n}\sum|W[\sigma(i)]-W[\sigma(i+1)]|\] &lt;p&gt;\(W\) 는 weight tensor,\(\sigma\) 는 permutation,\(\sigma(i)\) 는 i-th element의 새로운 위치이다.&lt;/p&gt; &lt;h3 id=&quot;optimization-of-continuous-weights&quot;&gt;Optimization of continuous weights&lt;/h3&gt; &lt;p&gt;INN은 보통의 gradient descent-based method를 사용할 수 있으며 Lemma 1을 사용하여 다음의 학습 알고리즘으로 학습을 진행하게 된다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/algorithm1.png&quot; width=&quot;60%&quot; /&gt; &lt;/p&gt; &lt;p&gt;또한 매 iteration마다 partition size가 달라질 수 있기 때문에 다음과 같은 식을 objective로 설정하여 다른 cube partition간 차이를 최소화한다.&lt;/p&gt; \[|Net(X,P_1)-Net(X,P_2)|\leq|Net(X,P_1)-Y|+|Net(X,P_2)-Y|\] &lt;h1 id=&quot;expertimant&quot;&gt;Expertimant&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/pipline.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;실험 시나리오는 3개로 설정했다.&lt;/p&gt; &lt;h2 id=&quot;pipeline-a-comparison-with-discrete-nns&quot;&gt;Pipeline A. Comparison with discrete NNs&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/table1.png&quot; width=&quot;60%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Discrete 모델을 변환하여 finetuning한 INN이 discrete 모델보다 성능이 비슷하거나 더 좋았다. 하지만 scratch model은 성능이 안 좋았는데 이는 batch normalization을 사용하지 않아서 그렇다고 한다. Super Resolution에서도 비슷한 결과가 나왔다.&lt;/p&gt; &lt;h2 id=&quot;pipeline-b-structured-pruning-without-fine-tuning-through-conversion-to-inn&quot;&gt;Pipeline B. Structured pruning without fine-tuning through conversion to INN&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/table2.png&quot; width=&quot;60%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Section 4에서 partitioning을 finetuning할 수 있다고 했다. 따라서 DNN을 INN으로 변환할 때 partition tuning 유무에 따라 성능비교를 했을 때 partition tuning을 한 모델이 성능이 좋은 것을 알 수 있다.&lt;/p&gt; &lt;h2 id=&quot;pipeline-c-structured-pruning-without-fine-tuning-of-discrete-nns&quot;&gt;Pipeline C. Structured pruning without fine-tuning of discrete NNs&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/integral-neural-network/fig1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;기존의 pruning 방법과도 비교해봤다. 그 결과 INN을 통해서 pruning하는 것이 성능하락이 적었으며 몇 개의 데이터로만 partition tuning을 했을 때 성능하락이 가장 적은 것을 알 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;comment&quot;&gt;Comment&lt;/h1&gt; &lt;p&gt;합리적이고 흥미로운 논문인 것 같다. Training method를 보았을 때 DNN과 INN의 변환이 계속 일어나 학습시간이 느릴 수 있으나 기존 대형모델을 INN으로 만들어 finetuning한 후 크기별로 export하여 많은 device에 사용할 수 있을 것 같다. 하지만 비교적 가벼운 모델을 위주로 실험하고 ViT 계열의 실험은 안들어가있어 실제로 이를 원래 목적대로 사용할 수 있을지는 의문이다. 이론적 토대가 더 만들어진다면 임팩트가 있는 방법론이 되지 않을까 싶다.&lt;/p&gt; </description> <pubDate>Wed, 21 Jun 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/integral-neural-network/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/integral-neural-network/</guid> <category>backbone</category> <category>paper</category> <category>cvpr</category> <category>vit</category> <category>backbone</category> <category>paper</category> <category>vit</category> </item> <item> <title>Invariant Representation for Unsupervised Image Restoration</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;Image restortation은 대부분 pair한 dataset이 필요하다. 하지만 이를 구하는 것은 어려워서 CycleGAN을 이용하여 Image Restoration을 진행하는 경우가 있다. 하지만 CycleGAN과 같은 Image to Image translation과 DIRT와 같은 unsupervised domain adaptation은 다음과 같은 단점을 가지고 있다.&lt;/p&gt; &lt;h3 id=&quot;indistint-domain-boundary&quot;&gt;Indistint Domain Boundary&lt;/h3&gt; &lt;p&gt;Horse to zebra와 같이 image translation은 분명한 domain boundary가 존재한다. 하지만 image restoration은 noise level과 복잡한 backbond가 domain boundary를 희미하게 만들어서 이미지 퀄리티가 낮아진다.&lt;/p&gt; &lt;h3 id=&quot;weak-representation&quot;&gt;Weak Representation&lt;/h3&gt; &lt;p&gt;Unsupervised Domain Adaptation은 high-level representation만 추출한다. 이는 domain shift problem을 야기하여 low-quality reconstruction을 만들어낸다.&lt;/p&gt; &lt;h3 id=&quot;poor-generalization&quot;&gt;Poor Generalization&lt;/h3&gt; &lt;p&gt;One-to-one image translation은 semantic representation과 texture representation을 분리하여 잡아내기 힘들다.&lt;/p&gt; &lt;p&gt;따라서 이 논문에서 위의 문제를 해결하며 다음의 contribution을 남겼다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Image restoration분야에서 unsuperviesd representation learning method를 제안했다.&lt;/li&gt; &lt;li&gt;Dual domain constraint를 통해 semantic representation과 texture representation; 두 개의 representation을 분리했다.&lt;/li&gt; &lt;li&gt;Domain transfer를 통해 unsupervised image restoration을 제안했다.&lt;/li&gt; &lt;/ol&gt; &lt;h1 id=&quot;the-proposed-method&quot;&gt;The Proposed Method&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;먼저 문제에 대한 정의를 하겠다. \(\mathcal{X}\)를 Noisy Image Domain, \(\mathcal{Y}\)를 Clean Image Domain이라고 하겠다. Encoder는 각각의 도메인을 같은 vector space인 shared-latent space \(\mathcal{Z}\)로 projection 시킨다. 따라서 vector space에 대하여 다음의 식이 성립한다.&lt;/p&gt; \[z=E_\mathcal{X}(x)=E_\mathcal{Y}(y)\] &lt;p&gt;Generator는 shared-latent space \(\mathcal{Z}\)에서 image를 만들어낸다. 따라서 다음과 같은 식이 성립한다.&lt;/p&gt; \[x=G_\mathcal{X}(z),y=G_\mathcal{Y}(z)\] &lt;p&gt;이 때 각각의 도메인에대해 Encoder와 Generator는 \(\{E_\mathcal{X}, G_\mathcal{X}\}, \{E_\mathcal{Y}, G_\mathcal{Y}\}\) 각각 존재한다. 각각의 encoder가 shared-latent space \(\mathcal{Z}\)로 projection을 시킨다고 하더라도 각각의 latent vector는 다르다. 따라서 latent vector를 구분하여 적어주겠다.&lt;/p&gt; \[z_\mathcal{X}=E_\mathcal{X}(x), z_\mathcal{Y}=E_\mathcal{Y}(y)\] &lt;p&gt;따라서 우리가 하고싶어하는 Image restoration과정은 다음과 같다.&lt;/p&gt; \[F^{\mathcal{X}\rightarrow\mathcal{Y}}(x)=G_\mathcal{Y}(z_\mathcal{X})\] &lt;h2 id=&quot;discrete-representation-learning&quot;&gt;Discrete Representation Learning&lt;/h2&gt; &lt;p&gt;먼저 one-to-one image translation의 poor generalization 문제를 해결하기 위해 semantic representation과 texture(noise) representation을 분리시켜야한다. 따라서 저자는 다음 4가지의 방법론을 제시했다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Detangling Representation&lt;/li&gt; &lt;li&gt;Forward Cross Translation&lt;/li&gt; &lt;li&gt;Backward Cross Reconstruction&lt;/li&gt; &lt;li&gt;Adversarial Domain Adaptation&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;detangling-representation&quot;&gt;Detangling Representation&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled%201.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;먼저 extra noise encoder(\(E_\mathcal{X}^N\))을 도입을 한다. \(E_\mathcal{X}^N\)은 noise를 나타내는 texture latent vector를 뽑아내는 역할로 이를 도입해서 semantic representation과 texture representation을 분리했다. 이를 통해 \(z_\mathcal{X}$와\)z_\mathcal{Y}\(는 같은 distribution을 가지게 된다. 만약 noise image를 self-reconstruction하려면\)x=G_\mathcal{X}(z_\mathcal{X}, z_\mathcal{X}^N)$$$을 통하여 같이 reconstruction하면 된다.&lt;/p&gt; &lt;h3 id=&quot;forward-cross-translation&quot;&gt;Forward Cross Translation&lt;/h3&gt; &lt;p&gt;CycleGAN처럼 noise image에서 clean image 변환과 clean image에서 noise이미지의 변환이 되어야한다. 따라서 다음과 같은 방법으로 이미지 변환을 한다. 이 때 \(\mathcal{Y}\)에 \(\mathcal{X}\)의 noise를 추가하기 위해 \(z_\mathcal{X}^N\)를 이용한다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Noise image → clean image: \(\tilde{x}^{\mathcal{X}\rightarrow\mathcal{Y}} =G_\mathcal{Y}(z_\mathcal{X})\)&lt;/li&gt; &lt;li&gt;Clean image → noise image: \(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}} =G_\mathcal{X}(z_\mathcal{Y}\oplus z_\mathcal{X}^N)\)&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;backward-cross-translation&quot;&gt;Backward Cross Translation&lt;/h3&gt; &lt;p&gt;Forward cross translation을 했으니 backward cross translation을 할 수 있다. 이 때 \(\mathcal{X}\)에 \(\mathcal{Y}\)의 noise를 추가하기 위해 \(E_\mathcal{X}^N(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}})\)$를 이용한다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Noise image → clean image: \(\hat{x}=G_\mathcal{X}(E_\mathcal{Y}(\tilde{x}^{\mathcal{X}\rightarrow\mathcal{Y}})\oplus E_\mathcal{X}^N(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}}))\)&lt;/li&gt; &lt;li&gt;Clean image → noise image: \(\hat{y}=G_\mathcal{Y}(E_\mathcal{X}(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}}))\)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Backward cross translatio를 학습하기 위해 loss를 다음과 같이 구성한다.&lt;/p&gt; \[\mathcal{L}_\mathcal{X}^{CC}(G_\mathcal{X},G_\mathcal{Y},E_\mathcal{X},E_\mathcal{Y},E_\mathcal{X}^N)=\mathbb{E}_\mathcal{X}[||G_\mathcal{X}(E_\mathcal{Y}(\tilde{x}^{\mathcal{X}\rightarrow\mathcal{Y}})\oplus E_\mathcal{X}^N(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}}))-x||_1]\] \[\mathcal{L}_\mathcal{Y}^{CC}(G_\mathcal{X},G_\mathcal{Y},E_\mathcal{X},E_\mathcal{Y},E_\mathcal{X}^N)=\mathbb{E}_\mathcal{X}[||G_\mathcal{Y}(E_\mathcal{X}(\tilde{y}^{\mathcal{Y}\rightarrow\mathcal{X}}))-y||_1]\] &lt;h3 id=&quot;adversarial-domain-adaptation&quot;&gt;Adversarial Domain Adaptation&lt;/h3&gt; &lt;p&gt;Semantic representation (\(z_\mathcal{X}, z_\mathcal{Y}\))은 같은 vector space를 사용해야한다. 따라서 이를 강제하기 위해서 reprenentation discriminator \(D_r\)를 사용한다.&lt;/p&gt; \[\mathcal{L}^\mathcal{R}_{adv}(E_\mathcal{X},E_\mathcal{Y},D_\mathcal{R})=\mathbb{E}_\mathcal{X}[\frac{1}{2}logD_\mathcal{R}(z_\mathcal{X}+\frac{1}{2}(1-logD_\mathcal{R}(z_\mathcal{X})))] + \mathbb{E}_\mathcal{Y}[\frac{1}{2}logD_\mathcal{R}(z_\mathcal{Y}+\frac{1}{2}(1-logD_\mathcal{R}(z_\mathcal{Y})))]\] &lt;h2 id=&quot;self-supervised-constraint&quot;&gt;Self-Supervised Constraint&lt;/h2&gt; &lt;h3 id=&quot;background-consistency-loss&quot;&gt;Background Consistency Loss&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled%202.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;복잡한 background의 Indistint Domain Boundary을 해결하기 위해 backgound consistency loss를 제안한다. Noise가 있는 이미지와 깨끗한 이미지는 gaussian blur를 하면 structure 정보만 남기 때문에 background의 비교가 가능하다. 따라서 저자는 다음과 같은 loss를 추가한다.&lt;/p&gt; \[\mathcal{L}_{BC}=\sum_{\sigma=5,9,15} \lambda_\sigma||B_\sigma(\mathcal{X})-B_\sigma(\tilde{\mathcal{X}})||_1\] &lt;h3 id=&quot;semantic-consistency-loss&quot;&gt;Semantic Consistency Loss&lt;/h3&gt; &lt;p&gt;perception loss에서 영감을 받아 pretrained-backbone을 통한 semantic한 정보는 noise가 줄어들 것이라고 기대가 된다. 따라서 VGG19에서 conv5-1 layer와 같이 깊은 layer에서 feature map을 뽑아 비교하여 semantic representations의 consistency를 유지한다.&lt;/p&gt; \[\mathcal{L}_{SC}=||\phi_l(\mathcal{X}-\phi_l(\tilde{\mathcal{X}})||_2^2\] &lt;h2 id=&quot;joint-optimizing&quot;&gt;Joint Optimizing&lt;/h2&gt; &lt;p&gt;좋은 성능을 위하여 다른 여러가지도 추가했다.&lt;/p&gt; &lt;h3 id=&quot;target-domain-adversarial-loss&quot;&gt;Target Domain Adversarial Loss&lt;/h3&gt; &lt;p&gt;Noise Domain과 clean image domain에서 결과물을 더 잘만들기 위해 GAN loss를 추가한다.&lt;/p&gt; \[\mathcal{L}_{adv}^\mathcal{X}=\mathbb{E}_{x\sim P_\mathcal{X}(x)}[logD_\mathcal{X}(x)]+\mathbb{E}_{y\sim P_\mathcal{Y}(y), x \sim P_\mathcal{X}(x)}[log(1-D_\mathcal{X}(G_\mathcal{X}(E_\mathcal{Y}(y), E_\mathcal{X}^N(x))))]\] \[\mathcal{L}_{adv}^\mathcal{Y}=\mathbb{E}_{y\sim P_\mathcal{Y}(y)}[logD_\mathcal{Y}(y)]+\mathbb{E}_{x \sim P_\mathcal{Y}(y)}[log(1-D_\mathcal{Y}(G_\mathcal{Y}(E_\mathcal{X}(x)))]\] &lt;h3 id=&quot;self-reconstruction-loss&quot;&gt;Self Reconstruction Loss&lt;/h3&gt; &lt;p&gt;안정적인 학습 진행을 위해서 self reconstruction loss도 추가해였다.&lt;/p&gt; \[\hat{x}=G_\mathcal{X}(E_\mathcal{X}(x)\oplus E_\mathcal{X}^N(x)), \hat{y}=G_\mathcal{Y}(E_\mathcal{Y}(y))\] \[\mathcal{L}^\mathcal{X}_{rec}=||\hat{x} - x||_1, \mathcal{L}^\mathcal{Y}_{rec}=||\hat{y} - y||_1\] &lt;h3 id=&quot;kl-divergence-loss&quot;&gt;KL Divergence Loss&lt;/h3&gt; &lt;p&gt;Noise는 보통 normal distribution을 따른다. 따라서 이 논문에서도 latent-vector가 normal distribution을 따르도록 KL divergence loss를 추가했다.&lt;/p&gt; \[p(z_\mathcal{X}^N\sim N(0, 1))\] &lt;h2 id=&quot;total-loss&quot;&gt;Total Loss&lt;/h2&gt; &lt;p&gt;모든 loss를 합치면 다음과 같다.&lt;/p&gt; \[\underset{E_\mathcal{X},E_\mathcal{X}^N,E_\mathcal{Y},G_\mathcal{X},G_\mathcal{Y}}{\operatorname{min}} \underset{D_\mathcal{X},D_\mathcal{Y},D_\mathcal{R}}{\operatorname{max}} =\lambda_\mathcal{R}\mathcal{L}^\mathcal{R}_{adv}+\lambda_{adv}\mathcal{L}^{domain}_{adv}+\lambda_{CC}\mathcal{L}^{CC}+\lambda_{rec}\mathcal{L}^{Rec}+\lambda_{bc}\mathcal{L}^{BC}+\lambda_{sc}\mathcal{L}^{SC}+\lambda_{KL}\mathcal{L}^{KL}\] &lt;h2 id=&quot;restoration&quot;&gt;Restoration&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled%203.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;학습이 끝난 후에 noise가 있는 이미지를 보구언하려면 cross encoder-generator $${ E_\mathcal{X}, E_\mathcal{Y}}$ 를 사용하면 된다.&lt;/p&gt; \[\tilde{x}^{\mathcal{X}\rightarrow \mathcal{Y}}=G_\mathcal{Y}(E_\mathcal{X}(x))\] &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;p&gt;성능은 unsupervised방법들 중에선 성능이 좋다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled%204.png&quot; width=&quot;70%&quot; /&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled%205.png&quot; width=&quot;70%&quot; /&gt; &lt;img src=&quot;/assets/post/image/unsupervised-image-restoration/Untitled%206.png&quot; width=&quot;70%&quot; /&gt; &lt;/p&gt; </description> <pubDate>Sat, 29 Apr 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/unsupervised-image-restoration/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/unsupervised-image-restoration/</guid> <category>image-restoration</category> <category>unsupervised-learning</category> </item> <item> <title>DINE: Domain Adaptation from Single and Multiple Black-box Predictors</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;정제된 데이터가 많이 있지만 새로운 도메인에 접근하려면 labeling을 진행하기 힘든 경우가 많다. 따라서 이를 위한 Unsupervised Domain Adaption(UDA) 연구가 진행되고있는다. 하지만 UDA의 문제점이 존재한다.&lt;/p&gt; &lt;h3 id=&quot;개인정보&quot;&gt;개인정보&lt;/h3&gt; &lt;p&gt;의료분야와 같은 개인정보로 인해 Source data에 접근이 힘든 경우가 있어 UDA시 이를 활용할 수 없을 수 있다. 또한 GAN을 이용하는 방법에서는 source data가 재생산 가능하여 또 다시 개인정보 문제가 나타날 수 있다.&lt;/p&gt; &lt;h3 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h3&gt; &lt;p&gt;보통의 UDA는 같은 네트워크 구조에서 진행한다. 하지만 사용자의 환경에 따라 경량화 네트워크를 쓸 수도 있기 때문에 다른 네트워크 구조에서도 작동할 수 있어야한다.&lt;/p&gt; &lt;p&gt;저자는 기존의 UDA는 위의 두 가지 문제점을 가지고 있다고 판단하여 source model이 완전히 black-box인 상황에서도 작동하는 DINE을 제시했다.&lt;/p&gt; &lt;h1 id=&quot;methodology&quot;&gt;Methodology&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/dine/Untitled.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Source Domain: \(n_s\) 개의 data, \(x_s^i \in \mathcal{X}_s, y_s^i \in \mathcal{Y}_s\) 에 대하여 $${x^i_s, y^i_s}^{n_s}_{i=1}$&lt;/li&gt; &lt;li&gt;Target Domain: \(n_t\) 개의 data, \(x_t^i \in \mathcal{X}_t, y_t^i \in \mathcal{Y}_t\) 에 대하여 $${x^i_t, y^i_t}^{n_t}_{i=1}$저저자&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;저자는 \(\mathcal{Y}_s=\mathcal{Y}_t\) 또는 \(\mathcal{Y}_s \supset \mathcal{Y}_t\) 인 경우를 다루고 있다.&lt;/p&gt; &lt;h2 id=&quot;source-domain-model&quot;&gt;Source Domain Model&lt;/h2&gt; &lt;p&gt;Source domain model 은 backbone network에 fc layer를 결합하여 사용한다. Source domain model을 학습시킬때는 일반적인 학습방법으로 label smoothing을 적용하여 학습을 시킨다.&lt;/p&gt; \[\mathcal{L}_s(f_s;\mathcal{X_s},\mathcal{Y}_s)=-\mathbb{E}_{(x_s, y_s)\in \mathcal{X}_s \times \mathcal{Y}_s}(q^s)^Tlogf_s(x_s)\] &lt;h2 id=&quot;target-domain-model&quot;&gt;Target Domain Model&lt;/h2&gt; &lt;p&gt;Target domain model을 학습시키이 위해서는 source model의 noise를 보정해줘야 한다. 따라서 저자는 두 가지 방법을 사용한다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adaptive self-knowledge distillation&lt;/li&gt; &lt;li&gt;Distillation with structural regularizations&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;adaptive-self-knowledge-distillation&quot;&gt;Adaptive self-knowledge distillation&lt;/h3&gt; &lt;p&gt;Target model 또한 backbone network에 fc later를 결합하여 사용한다. 하지만 source model과 backbone 구조는 다를 수 있다.&lt;/p&gt; &lt;p&gt;기존의 kowledge distillation은 다음과 같다.&lt;/p&gt; \[\mathcal{L}_{kd}(f_t;\mathcal{X}_t,f_s)=\mathbb{E}_{x_t\in \mathcal{X}_t} \mathcal{D}_{kl}(f_s(x_t)||f_t(x_t))\] &lt;p&gt;source domain model의 예측값과 target domain model의 kl divergence가 최소가 되도록 학습을 진행한다. 하지만 저자는 source domain과 target domain의 차이로인해 noise가 발생할 것이라고 했고 이를 보정하기 위해 Adaptive Label Smoothing(AdaLS)을 제안한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/dine/Untitled%201.png&quot; width=&quot;60%&quot; /&gt; &lt;/p&gt; &lt;p&gt;즉, top r개의 probability만 가져오고 나머지는 smoothing한 결과를 가져와 K-r개의 예측값은 nosize라 판단하여 무시하는 것이다. 저자는 이를 통해 self-weighted pseudo labeling효과를 낼 수 있다고한다. 만약 Source model이 여러개이면 AdaLS의 평균값을 사용한다.&lt;/p&gt; \[P^T \leftarrow \frac{1}{M}\sum^{M}_{m=1}AdaLS(f_s^{(m)}(x_t))\] &lt;p&gt;논문에서는 추가로 noise 감소를 위해 self-distillation strategy를 사용했다. EMA를 통해서 target domain model의 self-distillation을 하는 것이다.&lt;/p&gt; \[P^T(x_t) \leftarrow \gamma P^T(x_t)+(1-\gamma)f_t(x_t), \forall x_T \in \mathcal{X}_t\] &lt;h2 id=&quot;distillation-with-structural-regularizations&quot;&gt;Distillation with Structural Regularizations&lt;/h2&gt; &lt;p&gt;저자는 AdaLS로는 noise regularization이 부족하다 생각했고 structural regularization을 사용했다.&lt;/p&gt; &lt;h3 id=&quot;mixup&quot;&gt;Mixup&lt;/h3&gt; &lt;p&gt;Target domain network의 예측값으로 mixup loss를 만들어 target domain structural information을 사용했다. 이미지의 linear combination의 예측과과 각각의 예측의 linear combination을 이용하여 target domain에 대한 구조적 정보를 만든 것이다.&lt;/p&gt; \[\mathcal{L}_{mix}(f_t,\mathcal{X}_t)=\mathbb{E}_{x_i^t,x_j^t \in \mathcal{X}_t} \mathbb{E}_{\lambda\in Beta(\alpha,\alpha)}\] \[l_{ce}({Mix}_{\lambda}(f_t^\prime(x_i^t), f_t^\prime(x_j^t)), f_t({Mix}_{\lambda}(x_i^t, x_j^t))\] &lt;h3 id=&quot;mutual-information&quot;&gt;Mutual Information&lt;/h3&gt; &lt;p&gt;Image set을 이용하여 모델의 예측에 대한 엔트로피를 계상하여 global한 구조적 정보를 학습한다. 아래의 수식은 모든 예측값에 대한 엔트로피는 높혀 불명확한 것은 불명확하게, 이미지에 대한 예측값의 엔트로피는 낮춰 명확한 예측값은 명확하게 만든다.&lt;/p&gt; \[\mathcal{L}(f_t; \mathcal{X}_t)=H(\mathcal{Y}_t)-H(\mathcal{Y}_t|\mathcal{X}_t)=h(\mathbb{E}_{x_t\in \mathcal{X}_t}f_t(x_t))-\mathbb{E}_{x_t\in \mathcal{X}_t}h(f_t(x_t))\] &lt;h2 id=&quot;loss&quot;&gt;Loss&lt;/h2&gt; &lt;p&gt;전체적인 Loss는 다음과 같다.&lt;/p&gt; \[\mathcal{L}_t=\mathcal{D}_{kl}(P^T(x_t)||f_t(x_t))+\beta\mathcal{L}_{mix}-\mathcal{L}_{im}\] &lt;h2 id=&quot;finetune&quot;&gt;FineTune&lt;/h2&gt; &lt;p&gt;DIRT-T의 영감을 받아서 secondary training을 진행한다. 이때 loss는 mutual information을 사용한다.&lt;/p&gt; \[\mathcal{L}(f_t; \mathcal{X}_t)=H(\mathcal{Y}_t)-H(\mathcal{Y}_t|\mathcal{X}_t)=h(\mathbb{E}_{x_t\in \mathcal{X}_t}f_t(x_t))-\mathbb{E}_{x_t\in \mathcal{X}_t}h(f_t(x_t))\] &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;p&gt;실험결과로는 다른 Source data를 접근할 수 없는 방법들 중에서는 SOTA를 찍었고, source data를 접근할 수 있는 모델 만큼 성능이 높아졌다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/dine/Untitled%202.png&quot; width=&quot;100%&quot; /&gt; &lt;img src=&quot;/assets/post/image/dine/Untitled%203.png&quot; width=&quot;100%&quot; /&gt; &lt;img src=&quot;/assets/post/image/dine/Untitled%204.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;h1 id=&quot;analysis&quot;&gt;Analysis&lt;/h1&gt; &lt;p&gt;mutual information loss가 학습에서의 영향이 컸고, AdaLS는 top r개를 뽑는 것이 좋았다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/dine/Untitled%205.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; </description> <pubDate>Sun, 16 Apr 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/dine/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/dine/</guid> <category>domain-adaptation</category> </item> </channel> </rss>