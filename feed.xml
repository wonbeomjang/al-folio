<?xml version="1.0" encoding="UTF-8"?> <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"> <channel> <title> blank </title> <description>개발을 좋아하는 딥러닝 리서쳐 장원범입니다. </description> <link>https://www.wonbeomjang.kr/</link> <atom:link href="https://www.wonbeomjang.kr/feed.xml" rel="self" type="application/rss+xml"/> <pubDate>Mon, 30 Dec 2024 08:59:57 +0000</pubDate> <lastBuildDate>Mon, 30 Dec 2024 08:59:57 +0000</lastBuildDate> <generator>Jekyll v4.3.4</generator> <item> <title>LoRA vs Full Fine-tuning: An Illusion of Equivalence</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;Pre-trained 모델을 downstream task에 finetuning하는 것은 computation-, data-efficient한 방법이다. 하지만 full-finetuning은 시간과 비용적으로 부담이 크다. 이를 해결하기 위해 LoRA와 같은 PEFT 방법이 제시되었다. 그러나 LoRA로 full-finetuning과 동일한 성능을 내도록 학습했을 때, 두 방법이 실제로 동일하게 작동하는지는 명확하지 않다. 저자는 이러한 의문을 실험을 통해 분석하며 다음과 같은 결론을 얻었다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;LoRA는 intruder dimensions을 도입하며 full-finetuning과 구조적으로 다른 학습을 진행한다.&lt;/li&gt; &lt;li&gt;LoRA로 fine-tuned된 모델은 intruder dimensions을 만들며 pre-training distribution에서 더 많이 벗어나고, continual pre-training에서도 덜 robust하다.&lt;/li&gt; &lt;li&gt;Low-rank LoRA가 target task에서 잘 작동하더라도 higher-rank parameterization이 더 좋은 결과를 낸다.&lt;/li&gt; &lt;/ol&gt; &lt;h1 id=&quot;model-differences-between-lora-and-full-fine-tuning&quot;&gt;Model Differences Between LoRA and Full Fine-Tuning&lt;/h1&gt; &lt;p&gt;저자는 Sharma et al. (2024)의 Singular Value Decomposition (SVD)을 활용한 pruning에서 영감을 얻었다. 이를 통해 LoRA fine-tuned 모델과 full fine-tuned 모델의 weight matrices의 singular vector와 pre-trained weight의 singular vector의 cosine similarity를 비교했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Fig. 2(b)에서 볼 수 있듯 LoRA와 full finetuning의 singular vector는 다른 양상을 보인다. LoRA fine-tuned singular vector는 full fine-tuned singular vector에 비해 pre-trained singular vector와 cosine similarity가 낮았다. 그리고 LoRA rank가 높을수록 singular vector가 pre-trained singular vector의 cosine similarity가 높았다.&lt;/p&gt; &lt;p&gt;저자는 이렇게 cosine similarity가 낮은 singular vector를 intruder dimension이라고 명명한다.&lt;/p&gt; \[\text{Definition 1: A singular vector } y_i \text{ from the fine-tuned weight matrix } W_{\text{tuned}} \text{ is an intruder dimension if and only if } \text{max}_i(\cos(y_j,x_i)) &amp;lt; \epsilon, \text{ where } \epsilon \text{ is a similarity threshold and } x_i \text{ is a singular vector in } W_0.\] &lt;p&gt;Full fine-tuning에서는 pre-trained singular vector와 높은 cosine similarity를 가지는 singular vector가 비슷한 singular value를 가지고 있다. 이는 full fine-tuning이 pre-trained singular vector와 singular value를 활용해 small update를 진행한다는 것을 보여준다. 반면, LoRA는 새로운 singular vector를 도입해 large norm으로 update를 한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%201.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Fig. 3에서 empty column은 intruder dimensions을 나타내며, 이는 full fine-tuning과의 차이를 보여준다.&lt;/p&gt; &lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt; &lt;p&gt;저자는 RoBERTa-base로 실험을 진행했다.&lt;/p&gt; &lt;h3 id=&quot;1-lora-finetuned-model은-high-ranking-intruder-dimensions을-가지지만-fully-fine-tuned-model은-그렇지-않다&quot;&gt;1. LoRA finetuned model은 high-ranking intruder dimensions을 가지지만 fully fine-tuned model은 그렇지 않다.&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%202.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;위 알고리즘에 따라, top-k highest ranking singular vector에 대해 모든 pre-trained singular vector와 maximum cosine similarity를 측정했을 때 threshold \(\epsilon\) 이하면 intruder dimension으로 분류한다. 저자는 LoRA로 학습한 모델들이 작은 \(\epsilon\) 에 대해 \(r \leq 16\)일 때 지속적으로 intruder dimension을 가진다는 것을 확인했다. 또한 rank가 올라갈수록 intruder dimension이 줄어드는 것을 확인했다.&lt;/p&gt; &lt;h3 id=&quot;2-lora-fine-tuned-model이-full-fine-tuned-model보다-학습량이-적은-작업에서도-intruder-dimensions이-존재한다&quot;&gt;2. LoRA fine-tuned model이 full fine-tuned model보다 학습량이 적은 작업에서도 intruder dimensions이 존재한다.&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%203.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;수학, 코딩과 같은 학습량이 적은 데이터로 fine-tuning할 경우에도 intruder dimensions이 발생한다. Magicoder 모델과 같은 code model에서도 intruder dimension이 나타나는데, 이는 pre-training domain과 code domain의 차이에서 비롯된 것으로 판단된다. 이 경우에도 LoRA fine-tuned model이 intruder dimensions을 더 많이 가진다.&lt;/p&gt; &lt;h3 id=&quot;3-full-fine-tuning-updates는-lora-update보다-higher-effective-rank를-가진다-full-rank일지라도&quot;&gt;3. Full fine-tuning updates는 LoRA update보다 higher effective rank를 가진다. (Full rank일지라도)&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%204.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Full fine-tuning은 LoRA tuning보다 더 높은 effective rank를 가진다. 예를 들어 \(r=768\)인 RoBERTa도 평균적으로 effective rank를 300으로 업데이트한다. 이는 LoRA가 full capacity \(r\)을 사용하지 못하고 업데이트를 진행한다는 것을 의미한다. 따라서 LoRA와 full fine-tuning의 차이는 coding과 같은 어려운 작업에서 더 두드러진다.&lt;/p&gt; &lt;h3 id=&quot;4-intruder-dimension은-high-and-low-singular-values-모두에-존재한다&quot;&gt;4. Intruder dimension은 high and low singular values 모두에 존재한다.&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%205.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Fig. 11a에서 볼 수 있듯, singular value의 비율과 관계없이 full fine-tuning보다 항상 더 많은 intruder dimensions을 가진다.&lt;/p&gt; &lt;h3 id=&quot;5-scaling-alpha를-lora의-rank에-따라-조절하면-intruder-dimensions이-줄어들고-effective-rank이-늘어난다&quot;&gt;5. Scaling \(\alpha\)를 LoRA의 rank에 따라 조절하면 intruder dimensions이 줄어들고 effective rank이 늘어난다.&lt;/h3&gt; &lt;p&gt;많은 논문에서 \(\alpha=2r\)로 설정하여 학습을 진행한다. 저자는 \(\alpha\)의 영향을 확인하기 위해 \(\alpha=2r\)과 \(\alpha=8\)로 설정해 비교 실험을 진행했다. 고정된 \(\alpha\) 값에서는 모든 rank에서 LoRA가 intruder dimensions을 보였으며, \(\alpha=2r\)과 비교했을 때 훨씬 적은 effective rank를 가졌다.&lt;/p&gt; &lt;h3 id=&quot;6-intruder-dimensions의-수는-fine-tuning-dataset의-크기에-비례하여-늘어난다&quot;&gt;6. Intruder dimensions의 수는 fine-tuning dataset의 크기에 비례하여 늘어난다.&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%206.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Fig. 12에서 \(r=8\)인 경우 하나 이상의 데이터셋을 학습시킬 때 intruder dimensions이 추가로 발생한다. 반면 \(r=1\)인 경우 intruder dimensions 수가 비슷한데, 이는 \(r=1\)일 때 모델의 표현력 한계 때문으로 추정된다.&lt;/p&gt; &lt;h3 id=&quot;conjecture-intruder-dimensions은-norm과-stability에-큰-영향을-끼친다&quot;&gt;Conjecture: Intruder dimensions은 norm과 stability에 큰 영향을 끼친다.&lt;/h3&gt; &lt;p&gt;Pre-trained singular vector와 다르게, LoRA는 intruder dimensions을 추가하면서 smaller dataset에 fine-tuning하므로 pre-trained vectors보다 큰 영향을 미친다. 반면 full fine-tuning은 pre-trained 모델의 spectral property를 유지하며 효과적으로 적응한다. 이를 통해 LoRA 모델은 fine-tuning task 이외의 분야에서 부정적 영향을 미치고, full fine-tuning 모델은 이러한 악영향이 적음을 확인할 수 있다.&lt;/p&gt; &lt;h1 id=&quot;behavioral-differences-between-lora-and-full-fine-tuning&quot;&gt;Behavioral Differences Between LoRA and Full Fine-Tuning&lt;/h1&gt; &lt;h3 id=&quot;1-lower-rank에서-lora는-continual-learning에-robust하지-않고-이전-작업을-더-많이-잊어버린다&quot;&gt;1. Lower rank에서 LoRA는 continual learning에 robust하지 않고 이전 작업을 더 많이 잊어버린다.&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%207.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Fig. 8에서 볼 수 있듯 LoRA는 target task에 대해 full fine-tuning과 비슷한 성능을 내지만 rank가 작을 경우 continual pre-training 성능이 낮다. Rank를 올리면 forgetting이 줄어들며, full fine-tuning이 가장 낮은 forgetting 비율을 가진다.&lt;/p&gt; &lt;h3 id=&quot;2-같은-test-accuracy로-fine-tuning했을-때-pre-training-pseudo-loss가-u-shaped-curve를-그리는-것을-확인할-수-있다&quot;&gt;2. 같은 test accuracy로 fine-tuning했을 때 pre-training pseudo loss가 U-shaped curve를 그리는 것을 확인할 수 있다.&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%208.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;이는 downstream task에 대해 optimal한 rank가 존재한다는 것을 보여준다. Rank가 낮을 경우 intruder dimensions의 영향으로 forgetting이 발생하며, rank가 높을 경우 overparameterization으로 인해 target task에 대해 overfitting이 일어난다.&lt;/p&gt; &lt;h3 id=&quot;3-alpha를-적절하게-설정하면-model-performance에-긍정적인-영향을-미친다&quot;&gt;3. \(\alpha\)를 적절하게 설정하면 model performance에 긍정적인 영향을 미친다.&lt;/h3&gt; &lt;p&gt;LoRA는 rank와 관계없이 forgetting 현상이 발생한다. 그러나 \(\alpha=8\)이 \(\alpha=2r\)보다 intruder dimensions이 더 많음에도 불구하고 높은 \(\alpha\)는 continual pre-training에서 성능 향상에 기여한다.&lt;/p&gt; &lt;h1 id=&quot;왜-intruder-dimensions이-존재할까&quot;&gt;왜 Intruder Dimensions이 존재할까?&lt;/h1&gt; &lt;h3 id=&quot;1-pre-trained-matrix에-random-vector를-더하면-intruder-dimensions이-생긴다&quot;&gt;1. Pre-trained matrix에 random vector를 더하면 intruder dimensions이 생긴다.&lt;/h3&gt; &lt;p&gt;이를 확인하기 위해 pre-trained weights \(W \in \mathbb{R}^{n \times n}\), randomly sampled vector \(v \in \mathbb{R}^n\), \(W\)의 singular value보다 큰 \(\lambda\)에 대해 \(\text{SVD}(W + \lambda vv^{T})\)와 \(\text{SVD}(W)\)를 비교했다. 이를 통해 random vector 추가가 intruder dimensions 생성에 영향을 미친다는 것을 확인했다.&lt;/p&gt; &lt;h3 id=&quot;2-update-rule에서의-차이점&quot;&gt;2. Update rule에서의 차이점&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%209.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;LoRA는 더 큰 learning rate를 사용하며 low-rank space에서 gradient projection을 진행한다. 이러한 방식은 full fine-tuning과 차이를 보이며 intruder dimensions 생성에 영향을 준다.&lt;/p&gt; &lt;h3 id=&quot;3-product-parameterization-of-lora&quot;&gt;3. Product parameterization of LoRA&lt;/h3&gt; &lt;p&gt;Matrices 곱은 spectral differences를 증가시키며 lower effective rank를 초래한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-29-lora-vs-full-fine-tuning/image%2010.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;따라서 LoRA adaptor에서 B만 학습시키는 것이 intruder dimensions이 더 적다는 것을 알 수 있다.&lt;/p&gt; &lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt; &lt;p&gt;저자는 LoRA와 full fine-tuning이 weight metrics의 spectral properties에서 큰 차이가 있음을 확인했다. LoRA 모델은 intruder dimensions을 자주 가지며, pre-trained singular vectors와 거의 orthogonal하다. 반면, full fine-tuning은 pre-trained spectral properties를 유지하며 효과적으로 적응한다. LoRA는 일부 task에서는 효과적일 수 있지만, pre-training domain과 다른 domain에 대해 더 많은 한계를 가지며, 이러한 한계는 continual learning에서도 명확히 드러난다.&lt;/p&gt; </description> <pubDate>Sat, 28 Dec 2024 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2024/lora-vs-full-fine-tuning/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2024/lora-vs-full-fine-tuning/</guid> <category>paper</category> <category>llm</category> <category>paper</category> <category>llm</category> </item> <item> <title>2024년 회고</title> <description>&lt;p&gt;입사한 지 어느덧 1년이 되어간다. 정확히 말하자면 업무를 시작한 건 9개월 정도지만, 한 해가 마무리되고 평가가 끝난 만큼 이를 돌아보며 정리해본다.&lt;/p&gt; &lt;h1 id=&quot;2023-간단히-돌아보기&quot;&gt;2023 간단히 돌아보기&lt;/h1&gt; &lt;p&gt;성과는 냈지만 외부로 발표가 안 된 것들은 XXXX로 표기하겠다.&lt;/p&gt; &lt;h2 id=&quot;업무&quot;&gt;업무&lt;/h2&gt; &lt;p&gt;Computer Vision Research Engineer로서 여러 기술을 제품에 적용했다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smart labeling을 위한 multi-modal model 활용&lt;/li&gt; &lt;li&gt;Anmaly detection model 경량화&lt;/li&gt; &lt;li&gt;XXXXXX&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;평가&quot;&gt;평가&lt;/h2&gt; &lt;p&gt;주도성과 전문성은 강점이지만, 커뮤니케이션이 약점이라는 평가를 많이 받았다. 이를 개선하기 위해 금년에는 지식 공유와 논문 정리, 실험 결과 전달 등에 적극적으로 임했다.&lt;/p&gt; &lt;h1 id=&quot;2024-돌아보기&quot;&gt;2024 돌아보기&lt;/h1&gt; &lt;p&gt;마찬가지로 성과는 냈지만 외부로 발표가 안 된 것들은 XXXX로 표기하겠다.&lt;/p&gt; &lt;h2 id=&quot;업무-1&quot;&gt;업무&lt;/h2&gt; &lt;p&gt;LLM, 데이터 구축, AI 모델 평가라는 새로운 분야에 적응하는 데 시간이 걸렸다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-2024-review/image.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;(3월 위클리 노트. 지금 보니 많이 성장한 것 같다.)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Anthropic, OpenAI와 함께 TelcoLLM 개발&lt;/li&gt; &lt;li&gt;Synthetic data + hand-crafted 연구 XXXXXXX &lt;ul&gt; &lt;li&gt;XXXXXXX&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;XXXXXX&lt;/li&gt; &lt;li&gt;LLM-as-a-Judge 연구로 평가 자동화&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;평가-1&quot;&gt;평가&lt;/h2&gt; &lt;p&gt;작년 전직장보다 더 좋은 평가를 받았다. 업무할 때의 내 장점을 강화하고 단점을 보안하려고 많은 노력을 했고, 그것이 평가에 들어났다. 또한 1:1 면담에서 팀원들이 평가한 내 장점을 들으며 내가 프로젝트 리드할 수 있는 전문성을 인정받은 것을 알게 되었다. 신입으로서 정말 긍정적인 평가다. 전사적 관점으로 프로젝트를 이끌면 좋겠다는 의견이 있었는데 아직 회사 적응하는데 바빴고, 연차가 쌓이면 자연스럽게 해결 될 것이다.&lt;/p&gt; &lt;h1 id=&quot;회고&quot;&gt;회고&lt;/h1&gt; &lt;h2 id=&quot;keep-유지하고-싶은-점&quot;&gt;Keep (유지하고 싶은 점)&lt;/h2&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;논문 읽는 습관&lt;/strong&gt;&lt;br /&gt; 업무를 설계할 때 보통 5~6개의 논문을 읽으며 공통점을 도출하고 한계점을 파악했다. 한계점만 해결하면 성공 가능성이 높아지는 걸 경험했다.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;논문 쓰기&lt;/strong&gt;&lt;br /&gt; 이번에 &lt;em&gt;TelBench&lt;/em&gt; 논문이 EMNLP2024 Industry Track에 채택되었다. 뿌듯한 결과이며, 내년에도 논문 작업을 지속하고 싶다.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;최선을 다하는 자세&lt;/strong&gt;&lt;br /&gt; 업무든 취미든 항상 최선을 다했다. 덕분에 올해 진행한 일과 결정사항에 후회가 없다.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;데이터에 대한 고민&lt;/strong&gt;&lt;br /&gt; AI는 결국 데이터로 시작해서 데이터로 끝난다는 생각이 든다. 데이터에 대한 고민이 AI 활용과 이해를 한층 더 깊게 해줬다.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;취미 생활&lt;/strong&gt;&lt;br /&gt; 이번 해는 유난히 취미 생활에 집중했다. 덕분에 심리적으로 지치지 않고 1년을 보낼 수 있었다. (하지만 해외여행은 조금 자제하자. 내 지갑이 지친다.)&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;problem-개선하고-싶은-점&quot;&gt;Problem (개선하고 싶은 점)&lt;/h2&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;설득 기술 부족&lt;/strong&gt;&lt;br /&gt; 기술적인 부분에만 집중하고, 상사에게 기대효과와 진행 이유를 설득하지 못했다. PM에게 스토리라인 구성과 목표 설정 및 달성 후 계획 등을 배우며 개선하고 있다.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;영어 능력 부족&lt;/strong&gt;&lt;br /&gt; 해외 기업과의 협업이 빈번한 우리 팀 특성상 영어는 필수다. 프랑스 스타트업과 프로젝트를 리드하며 업무는 문제없이 처리하고 있지만, 원활한 소통과 발전을 위해 영어 실력을 키워야겠다.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;프레임워크 활용 부족&lt;/strong&gt;&lt;br /&gt; 연구 중심의 업무를 하다 보니 프레임워크 활용이 부족했다. LangChain, vLLM 등 다양한 프레임워크를 분석하고 활용하며 실력을 키우고 싶다.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;try-구체적인-시도&quot;&gt;Try (구체적인 시도)&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;전사적인 관점을 갖고 업무 설계.&lt;/li&gt; &lt;li&gt;주 2회 이상 영어 연습.&lt;/li&gt; &lt;li&gt;LangChain, vLLM 등 프레임워크 분석.&lt;/li&gt; &lt;li&gt;논문 정리.&lt;/li&gt; &lt;/ol&gt; &lt;h1 id=&quot;2024-타임라인&quot;&gt;2024 타임라인&lt;/h1&gt; &lt;h2 id=&quot;1월--2월&quot;&gt;1월 ~ 2월&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-2024-review/image-1.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;8주간 신입사원 연수를 다녀왔다. SK 멤버사와 SK텔레콤, SK브로드밴드의 다양한 동기들을 만났다. 활발히 활동해 더 많은 사람을 만나지 못한 게 아쉽다.&lt;/p&gt; &lt;h2 id=&quot;2월&quot;&gt;2월&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-2024-review/05D9BD5F-F70F-4030-B1A8-59C8E68B4FD3_1_105_c.jpeg&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;학부 졸업. 운 좋게도 최종 학점 4.45로 수석을 하고 상장도 받았다. 전공과 적성이 잘 맞은 덕분인 것 같다.&lt;/p&gt; &lt;h2 id=&quot;4월&quot;&gt;4월&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-2024-review/A73C64A2-2E38-4EB2-B383-7C738A563522_1_102_o.jpeg&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;첫 일본 여행으로 도쿄를 방문. 클라이밍과 술로 가득 찬 2박 3일이었다.&lt;/p&gt; &lt;h2 id=&quot;5월&quot;&gt;5월&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-2024-review/image-2.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;발리 서핑 캠프 재방문. 서핑 실력이 급격히 늘었고 정말 즐거웠다.&lt;/p&gt; &lt;h2 id=&quot;7월&quot;&gt;7월&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-2024-review/804E272D-7564-4FFF-8C1C-4F203FAC60E1_1_105_c.jpeg&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;친구와 다낭 여행. 처음으로 관광 중심의 여행을 즐겼다.&lt;/p&gt; &lt;h2 id=&quot;9월&quot;&gt;9월&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-2024-review/3CB923FE-FCA4-47D2-8361-90FE310CB507_1_102_o.jpeg&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;다시 발리 방문. 이번엔 보드에서 투스텝까지 시도. 개선해야 할 점이 많음을 느꼈다.&lt;/p&gt; &lt;h2 id=&quot;10월&quot;&gt;10월&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-2024-review/image-4.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;EMNLP2024에 두 번째 논문 게재. 오랜만의 논문 작업이 힘들었지만 보람 있었다. 내년 2월쯤 또 논문을 쓸 계획이다.&lt;/p&gt; &lt;h2 id=&quot;11월&quot;&gt;11월&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-2024-review/image-3.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;삿포로 여행. 눈 덮인 풍경과 맛있는 음식이 인상적이었다.&lt;/p&gt; &lt;h2 id=&quot;2025년-1월&quot;&gt;2025년 1월&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-2024-review/89A1290C-DEE6-4ACB-BDDF-22E07E4FAE20_1_105_c.jpeg&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;태국 끄라비로 클라이밍 여행 예정.&lt;/p&gt; &lt;h1 id=&quot;맺으며&quot;&gt;맺으며&lt;/h1&gt; &lt;p&gt;정말 만족스러운 1년이다. 내년에도 더 잘해서 더 많은 성과를 이루어내길…&lt;/p&gt; </description> <pubDate>Wed, 18 Dec 2024 16:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2024/2024-review/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2024/2024-review/</guid> <category>daily</category> <category>self-reflection</category> <category>daily</category> <category>self-refection</category> </item> <item> <title>Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method 설명</title> <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;많은 LLM(대규모 언어 모델) 개발자들은 사용된 학습 코퍼스를 비공개로 처리합니다. 이는 저작권과 윤리적 문제와 같은 이유 때문입니다. 이러한 상황에서 저자는 블랙박스 LLM과 텍스트가 주어졌을 때, 해당 텍스트가 학습 데이터에 포함되어 있는지 확인할 수 있는 방법론을 제시합니다.&lt;/p&gt; &lt;h2 id=&quot;아이디어&quot;&gt;아이디어&lt;/h2&gt; &lt;p&gt;이 연구는 &lt;strong&gt;Divergence-from-randomness&lt;/strong&gt;에서 영감을 받았습니다. 특정 단어의 &lt;strong&gt;문서 내 사용 빈도(Within-document term-frequency)&lt;/strong&gt;와 &lt;strong&gt;전체 문서 컬렉션 내 사용 빈도(frequency of a word within the collection)&lt;/strong&gt; 간 차이를 측정함으로써 해당 단어가 문서에서 얼마나 중요한 정보를 담고 있는지 알 수 있다는 개념입니다. 이를 기반으로 다음과 같은 측정 방법이 제안되었습니다:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Within-document term-frequency&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM이 예측한 토큰의 확률로 계산됩니다.&lt;/li&gt; &lt;li&gt;이는 토큰 확률 분포(Token probability distribution)를 의미합니다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frequency of a word within the collection&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;코퍼스에서 해당 토큰의 평균 등장 빈도를 나타냅니다.&lt;/li&gt; &lt;li&gt;이는 토큰 빈도 분포(Token frequency distribution)로 정의됩니다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;토큰 확률 분포와 토큰 빈도 분포 간의 &lt;strong&gt;Divergence&lt;/strong&gt;가 높다면, 해당 텍스트가 모델의 학습 코퍼스에 포함되었을 가능성을 나타냅니다.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;방법론&quot;&gt;방법론&lt;/h2&gt; &lt;h3 id=&quot;문제-정의&quot;&gt;문제 정의&lt;/h3&gt; &lt;p&gt;텍스트 \(x\), LLM \(\mathcal{M}\), 정보가 없는 학습 코퍼스 \(D\), 학습 데이터 검출 과제 \(\mathcal{A}\)에 대해 다음을 정의합니다:&lt;/p&gt; \[\mathcal{A}(x,\mathcal{M})\rightarrow\{0,1\}\] &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Token Probability Distribution Computation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM \(\mathcal{M}\)에 텍스트 \(x\)를 질의하여 각 토큰 확률을 계산합니다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Token Frequency Distribution Computation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;접근 가능한 대규모 참조 코퍼스 \(\mathcal{D}^\prime\)를 사용하여 토큰 빈도를 추정합니다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Score Calculation via Comparison&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;두 분포를 비교하여 각 토큰의 확률을 조정(calibration)하고, 이를 기반으로 학습 데이터 여부를 판단할 점수를 계산합니다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Binary Decision&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;점수에 임계값을 적용하여 \(x\)가 모델 \(\mathcal{M}\)의 학습 코퍼스에 포함되어 있는지 예측합니다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h3 id=&quot;세부-절차&quot;&gt;세부 절차&lt;/h3&gt; &lt;h4 id=&quot;token-probability-distribution-computation&quot;&gt;Token Probability Distribution Computation&lt;/h4&gt; &lt;p&gt;시작 토큰 \(x_0\)를 포함하여 텍스트 \(x\)는 다음과 같이 정의됩니다:&lt;/p&gt; \[x^\prime=x_0x_1x_2...x_n\] &lt;p&gt;\(\mathcal{M}\)에 \(x\)를 질의하여 다음을 계산합니다:&lt;/p&gt; \[\{p(x_i|x_{&amp;lt; i};\mathcal{M}): 0 &amp;lt; i \le n\}\] &lt;h4 id=&quot;frequency-of-a-word-within-the-collection&quot;&gt;Frequency of a Word within the Collection&lt;/h4&gt; &lt;p&gt;참조 코퍼스 \(\mathcal{D}^\prime\)에서 특정 토큰 \(x_i\)의 빈도는 다음과 같이 계산됩니다:&lt;/p&gt; \[p(x_i, \mathcal{D}^\prime) = \frac{\text{count}(x_i)}{N^\prime}\] &lt;p&gt;만약 \(x_i\)가 코퍼스에 존재하지 않는 경우, 라플라스 스무딩(Laplace Smoothing)을 적용합니다:&lt;/p&gt; \[p(x_i; D^\prime) = \frac{\text{count}(x_i) + 1}{N^\prime + |V|}\] &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;여기서 $$&lt;/td&gt; &lt;td&gt;V&lt;/td&gt; &lt;td&gt;$$는 어휘(vocabulary) 크기입니다.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h4 id=&quot;score-calculation-through-compression&quot;&gt;Score Calculation through Compression&lt;/h4&gt; &lt;p&gt;토큰 확률 \(p(x_i;\mathcal{M})\)와 참조 코퍼스 확률 \(p(x_i;D^\prime)\) 간의 크로스 엔트로피(Cross-Entropy)는 다음과 같이 계산됩니다:&lt;/p&gt; \[\alpha_i = -p(x_i; \mathcal{M}) \cdot \log p(x_i; D^\prime).\] &lt;p&gt;특정 토큰이 우세한 영향을 미치지 않도록 상한선을 정의합니다:&lt;/p&gt; \[\alpha_i = \begin{cases} \alpha_i, &amp;amp; \text{if } \alpha_i &amp;lt; a \\ a, &amp;amp; \text{if } \alpha_i \geq a \end{cases}\] &lt;p&gt;텍스트 \(x\)에서 여러 토큰 \(x_i\)가 존재할 때, 평균을 계산하여 최종 점수를 구합니다:&lt;/p&gt; \[\beta = \frac{1}{|\text{FOS}(x)|} \sum_{x_j \in \text{FOS}(x)} \alpha_j\] &lt;h4 id=&quot;binary-decision&quot;&gt;Binary Decision&lt;/h4&gt; &lt;p&gt;최종적으로 점수 \(\beta\)에 임계값 \(\tau\)를 적용하여 학습 코퍼스 포함 여부를 판단합니다:&lt;/p&gt; \[\text{Decision}(x, \mathcal{M}) = \begin{cases} 0 \quad (x \notin \mathcal{D}), &amp;amp; \text{if } \beta &amp;lt; \tau, \\ 1 \quad (x \in \mathcal{D}), &amp;amp; \text{if } \beta \geq \tau. \end{cases}\] &lt;hr /&gt; &lt;h2 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h2&gt; &lt;h3 id=&quot;main-result&quot;&gt;Main Result&lt;/h3&gt; &lt;p&gt;Wiki 데이터를 기반으로 한 실험 결과는 아래와 같습니다:&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%201.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&quot;ablation-studies&quot;&gt;Ablation Studies&lt;/h3&gt; &lt;p&gt;다양한 설정에서 실험한 결과는 다음과 같습니다:&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%202.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%203.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h4 id=&quot;baselines&quot;&gt;Baselines&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CLD&lt;/strong&gt;: Baseline&lt;/li&gt; &lt;li&gt;&lt;strong&gt;+LUP&lt;/strong&gt;: Upper Bound 추가&lt;/li&gt; &lt;li&gt;&lt;strong&gt;+SFO&lt;/strong&gt;: 동적 Threshold 적용&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&quot;reference-corpus&quot;&gt;Reference Corpus&lt;/h4&gt; &lt;p&gt;참조 코퍼스로 무엇을 사용하더라도 결과에는 큰 차이가 없음을 보여줍니다:&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%204.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;h4 id=&quot;upper-bound&quot;&gt;Upper Bound&lt;/h4&gt; &lt;p&gt;Upper Bound는 각 토큰에 대해 다르게 적용해야 합니다:&lt;/p&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2024-12-19-pretraining-data-dection-for-large-language-models/image%205.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; </description> <pubDate>Wed, 11 Dec 2024 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2024/pretraining-data-dection-for-large-language-models/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2024/pretraining-data-dection-for-large-language-models/</guid> <category>paper</category> <category>llm</category> <category>paper</category> <category>llm</category> </item> <item> <title>META-REWARDING LANGUAGE MODELS: Self-Improving Alignment with LLM-as-a-Meta-Judge 설명</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;InstructGPT의 성공 이후, LLM의 instruction following 능력은 매우 중요한 요소로 자리 잡았다.&lt;br /&gt; 이를 개선하기 위해 SFT, preference optimization(RLHF, DPO, PPO, KPO 등)과 같은 방법들이 사용되었으나, 이러한 방식들은 많은 시간과 비용이 소요된다는 한계가 있다.&lt;/p&gt; &lt;p&gt;이를 해결하기 위해 Self-Reward 방법론이 제시되었다. 이 접근법에서는 하나의 LLM이 Actor와 Judge 두 가지 역할을 수행하며 자체적으로 preference optimization을 수행한다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Actor&lt;/strong&gt;: 주어진 instruction에 대한 response를 생성.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Judge&lt;/strong&gt;: Actor가 생성한 response를 평가하며, LLM-as-a-Judge 방식을 활용해 reward가 되는 preference pair를 생성.&lt;br /&gt; 하지만 기존 방식은 Actor가 좋은 response를 생성하는 데만 초점이 맞춰져 있어 Judge의 성능에는 관심을 두지 않는다는 한계가 있다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;이 문제를 해결하기 위해 저자는 &lt;strong&gt;LLM-as-a-Meta-Judge&lt;/strong&gt;를 제안했다.&lt;br /&gt; 이 방법론의 핵심은 LLM이 Actor와 Judge 역할뿐만 아니라 Meta-Judge 역할까지 수행하도록 하여 Judge 능력에 대한 추가적인 reward를 제공하는 것이다.&lt;/p&gt; &lt;hr /&gt; &lt;h1 id=&quot;meta-rewarding&quot;&gt;Meta-Rewarding&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/fig1.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Meta-Rewarding은 세 가지 주요 구성 요소로 이루어진다:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Actor&lt;/strong&gt;: 주어진 instruction에 대해 다수의 response를 생성.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Judge&lt;/strong&gt;: LLM-as-a-Judge 프롬프트를 통해 각 response를 평가하고 score를 생성.&lt;br /&gt; 이 score는 Actor를 학습시키는 preference pair로 사용된다.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meta-Judge&lt;/strong&gt;: 여러 Judge를 평가해 가장 적합한 Judge를 선택.&lt;br /&gt; 여기서는 LLM-as-a-Meta-Judge 프롬프트를 활용해 결과를 생성하고, 이를 Judge 학습용 preference pair로 사용한다.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&quot;actor-preference-dataset-creation&quot;&gt;Actor Preference Dataset Creation&lt;/h2&gt; &lt;h3 id=&quot;1-sample-responses-from-actor&quot;&gt;1. Sample Responses from Actor&lt;/h3&gt; &lt;p&gt;Iteration \(t\)에서 현재 모델 \(M_t\)를 사용하여 \(K\)개의 response를 생성.&lt;/p&gt; \[\{y_1, ..., y_{K}\}\] &lt;h3 id=&quot;2-aggregate-multiple-judgements&quot;&gt;2. Aggregate Multiple Judgements&lt;/h3&gt; &lt;p&gt;각 response \(y_k\)에 대해 N개의 서로 다른 Judge를 생성하며, 5점 척도로 평가.&lt;br /&gt; 만약 parsing이 불가능한 경우 해당 데이터를 제외(drop).&lt;/p&gt; \[\{j_k^1, ..., j_k^N\}\] &lt;h3 id=&quot;3-preference-data-selection-with-length-control&quot;&gt;3. Preference Data Selection with Length-Control&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;최고 점수 \(S_{\text{max}}\)의 response \(y_c\)와 최저 점수 \(S_{\text{min}}\)의 response \(y_r\)를 선택.&lt;/li&gt; &lt;li&gt;길이 조정을 통해 response quality를 일정 수준 이상 유지.&lt;/li&gt; &lt;li&gt;점수 범위 내 비슷한 quality는 제외(drop).&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&quot;judge-preference-data-creation&quot;&gt;Judge Preference Data Creation&lt;/h2&gt; &lt;h3 id=&quot;1-responses-selection&quot;&gt;1. Responses Selection&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;모든 데이터를 사용하는 것은 비효율적이므로, judge confidence가 낮은 데이터를 우선 선택.&lt;/li&gt; &lt;li&gt;instruction에 대한 response score의 분산(variance)이 가장 높은 데이터를 활용.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;2-pairwise-meta-judge-evaluation&quot;&gt;2. Pairwise Meta-Judge Evaluation&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;\(\{j^1, ..., j^N\}\)에서 두 개의 judgement를 선택해 \((j^m, j^n)\) 구성.&lt;/li&gt; &lt;li&gt;두 judge 순서를 바꿔 평가하여 position bias를 해결.&lt;/li&gt; &lt;li&gt;평가 결과가 같으면 accept, 다르면 reject.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Position별 가중치 계산:&lt;/p&gt; \[\omega_{1} = \frac{\text{win}_{\text{2nd}}}{\text{win}_{\text{1nd}} + \text{win}_{\text{2nd}}}, \text{ } \omega_{2} = \frac{\text{win}_{\text{1nd}}}{\text{win}_{\text{1nd}} + \text{win}_{\text{2nd}}}\] &lt;p&gt;Meta-Judge 결과로 battle result 계산:&lt;/p&gt; \[r_{mn} = \begin{cases} 1 &amp;amp; \text{if the meta-judge prefers } j_m \\ -1 &amp;amp; \text{if the meta-judge prefers } j_n \\ 0 &amp;amp; \text{if tie or parse error.} \end{cases}\] \[B_{mn} = \omega_1 \mathbb{1}[r^{mn} = 1] + \omega_2 \mathbb{1}[r^{nm} = -1]\] &lt;h3 id=&quot;3-elo-score-and-pair-selection&quot;&gt;3. Elo Score and Pair Selection&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Elo score를 통해 judge의 reward 계산.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt; &lt;h2 id=&quot;experiment-set-up&quot;&gt;Experiment Set-up&lt;/h2&gt; &lt;p&gt;Iteration마다 학습 방법을 달리하여 Meta-Rewarding 효과를 평가.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Iter 1&lt;/strong&gt;: SFT 모델에서 시작해 DPO를 통해 Actor와 Judge preference pair를 학습하여 \(M_1\) 생성.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Iter 2&lt;/strong&gt;: \(M_1\)을 기반으로 Actor와 Judge preference pair를 학습하여 \(M_2\) 생성.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Iter 3&lt;/strong&gt;: \(M_2\)에서 Actor preference pair만 학습하여 \(M_3\) 생성.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Iter 4&lt;/strong&gt;: \(M_3\)에서 Actor preference pair만 학습하여 \(M_4\) 생성.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&quot;instruction-following-evaluation&quot;&gt;Instruction Following Evaluation&lt;/h2&gt; &lt;h3 id=&quot;meta-rewarding-iterations-significantly-improve-the-win-rate&quot;&gt;Meta-Rewarding iterations significantly improve the win rate&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/fig3.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Meta-Rewarding은 특히 Length Control 조건에서 높은 성능을 보임.&lt;/p&gt; &lt;h3 id=&quot;the-meta-judge-and-length-control-mechanism-are-important&quot;&gt;The meta-judge and length-control mechanism are important&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table1.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Table 1에 따르면, iteration이 진행됨에 따라 평균 길이가 증가하지 않음을 확인.&lt;/p&gt; &lt;h3 id=&quot;meta-rewarding-improves-nearly-all-instruction-categories&quot;&gt;Meta-Rewarding improves nearly all instruction categories&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/fig4.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;거의 모든 카테고리에서 성능 향상 확인.&lt;/p&gt; &lt;h3 id=&quot;meta-rewarding-enhances-responses-to-complex-and-hard-questions&quot;&gt;Meta-Rewarding enhances responses to complex and hard questions&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table2.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;복잡한 질문(arena-hard)에 대해서도 높은 성능 보임.&lt;/p&gt; &lt;h3 id=&quot;meta-rewarding-does-not-sacrifice-multi-turn-ability&quot;&gt;Meta-Rewarding does not sacrifice multi-turn ability&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table6.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Single-turn 데이터만으로 학습했음에도 multi-turn 성능 유지.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;reward-modeling-evaluation&quot;&gt;Reward Modeling Evaluation&lt;/h2&gt; &lt;h3 id=&quot;the-model-improves-in-judging-after-judge-training&quot;&gt;The model improves in judging after judge training&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table3.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Meta-Rewarding은 GPT-4와의 judge 상관관계를 개선.&lt;/p&gt; &lt;h3 id=&quot;meta-rewarding-improves-human-judge-correlation&quot;&gt;Meta-Rewarding improves human judge correlation&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table7.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;사람과의 judge 상관관계 역시 개선.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;ablations-and-analysis&quot;&gt;Ablations and Analysis&lt;/h2&gt; &lt;h3 id=&quot;length-control-mechanism&quot;&gt;Length-Control Mechanism&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table4.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Length Control이 없으면 verbosity 증가.&lt;/p&gt; &lt;h3 id=&quot;meta-judge-biases&quot;&gt;Meta-Judge Biases&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/table5.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;높은 점수를 준 judge를 선호하는 경향 발견.&lt;/p&gt; &lt;h3 id=&quot;judge-scoring-shift&quot;&gt;Judge Scoring Shift&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/llm-as-a-meta-judge/fig5.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;Score 분포가 학습 중 5점으로 집중됨(score-bias).&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Judge 모델이 적은 quality 차이를 tie로 판단하는 경향 있음.&lt;/li&gt; &lt;li&gt;Meta-Judge에서 bias가 존재.&lt;/li&gt; &lt;/ul&gt; </description> <pubDate>Thu, 19 Sep 2024 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2024/llm-as-a-meta-judge/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2024/llm-as-a-meta-judge/</guid> <category>paper</category> <category>llm</category> <category>paper</category> <category>llm</category> </item> <item> <title>2023 Review</title> <description>&lt;h3 id=&quot;2023년-회고록&quot;&gt;2023년 회고록&lt;/h3&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;2023년을 한마디로 정리하자면, ‘노력의 결실을 맺은 해’였다.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;대학교 입학 때부터 딥러닝과 컴퓨터 비전에 관심을 두고 꾸준히 공부하고 프로젝트를 진행해왔다. 이를 바탕으로 CV와 포트폴리오를 작성했으며, 이후에는 뉴로클에서 리서치 인턴으로 활동하면서 제품 개발 및 고도화를 진행하고 성과를 냈다. 이러한 경험을 토대로 SK텔레콤 공채에 지원해 최종 합격했다.&lt;/p&gt; &lt;p&gt;4년 동안의 노력과 준비의 결과물이기도 하지만, 좋은 기회가 주어졌기에 감사한 마음으로 한 해를 성공적으로 마무리할 수 있었다. 한 해 동안 있었던 주요 사건들을 돌아보자.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&quot;뉴로클에서의-리서치-인턴-116825&quot;&gt;뉴로클에서의 리서치 인턴 (1/16~8/25)&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2023-review/2F49877F-F3D5-427E-91EA-A482AC3E65EA_1_105_c.jpeg&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;뉴로클이라는 스타트업에서 8개월간 리서치 인턴으로 근무했다. 이 기간 동안 산업계로 갈지, 학계로 갈지를 고민했고, 이를 탐색하기 위해 스타트업 인턴십을 선택했다.&lt;/p&gt; &lt;p&gt;이 과정에서 매주 1~2편씩 논문을 읽으며 약 30편 이상의 논문을 섭렵했다. OCR 고도화 연구뿐만 아니라 스마트 라벨링, 전이 학습 성능 검증, 초해상도 기술 검토, 이상 탐지 모델 경량화 등 다양한 직무를 경험했다.&lt;/p&gt; &lt;p&gt;이러한 과정에서 새로운 모델을 개발하고 성능을 높이는 작업보다 &lt;strong&gt;기존 모델을 메모리, 추론 시간, 에너지 소비를 최적화하며 서빙하고 개인화하는 데 흥미가 더 크다는 사실을 깨달았다.&lt;/strong&gt; 이를 바탕으로 Low-Precision, PEFT, 하드웨어 최적화, 뉴로모픽 엔지니어링 등에 관심을 두게 되었고, 산업계 진출을 결심하게 되었다.&lt;/p&gt; &lt;p&gt;자세한 이야기는 &lt;a href=&quot;https://www.wonbeomjang.kr/blog/2023/startup-research-intern-review/&quot;&gt;스타트업 리서치 인턴 후기&lt;/a&gt;에서 확인 가능하다.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&quot;발리-서핑캠프-82692&quot;&gt;발리 서핑캠프 (8/26~9/2)&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2023-review/Pasted image 20240103224106.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;취미다운 취미를 찾기 위해 발리 서핑캠프에 다녀왔다. 1주일간의 캠프는 내 서핑 습관을 모두 고치기엔 짧았지만, 매우 즐거운 시간이었다. 시간이 허락한다면 발리에서 몇 달간 살며 서핑만 하고 싶다는 생각이 들 정도로 좋았다.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&quot;복학-및-취준-941221&quot;&gt;복학 및 취준 (9/4~12/21)&lt;/h3&gt; &lt;p&gt;휴학과 복학 사이에서 고민하다가, 취업 시장이 더 좁아지기 전에 빨리 시작해야겠다는 생각으로 복학했다. 다행히 11학점만 이수하면 되었기에 여유롭게 수업을 듣는 한편, 여행과 취미생활도 즐기며 취업 준비를 병행했다.&lt;/p&gt; &lt;p&gt;취업을 준비하면서 가장 중요하게 생각한 것은 바로 &lt;strong&gt;‘나의 강점과 차별성을 어떻게 어필할 것인가’였다.&lt;/strong&gt;&lt;/p&gt; &lt;h4 id=&quot;취업-후기-요약&quot;&gt;취업 후기 요약&lt;/h4&gt; &lt;blockquote&gt; &lt;p&gt;사실 1학년 때부터 퍼스널 브랜딩과 커리어에 관심이 많아 미리 준비를 해온 것이 큰 도움이 되었다. 취업 과정에서 단순한 스펙이나 어학 실력이 아닌, ‘나 자신’과 ‘내가 가진 강점’을 어필하는 것이 중요하다는 것을 느꼈다.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;강조한 세 가지는 다음과 같다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;사용자 이해&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;디자이너와 기획자들과 협업해 사용자 중심의 서비스 기획과 개발 경험을 강조했다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;실용적인 AI 개발자&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;단순히 모델 성능을 높이는 것이 아니라, &lt;strong&gt;추론 시간을 유지하거나 줄이면서도 성능을 높이는 기술에 집중&lt;/strong&gt;했다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;연구 능력&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;딥러닝 스타트업에서 연구와 실용화를 접목한 경험을 바탕으로, 실용적인 연구개발자로서의 역량을 강조했다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;결과적으로 6개 기업에 지원해 2곳에서 최종 면접까지 갔으며, 최종적으로 SK텔레콤에 합격했다.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&quot;끄라비-암벽등반-여행-12211231&quot;&gt;끄라비 암벽등반 여행 (12/21~12/31)&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2023-review/Pasted image 20240102222158.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;대학 졸업 전 마지막으로 마음 편히 즐길 수 있는 여행을 떠났다. 끄라비에서 암벽등반을 하며 처음으로 5.11c/6c+ 등급을 달성했다는 점에서 더욱 의미 있는 여행이었다.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&quot;sk-신입구성원-과정-12&quot;&gt;SK 신입구성원 과정 (1/2~)&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/post/image/2023-review/Pasted image 20240103231802.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt; &lt;p&gt;현재 SK텔레콤 신입 연수를 받고 있다. 하루밖에 지나지 않았지만, 이곳에서 많은 성장을 할 수 있을 것 같아 기대가 크다.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&quot;앞으로의-계획&quot;&gt;앞으로의 계획&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;올해의 목표: 발전을 위한 초석 다지기&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;올해는 새로운 직장에 적응하고 빠르게 배워나가는 것이 우선이다. 동시에 3년 후 더 큰 기회를 맞이할 준비를 해야 한다고 생각한다. 이를 위해 다음과 같은 계획을 세웠다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;논문 꾸준히 읽기&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;너무 당연한 목표라 특별히 부연 설명할 필요는 없을 것 같다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;수학 공부&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;FlashAttention을 보면서 인공지능 최적화에는 엄밀한 수학적 증명이 필요하다는 것을 느꼈다. 이를 보완하기 위해 선형대수, 수리통계학, 표본론을 체계적으로 학습할 계획이다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;코딩 공부&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;지금까지 컴퓨터 비전에 특화되어 있어, 전반적인 개발 역량이 부족하다는 생각이 들었다.&lt;/li&gt; &lt;li&gt;디자인 패턴, 서버, 클라우드, 그리고 CUDA에 집중적으로 공부할 예정이다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;하드웨어 공부&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;앞으로 NPU를 활용한 최적화 솔루션이 더욱 중요해질 것으로 예상된다. 이를 위해 하드웨어에 대한 이해도를 높이고 관련 프로그래밍 역량을 강화할 것이다.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; </description> <pubDate>Thu, 28 Dec 2023 00:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/2023-review/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/2023-review/</guid> </item> <item> <title>Keras 3.0 설명</title> <description>&lt;h1 id=&quot;why&quot;&gt;Why?&lt;/h1&gt; &lt;p&gt;과거 keras는 Tensorflow, Theano, MXNet등 여러 deep learning backend framework가 있을 때 multi-backend 지원의 강점을 가지며 출시되었다. 하지만 Teano와 MXNet등 여러 framework들은 쇠퇴의 길을 걸었고, tensorflow만 살아남게 되었다. 하지만 당시 tensorflow도 문제점은 가지고 있었다. 그것은 model 선언이 비직관적이라는 것이다. 따라서 tensorflow는 keras를 공식 레포에 집어넣어 keras.layer로 모델을 만들고 tensorflow backend로 학습하는 구조로 발전했다.&lt;/p&gt; &lt;p&gt;하지만 현재는 여러 연구에서 pytorch를 사용하고, pytorch 기반의 huggingface가 등장하면서 keras입장에서는 pytorch가 매력적인 시장으로 보였다. 그리고 tensorflow의 사용자가 줄어가고, 윈도우 네이티브 업데이트 지원을 종료하면서 다시 multi-backend의 강점을 다시 살리기로 했다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;그래서 keras는 다음과 같은 기능을 강조하면서 keras3.0을 출시했다.&lt;/p&gt; &lt;h1 id=&quot;주요-기능&quot;&gt;주요 기능&lt;/h1&gt; &lt;p&gt;Keras 3.0을 출시하면서 다음과 같은 중요한 기능을 제시했다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The full Keras API, available for TensorFlow, JAX, and PyTorch&lt;/li&gt; &lt;li&gt;A cross-framework low-level language for deep learning&lt;/li&gt; &lt;li&gt;Seamless integration with native workflows in JAX, PyTorch, and TensorFlow&lt;/li&gt; &lt;li&gt;Support for cross-framework data pipelines with all backends&lt;/li&gt; &lt;li&gt;A new distribution API for large-scale data parallelism and model parallelism&lt;/li&gt; &lt;li&gt;Pretrained models&lt;/li&gt; &lt;li&gt;Progressive disclosure of complexity&lt;/li&gt; &lt;li&gt;A new stateless API for layers, models, metrics, and optimizers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;하나씩 살펴보자&lt;/p&gt; &lt;h2 id=&quot;the-full-keras-api-available-for-tensorflow-jax-and-pytorch&quot;&gt;The full Keras API, available for TensorFlow, JAX, and PyTorch&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;keras로 선언된 모델과 함수들은 tensorflow, jax, pytorch에서 모두 사용가능하다. 즉, 3개의 프레임워크에서 모두 keras 함수를 사용할 수 있다는 것이다. 여기서 재밌는 점은 기존에 tf.keras로 선언된 모델도 jax, pytorch에서 실행 가능하다.&lt;/p&gt; &lt;h2 id=&quot;a-cross-framework-low-level-language-for-deep-learning&quot;&gt;A cross-framework low-level language for deep learning&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;딥러닝 모델을 구성하다보면 matmul, stack 등 기본적인 연산자가 필요할 때 있다. 이럴때는 keras.ops를 사용하여 기본적인 연산자를 구성하면 tensorflow, jax, pytorch에서 모두 사용가능하다. 이 떄 keras는 두 가지를 중심으로 구현했다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Numpy에 관련한 연산자는 모두 구현한다. ex) ops.matmul, ops.sum, ops.stack, ops.einsum&lt;/li&gt; &lt;li&gt;Neural-specific function을 구현한다. ex) ops.softmax, ops.binary_crossentropy, ops.conv&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;seamless-integration-with-native-workflows-in-jax-pytorch-and-tensorflow&quot;&gt;Seamless integration with native workflows in JAX, PyTorch, and TensorFlow&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_5.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Integration하다보면 기존의 training loop 등 workflow를 그대로 유지해야할 경우가 있다. 물론 keras3.0은 이 경우도 지원한다.&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;Write a low-level JAX training loop to train a Keras model using an optax optimizer, jax.grad, jax.jit, jax.pmap.&lt;/li&gt; &lt;li&gt;Write a low-level TensorFlow training loop to train a Keras model using tf.GradientTape and tf.distribute.&lt;/li&gt; &lt;li&gt;Write a low-level PyTorch training loop to train a Keras model using a torch.optim optimizer, a torch loss function, and the torch.nn.parallel.DistributedDataParallel wrapper.&lt;/li&gt; &lt;li&gt;Use Keras layers in a PyTorch Module (because they are Module instances too!)&lt;/li&gt; &lt;li&gt;Use any PyTorch Module in a Keras model as if it were a Keras layer.&lt;/li&gt; &lt;li&gt;etc.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;a-new-distribution-api-for-large-scale-data-parallelism-and-model-parallelism&quot;&gt;A new distribution API for large-scale data parallelism and model parallelism&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_7.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_8.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;keras에서는 여러 data parallelism을 제공한다. 단 두줄 만으로 분산학습이 된다는게 신기하긴 하다.&lt;/p&gt; &lt;h2 id=&quot;support-for-cross-framework-data-pipelines-with-all-backends&quot;&gt;Support for cross-framework data pipelines with all backends&lt;/h2&gt; &lt;p&gt;각 framework별로 다른 dataset 객체를 사용한다. Keras3.0은 이를 모두 지원한다.&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;tf.data.Dataset pipelines: the reference for scalable production ML.&lt;/li&gt; &lt;li&gt;torch.utils.data.DataLoader objects.&lt;/li&gt; &lt;li&gt;NumPy arrays and Pandas dataframes.&lt;/li&gt; &lt;li&gt;Keras’s own keras.utils.PyDataset objects.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;pretrained-models&quot;&gt;Pretrained models&lt;/h2&gt; &lt;p&gt;Keras3.0은 다음과 같은 pretrained model을 지원한다.&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;BERT&lt;/li&gt; &lt;li&gt;OPT&lt;/li&gt; &lt;li&gt;Whisper&lt;/li&gt; &lt;li&gt;T5&lt;/li&gt; &lt;li&gt;StableDiffusion&lt;/li&gt; &lt;li&gt;YOLOv8&lt;/li&gt; &lt;li&gt;SegmentAnything&lt;/li&gt; &lt;li&gt;etc.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h2 id=&quot;progressive-disclosure-of-complexity&quot;&gt;Progressive disclosure of complexity&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/keras3/img_6.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;개발하다보면 pytorch lightening, pytorch ignite, tensorflow orbit등 disclosure를 위한 툴을 쓴 경험이 있을 것이다. Keras는 이것이 keras api의 핵심 디자인으로 삼았으며 이를 지원한다 한다. &lt;del&gt;그냥 다른거 쓸 것 같긴한데…&lt;/del&gt;&lt;/p&gt; &lt;h2 id=&quot;a-new-stateless-api-for-layers-models-metrics-and-optimizers&quot;&gt;A new stateless API for layers, models, metrics, and optimizers.&lt;/h2&gt; &lt;p&gt;함수형 프로그래밍을 좋아하는 사람을 위해 stateless한 함수들을 만들었다.&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;All layers and models have a stateless_call() method which mirrors &lt;strong&gt;call&lt;/strong&gt;().&lt;/li&gt; &lt;li&gt;All optimizers have a stateless_apply() method which mirrors apply().&lt;/li&gt; &lt;li&gt;All metrics have a stateless_update_state() method which mirrors update_state() and a stateless_result() method which mirrors result().&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h1 id=&quot;example&quot;&gt;Example&lt;/h1&gt; &lt;p&gt;Tensorflow는 기존의 방법과 동일해서 설명을 생략하겠다.&lt;/p&gt; &lt;h3 id=&quot;mnist-with-keras-vgg19-pytorch-beckend&quot;&gt;MNIST with keras vgg19 (Pytorch Beckend)&lt;/h3&gt; &lt;script src=&quot;https://gist.github.com/wonbeomjang/e935128f7f55045ab2d08e091cc2b8e2.js&quot;&gt;&lt;/script&gt; &lt;p&gt;이렇게 하면 기존 tensorflow나 keras vgg를 weight를 포함하여 사용할 수 있다. 여기서 주의할 점은 dataset augmentation 부분에서 CHW를 HWC로 바꿔줘야한다는 것이다.&lt;/p&gt; &lt;h3 id=&quot;declare-pytorch-model-using-keras-application&quot;&gt;Declare Pytorch Model Using Keras Application&lt;/h3&gt; &lt;script src=&quot;https://gist.github.com/wonbeomjang/c76b1da2952d231e209a0d03896c4aef.js&quot;&gt;&lt;/script&gt; &lt;p&gt;재밌는 것은 keras.layer가 torch.nn.Module과 호환이되어 다음과 같이 모델을 선언할 수 있다.&lt;/p&gt; &lt;h1 id=&quot;맺으며&quot;&gt;맺으며&lt;/h1&gt; &lt;p&gt;너무 많은 담기 그래서 이쯤으로 마치고, 더 많은 예제는 다음에 다루기로 하겠다.&lt;/p&gt; </description> <pubDate>Sun, 03 Dec 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/keras-3/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/keras-3/</guid> <category>framework</category> <category>framework</category> </item> <item> <title>What Makes Multi-modal Learning Better than Single (Provably)</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;우리 세상에는 다양한 modality가 존재합니다. 직관적으로 여러 modal의 정보를 결합(fusion)하면, uni-modal보다 더 나은 성능을 얻을 수 있을 것이라 생각됩니다. 이에 대해 다음과 같은 질문을 던질 수 있습니다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; _multi-modal learning이 uni-modal learning보다 항상 좋은 성능을 제공할까?_ &lt;/p&gt; &lt;p&gt;저자는 이 질문을 중심으로 연구를 시작하며, 다음 두 가지를 중점적으로 살펴보았습니다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;어떤 상황에서 multi-modal이 uni-modal보다 성능이 좋은가?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;multi-modal 학습이 더 나은 성능을 제공하는 이유는 무엇인가?&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;이를 통해 저자는 다음과 같은 기여를 했습니다:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-modal learning을 population risk 관점에서 설명하고, latent representation quality와 연결 지었습니다.&lt;/li&gt; &lt;li&gt;특정 modality subset으로 학습한 network의 성능 상한선을 이론적으로 제시했습니다.&lt;/li&gt; &lt;li&gt;Modalities의 subset만 사용할 경우 성능이 저하되는 이유를 분석했습니다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;결론은 다음과 같습니다:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multiple modalities는 특정 modal subset보다 낮은 population risk를 갖습니다.&lt;/li&gt; &lt;li&gt;이는 multi-modal 학습이 더 정확한 latent space representation을 제공하기 때문입니다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;이제 세부적으로 살펴보겠습니다.&lt;/p&gt; &lt;hr /&gt; &lt;h1 id=&quot;the-multi-modal-learning-formulation&quot;&gt;The Multi-modal Learning Formulation&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/figure1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;multi-modal-데이터의-수학적-정의&quot;&gt;Multi-modal 데이터의 수학적 정의&lt;/h2&gt; &lt;p&gt;K개의 modalities에 대해 데이터는 다음과 같이 표현됩니다.&lt;br /&gt; \(\mathbb{x}:=(x^{(1)},\cdots,x^{(K)})\)&lt;br /&gt; 이때, \(x^{(k)} \in \mathcal{X}^{(k)}\) 이며, 전체 input data space는 다음과 같습니다.&lt;br /&gt; \(\mathcal{X}=\mathcal{X}^{1} \times \cdots \times \mathcal{X}^{k}\)&lt;/p&gt; &lt;p&gt;target domain은 \(\mathcal{Y}\), 공통된 latent space는 \(\mathcal{Z}\)로 정의합니다. True mapping은 다음과 같습니다:&lt;br /&gt; \(g^\star: \mathcal{X} \mapsto \mathcal{Z}, \quad h^\star: \mathcal{Z} \mapsto \mathcal{Y}\)&lt;/p&gt; &lt;p&gt;데이터의 분포는 다음과 같이 정의됩니다.&lt;br /&gt; \(\mathbb{P}_\mathcal{D}(\mathbb{x},y)\triangleq\mathbb{P}_{y|x}(y|h^\star\circ g^\star(\mathbb{x}))\mathbb{P}_\mathbb{x}(\mathbb{x})\)&lt;/p&gt; &lt;h2 id=&quot;subset-modalities&quot;&gt;Subset Modalities&lt;/h2&gt; &lt;p&gt;우리는 K개의 modalities 중 \(\mathcal{N} \leq \mathcal{M}\) 인 subset을 선택할 수 있습니다.&lt;br /&gt; 이때 modality의 superset은 다음과 같습니다:&lt;br /&gt; \(\mathcal{X}^\prime := (\mathcal{X}^{(1)}\cup\bot)\times\cdots\times(\mathcal{X}^{(K)}\cup\bot)\)&lt;br /&gt; 여기서 \(\bot\)은 특정 modality를 사용하지 않음을 의미합니다.&lt;/p&gt; &lt;p&gt;modality 선택 함수 \(p_\mathcal{M}\)는 다음과 같이 정의됩니다.&lt;/p&gt; \[p_\mathcal{M}(\mathbb{x})^{(k)}= \begin{cases} \mathbb{x}^{(k)} &amp;amp; \text{if } k\in\mathcal{M}, \\ \bot &amp;amp; \text{else}. \end{cases}\] &lt;h2 id=&quot;학습-목표-empirical-risk-minimization-erm&quot;&gt;학습 목표: Empirical Risk Minimization (ERM)&lt;/h2&gt; &lt;p&gt;우리의 목표는 ERM에 따라 학습 objective를 최소화하는 것입니다:&lt;/p&gt; \[\text{min } \hat{r}(h\circ g_\mathcal{M}) = \frac{1}{m}\sum_{i=1}^ml(h\circ g_\mathcal{M}(\mathbb{x}_i),y_i), \quad \text{s.t. } h \in \mathcal{H}, g_\mathcal{M} \in \mathcal{G}.\] &lt;p&gt;최종적으로 population risk는 다음과 같이 정의됩니다.&lt;br /&gt; \(r(h\circ g_\mathcal{M})=\mathbb{E}_{(\mathbb{x}_i, y_i)\sim\mathcal{D}}[\hat{r}(h\circ g_\mathcal{M})]\)&lt;/p&gt; &lt;hr /&gt; &lt;h1 id=&quot;main-result&quot;&gt;Main Result&lt;/h1&gt; &lt;h3 id=&quot;latent-representation-quality-정의&quot;&gt;Latent Representation Quality 정의&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Definition 1.&lt;/strong&gt;&lt;br /&gt; 데이터 분포에서 학습된 latent representation mapping \(g \in \mathcal{G}\)의 &lt;em&gt;quality&lt;/em&gt;는 다음과 같이 정의됩니다.&lt;br /&gt; \(\eta(g) = \text{inf}_{h\in\mathcal{H}}[r(h\circ g)-r(h^\star\circ g^\star)]\)&lt;br /&gt; 즉, true latent space와의 차이를 측정하며, 이를 latent space quality라 부릅니다.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2 id=&quot;rademacher-complexity&quot;&gt;Rademacher Complexity&lt;/h2&gt; &lt;p&gt;Model complexity를 측정하는 Rademacher complexity는 다음과 같습니다.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;\(\mathcal{F}\)를 \(\mathbb{R}^d \mapsto \mathbb{R}\)인 함수 집합으로 정의합니다.&lt;/li&gt; &lt;li&gt;\(Z_1, \ldots, Z_m\)은 \(\mathbb{R}^d\)에서 iid로 샘플된 데이터이고, \(S=(Z_1,\ldots,Z_m)\)라고 합니다.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Empirical Rademacher complexity는 다음과 같이 정의됩니다.&lt;br /&gt; \(\hat{\mathfrak{R}}_S(\mathcal{F}):=\mathbb{E}_\sigma[\underset{f\in\mathcal{F}}{\text{sup}} \frac{1}{m}\sum_{i=1}^m\sigma_if(Z_i)]\)&lt;/p&gt; &lt;p&gt;여기서 \(\sigma=(\sigma_1,...,\sigma_m)^\top\)이고, \(\sigma_i\)는 \(\{-1, 1\}\)에서 uniform하게 추출된 random variable입니다.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;latent-space-quality와-population-risk의-관계&quot;&gt;Latent Space Quality와 Population Risk의 관계&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Theorem 1.&lt;/strong&gt;&lt;br /&gt; \(S = \{(x_i, y_i)\}_{i=1}^m\)이 데이터셋이고, \(\mathcal{M}, \mathcal{N}\)은 modality의 두 subset입니다. \(\mathcal{M}\)과 \(\mathcal{N}\)으로 각각 학습된 empirical risk minimizers \((\hat{h}_\mathcal{M}, \hat{g}_\mathcal{M})\)와 \((\hat{h}_\mathcal{N}, \hat{g}_\mathcal{N})\)에 대해, 다음이 성립합니다.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;\(r(\hat{h}_{\mathcal{M}} \circ \hat{g}_{\mathcal{M}}) - r(\hat{h}_{\mathcal{N}} \circ \hat{g}_{\mathcal{N}}) \leq \gamma_{\mathcal{S}}(\mathcal{M},\mathcal{N}) + \text{O}(1/m)\)&lt;br /&gt; 여기서 \(\gamma_S(\mathcal{M},\mathcal{N})\triangleq\eta(\hat{g}_\mathcal{M})-\eta(\hat{g}_\mathcal{N})\)는 latent space quality의 차이입니다.&lt;/p&gt; &lt;hr /&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;h2 id=&quot;dataset-iemocap&quot;&gt;Dataset: IEMOCAP&lt;/h2&gt; &lt;p&gt;Interactive Emotional Dyadic Motion Capture 데이터셋을 사용했습니다. 데이터셋에는 Text, Video, Audio 정보가 포함되어 있으며, 발화자의 감정을 예측하는 것이 목표입니다.&lt;/p&gt; &lt;h3 id=&quot;결과&quot;&gt;결과&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Modalities가 많을수록 정확도가 향상됩니다.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sample이 적을 경우, subset modalities가 더 나은 성능을 보일 수 있습니다.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-modal 학습은 더 나은 latent space quality를 제공합니다.&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/multi-modal-vs-uni-modal/table3.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;결론&quot;&gt;결론&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;데이터 크기가 충분히 클 때 multi-modal을 사용하는 것이 유리합니다.&lt;/li&gt; &lt;li&gt;multi-modal 학습은 더 정확한 latent space representation을 학습할 수 있습니다.&lt;/li&gt; &lt;li&gt;이론적 분석과 실험 결과 모두 이를 뒷받침합니다.&lt;/li&gt; &lt;/ul&gt; </description> <pubDate>Sun, 19 Nov 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/multimodal-vs-unimodal/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/multimodal-vs-unimodal/</guid> <category>multi-modal</category> <category>paper</category> <category>multi-modal</category> <category>paper</category> </item> <item> <title>스타트업 리서치 인턴 후기</title> <description>&lt;h1 id=&quot;왜-시작했나요&quot;&gt;왜 시작했나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/img.jpeg&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; *&apos;Researcher일까? Engineer일까?&apos;* &lt;/p&gt; &lt;p&gt;나는 대학에 다니면서 항상 그런 고민을 했다. 고등학생 때 우연히 DeepMind에서 발표한 Playing Atari with Deep Reinforcement Learning이라는 논문을 보게되었고 인공지능에 빠져들었다. 대학 와서는 computer vision을 공부하게 되었다. 그저 인공지능이 좋아 backend, frontend 등 다른 분야보다는 인공지능 공부와 개발만 하게 되었다. 그러다 대학을 졸업할 때가 되었고, researcher와 engineer를 선택해야 할 순간이 다가왔다.&lt;/p&gt; &lt;p&gt;불행인지 다행인 건지 중앙대학교에서는 인턴을 해야지 졸업을 할 수 있었고, 관심 있는 두 군데 스타트업에 접촉하여 그중 한 회사인 뉴로클에서 인턴을 진행하게 되었다. 뉴로클을 선택한 이유는 간단했다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Computer Vision을 중점으로 한다.&lt;/li&gt; &lt;li&gt;자체 서비스를 판매하고 있다.&lt;/li&gt; &lt;li&gt;기업매출을 보니 매출도 성장세였고, 흑자를 내기 시작했다.&lt;/li&gt; &lt;li&gt;내가 내 일을 할 수 있고, 주체적으로 일할 수 있는 규모가 작지도 않고 크지도 않는 회사이다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;결론적으로 이러한 예측이 맞았고 성공적으로 인턴을 만들 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;무엇을-했나요&quot;&gt;무엇을 했나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/img.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;기본적으로 리서치 인턴의 역할을 수행했으나 후반에는 리서치 엔지니어의 역할을 하게 되었다. 퍼포먼스가 좋아서 그런지 생각보다 많은 일을 하게 되었다. (외부에 공개적으로 자료가 나간 것들만 포함했다)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Pretrained-OCR: OCR Auto-labeling 기능 추가 및 OCR 모델 성능 향상. 학습 속도 기존의 30%, 정확도 4%p 향상&lt;/li&gt; &lt;li&gt;Smart labeling (segmentation): 기술 검토 및 테스트, 모델 변환&lt;/li&gt; &lt;li&gt;Smart labeling (object detection): 기술 검토 및 모델 변환&lt;/li&gt; &lt;li&gt;회사 블로그 제작&lt;/li&gt; &lt;li&gt;리서치팀 docker 등 개발환경 관리&lt;/li&gt; &lt;li&gt;(방향성만 제시했지만) Neuro-I 성능개선, 다른 사람 연구 해결책 찾기 등등…&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;8개월동안 이걸 다 했다고?? 놀랍게도 그렇다. 개발 base로 computer vision을 공부했다 보니 구현 속도와 실험 속도가 압도적으로 빠른 것 같다.&lt;/p&gt; &lt;h1 id=&quot;무슨-경험이-도움이-되었나요&quot;&gt;무슨 경험이 도움이 되었나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/project.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;동아리에서 공모전에서 프로젝트 경험이 많았고 협업도 많이 진행했다. 그래서 computer vision, 선형대수, 수치해석, 위상수학, 표 본론 등 과 같은 지식뿐만 아니라 tensorrt, onnx, quantization 등 많은 기술, pandas, matplotlib, seaborn과 같은 데이터 시각화, 딥러닝 모델이 제품에 어떻게 탑재해야 하는지에 대한 감도 있었다. 이 모든 경험을 회사에서 다 썼다. (진짜 다 썼다) 이러한 다양한 경험은 여러 기능에 기여를 할 수 있었던 것 같다.&lt;/p&gt; &lt;h1 id=&quot;무엇을-얻었나요&quot;&gt;무엇을 얻었나요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/company.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;협업&quot;&gt;협업&lt;/h2&gt; &lt;p&gt;학생 때와 차원이 다른 협업을 하게 되었다. 학교에는 기껏해야 backed, fronted고 인원도 적어서 체계도 없이 작업을 해도 되었다. 하지만 인턴을 하면서 backed, fronted뿐만 아니라 영업, 마케팅, backbend, 기획 등 여러 사람과 협업을 진행했다.&lt;/p&gt; &lt;h3 id=&quot;요구사항을-명확하게-하자&quot;&gt;요구사항을 명확하게 하자&lt;/h3&gt; &lt;p&gt;협업은 기본적으로 background가 완전하게 동일하지 않은 사람들끼리 작업을 한다. 따라서 같은 목표를 바라보고있어도 세부 사항이 다를 수 있다. 만약 이를 조정하지 않고 일을 진행하다 보면 다음에 다시 조정하고 어려울뿐더러 비용 역시 많이 발생한다.&lt;/p&gt; &lt;h3 id=&quot;방향성-설정&quot;&gt;방향성 설정&lt;/h3&gt; &lt;p&gt;하나의 기능이 만들어지기 위해서 다음과 같은 과정을 거친다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;새로운 연구 주제로 연구한다.&lt;/li&gt; &lt;li&gt;기획 및 디자인팀에서 제품 탑재 방향을 결정한다.&lt;/li&gt; &lt;li&gt;개발팀에서 제품을 개발한다.&lt;/li&gt; &lt;li&gt;마케팅팀에서 협력사에 제공할 데이터와 대외 홍보용 자료를 제작한다.&lt;/li&gt; &lt;li&gt;영업을 통해 제품을 판매한다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;라이프 사이클에서 연구는 최상단을 차지한다. 따라서 첫 단추가 잘못 채워지면 전체적인 제품 방향이 엇나갈 수 있다.&lt;/p&gt; &lt;h3 id=&quot;문서화를-체계적으로-하자&quot;&gt;문서화를 체계적으로 하자&lt;/h3&gt; &lt;p&gt;내가 하는 연구를 follow up 하는 사람은 극소수다. 연구 이후 제품화할 때 조직되는 관련자들은 내가 작성해 놓은 document를 보고 작업을 시작한다. 그래서 진행한 연구와 모델을 제대로 이해시키려면 구조적으로 잘 문서화를 해야 한다.&lt;/p&gt; &lt;h2 id=&quot;지식&quot;&gt;지식&lt;/h2&gt; &lt;h2 id=&quot;tensorflow&quot;&gt;Tensorflow&lt;/h2&gt; &lt;p&gt;나는 지금까지 pytorch를 이용하여 작업을 했다. Tensorflow는 회사 들어와서 거의 처음 쓰게 된 것이다. Tensorflow는 기본적으로 eager mode가 제공되지 않아 코딩하는 것이 힘들었지만 tensorflow와 tensorflow orbit을 익히게 되는 좋은 기회가 되었다.&lt;/p&gt; &lt;h3 id=&quot;논문&quot;&gt;논문&lt;/h3&gt; &lt;p&gt;회사에 들어와서 논문을 진짜 많이 읽었다. 연구에 기반이 되는 논문뿐만 아니라 적용할 만한 최신논문, 기술 리포트, 워크숍 논문 등 다양하게 많이 읽었다. 이를 통해 ViT, active learning, OCR, anomaly detection, super resolution, backbone for edge device, federated learning 등 다양한 분야에 대해 기초지식을 쌓을 수 있었다.&lt;/p&gt; &lt;h3 id=&quot;데이터-분석&quot;&gt;데이터 분석&lt;/h3&gt; &lt;p&gt;기본적으로 데이터가 부족한 상황에서 모델의 성능을 올리는 방법을 고민을 했다. 이 때문에 사용자가 다룰 예상데이터의 특성을 분석하고 데이터에 적합한 방법론을 사용하여 성능을 높일 수 있었다.&lt;/p&gt; &lt;h2 id=&quot;일은-잘했나요&quot;&gt;일은 잘했나요?&lt;/h2&gt; &lt;p&gt;나에 대한 평가가 긍정적인 것을 보면 일을 잘했던 것 같다. 무엇보다도 나랑 같이 인턴을 진행한 분과 잘하는 것이 달라서 서로 시너지가 났던 것 같다. 원래 회사에도 리서치 인턴이 없었는데 인턴 둘이 좋은 선례를 만들어서 앞으로 계속 채용할 예정이다. (사실 면접이 완료되어 다음 리서치 인턴도 정해졌다.)&lt;/p&gt; &lt;h1 id=&quot;앞으로-무엇을-할-것인가요&quot;&gt;앞으로 무엇을 할 것인가요?&lt;/h1&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/starup-intern/todo.png&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; &lt;p&gt;감사하게도 회사에서 정규직 제안을 받았다. 스타트업 리서치 엔지니어에 가깝지만, 학사 출신으로 연구를 할 수 있다는 것이 흔치 않은 기회이다. 하지만 회사에서 원하는 인재와 내가 가고자 하는 방향이 달랐다. 회사에서 원하는 인재는 generalist이지만 나는 한 분에서 specialist가 되고 싶었다. 분야 또한 일반적인 모델링이 아닌 모델 경량화, hardware optimization, low cost serving 쪽으로 가고 싶다. 그리고 내가 다루어야 하는 target data가 무엇인지 명확하게 정할 수 있는 연구개발을 하고 싶다.&lt;/p&gt; &lt;p&gt;이제 학교에 다시 돌아간다. 8개월 동안 뉴로클 덕분에 좋은 경험을 했고 내 실력도 엄청나게 향상되었다. 이제 4학년 2학기이다. 졸업도 얼마 안 남아서 취업 준비나 대학원 준비를 해야겠지만 DL engineer 쪽 공부도 더욱 열심히 하면서 내가 목표하는 커리어를 만들어 가야겠다.&lt;/p&gt; </description> <pubDate>Tue, 22 Aug 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/startup-research-intern-review/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/startup-research-intern-review/</guid> <category>daily</category> <category>daily</category> </item> <item> <title>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</title> <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;GPT부터 시작해서 ViT 등 여러 분야에서 attention layer를 많이 쓰고 있다. 그런데 이 attention layer는 dimension의 제곱에 비례해서 계산 비용이 커서 모델의 병목이 될 수 있다. 그래서 attention layer를 효율적으로 만드는 여러 시도가 있는데, 그 중 하나가 FlashAttention이다. FlashAttention은 tiling과 kernel fusion을 사용해서 기존 attention layer보다 2.4배 더 빠르게 동작한다. 하지만 FlashAttention도 GPU의 이론적 성능에 비해 25~40%밖에 성능을 내지 못한다고 한다.&lt;/p&gt; &lt;p&gt;이런 문제를 해결하기 위해 저자는 FlashAttention을 분석하면서 thread block 간 work partitioning이 비효율적이라는 점을 발견했다. 이로 인해 GPU에서 low-occupancy와 불필요한 memory IO가 발생한다고 느꼈다. 그래서 저자는 이를 개선하기 위해 세 가지 방법을 제안했다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Output을 바꾸지 않고 non-matmul operation의 FLOPS를 줄인다.&lt;/li&gt; &lt;li&gt;Single head attention이라도 병렬 처리를 하도록 연산 순서를 바꾼다.&lt;/li&gt; &lt;li&gt;Thread block 내에서 warps 간 통신을 줄인다.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;저자는 이 세 가지 방법을 통해 기존 FlashAttention보다 2배 빠른 성능을 달성하고, GPU 이론적 성능의 50~73%까지 성능을 끌어올렸다.&lt;/p&gt; &lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt; &lt;p&gt;하드웨어 최적화 관련 논문은 익숙하지 않아서 배경 부분을 꼼꼼하게 읽어보자.&lt;/p&gt; &lt;h2 id=&quot;hardware-characteristics&quot;&gt;Hardware characteristics&lt;/h2&gt; &lt;h3 id=&quot;gpu-performance-characteristics&quot;&gt;GPU performance characteristics&lt;/h3&gt; &lt;p&gt;GPU는 compute element와 memory hierarchy를 가지고 있다. 예를 들어, Nvidia의 tensor core는 FP16/BF16 같은 저정밀도 연산을 matmul에 최적화하고 있다. 하지만 non-matmul 연산은 최적화가 부족해서 matmul보다 최대 16배 느리다.&lt;/p&gt; &lt;p&gt;메모리 계층 구조에 대해 보면, GPU는 기본적으로 high bandwidth memory(HBM)와 on-chip SRAM(공유 메모리)을 갖고 있다. A100 기준으로 40~80GB의 HBM은 1.5~2.0TB/s의 대역폭을 가지며, 108개의 stream multiprocessor는 각각 192KB의 on-chip SRAM을 갖고 있어 19TB/s의 대역폭을 제공한다. L2 캐시도 있지만, 이는 사용자가 컨트롤할 수 없어서 논의에서는 제외한다.&lt;/p&gt; &lt;h3 id=&quot;execution-model&quot;&gt;Execution Model&lt;/h3&gt; &lt;p&gt;GPU는 수많은 thread로 구성되며, 이 thread들이 모여 thread block을 구성한다. 각 thread block은 stream multiprocessor(SM)에서 실행된다. thread block 내에서 thread는 warp이라는 단위로 묶이며, 이 warp들은 공유 메모리를 통해 서로 통신한다.&lt;/p&gt; &lt;h2 id=&quot;standard-attention-implementation&quot;&gt;Standard Attention Implementation&lt;/h2&gt; &lt;p&gt;기존의 attention은 query, key, value들 간의 연산으로 구성된다. 시퀀스 길이를 N, head dimension을 d라고 하자. Input sequence \(Q, K, V \in \mathbb{R}^{N\times d}\)에 대해 attention output \(O \in \mathbb{R}^{N \times d}\)를 계산하는 방식은 아래와 같다.&lt;/p&gt; \[S=QK^{\intercal}\in \mathbb{R}^{N\times N}\] \[P=\text{softmax}(S)\in\mathbb{R}^{N\times N}\] \[O=PV\in \mathbb{R}^{N\times d}\] &lt;p&gt;여기서 softmax는 row-wise로 적용된다. Backward pass는 아래 과정을 거친다.&lt;/p&gt; \[dV=P^{\intercal}dO\in\mathbb{R}^{N\times d}\] \[dP=dOV^{\intercal}\in\mathbb{R}^{N\times N}\] \[dS=\text{dsoftmax}(dP)\in\mathbb{R}^{N\times N}\] \[dQ=dSK\in\mathbb{R}^{N\times d}\] \[dK=QdS^\intercal\in\mathbb{R}^{N\times d}\] &lt;p&gt;FlashAttention에 대해 더 자세한 내용은 다른 포스트에서 확인할 수 있다.&lt;/p&gt; &lt;h2 id=&quot;flashattention&quot;&gt;FlashAttention&lt;/h2&gt; &lt;p&gt;FlashAttention의 구체적인 내용은 이전에 다뤘던 &lt;a href=&quot;https://www.wonbeomjang.kr/blog/2023/fastattention/&quot;&gt;FlashAttention 1 포스트&lt;/a&gt;에서 참고할 수 있다.&lt;/p&gt; &lt;h3 id=&quot;forward-pass&quot;&gt;Forward pass&lt;/h3&gt; &lt;p&gt;FlashAttention은 K와 V를 tiling하여 병렬적으로 계산한 뒤, on-line softmax를 통해 병렬적으로 softmax를 적용한다. 그 후 tiling한 Q를 불러와 on-chip 연산을 한다. 이를 통해 연산을 fusion하고, Q, K, V는 HBM에서 불러와 연산을 마친 후 다시 HBM에 저장한다. 연산 과정은 아래와 같다. 여기서 \(S\)는 \(S=QK^T\)이다.&lt;/p&gt; \[m^{(1)}=\text{rowmax}(S^{(1)})\in\mathbb{R}^{B_r}\] \[l^{(1)}=\text{rowsum}(e^{S^{(1)}-m^{(1)}})\in\mathbb{R}^{B_r\times B_c}\] \[\tilde{P}^{(1)}=\text{diag}(l^{(1)})^{-1}e^{S^{(1)}-m^{(1)}}\in\mathbb{R}^{B_r\times B_c}\] \[O^{(1)}=\tilde{P}^{(1)}V^{(1)}=\text{diag}(l^{(1)})^{-1}e^{S^{(1)}-m^{(1)}}V^{(1)}\in\mathbb{R}^{B_r\times d}\] \[m^{(2)}=\text{max}(m^{(1)},\text{rowmax}(S^{(2)}))=m\] \[l^{(2)}=e^{m^{(1)}-m^{(2)}}l^{(1)}+\text{rowsum}(e^{S^{(2)}-m})=\text{rowsum}(e^{S^{(1)}-m})+\text{rowsum}(e^{S^{(2)}-m})=l\] \[\tilde{P}^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m^{(2)}}\] \[O^{(2)}=\text{diag}(l^{(1)}/l^{(2)})^{-1}O^{(1)}+\tilde{P}^{(2)}V^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(1)}-m}V^{(1)}+\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m}V^{(2)}=O\] &lt;p&gt;이 과정에서 vector를 쪼개고 합치는 방식으로 memory IO를 줄여서 속도를 높였다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig1.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h3 id=&quot;backward-pass&quot;&gt;Backward Pass&lt;/h3&gt; &lt;p&gt;Backward pass에서는 attention 연산 중에 계산된 \(m\)과 \(l\)을 사용해서 다시 연산을 재구성할 수 있다.&lt;/p&gt; &lt;h1 id=&quot;flashattention-2&quot;&gt;FlashAttention-2&lt;/h1&gt; &lt;p&gt;FlashAttention-2는 기존 FlashAttention보다 non-matmul FLOPs를 줄인다. 예를 들어, Nvidia의 A100 GPU는 FP16/BF16 matmul 연산에서 이론적으로 312 TFLOPs/s의 성능을 보이지만, non-matmul 연산은 19.5 TFLOPs/s로 훨씬 느리다. 그래서 non-matmul 연산이 전체 연산에서 차지하는 비중이 크면, 이를 최적화하는 것이 중요하다.&lt;/p&gt; &lt;h2 id=&quot;forward-pass-1&quot;&gt;Forward pass&lt;/h2&gt; &lt;p&gt;FlashAttention에서는 on-line softmax를 먼저 주목하고, 이를 개선할 수 있는 방법을 제시했다.&lt;/p&gt; &lt;h3 id=&quot;recalling&quot;&gt;Recalling&lt;/h3&gt; &lt;p&gt;기존에는 \(\text{diag}(l^{(2)})^{-1}\)를 두 번 rescaling했으나, FlashAttention-2에서는 마지막 결과 \(\tilde{O}^{(last)}\)를 계산하고, 한 번에 \(\text{diag}(l^{(last)})^{-1}\)으로 rescaling을 한다.&lt;/p&gt; \[\tilde{O}^{(2)}=\text{diag}(l^{(1)})^{-1}O^{(1)}+e^{S^{(2)}-m^{(2)}}V^{(2)}\] \[O^{(2)}=\tilde{O}^{(2)}\text{diag}(l^{(2)})^{-1}\] &lt;h3 id=&quot;memorization&quot;&gt;Memorization&lt;/h3&gt; &lt;p&gt;Backward pass를 위해 \(m\)과 \(l\)을 저장할 필요 없이 \(L^{(j)}=m^{(j)}+\text{log}(l^{(j)})\)을 저장해도 같은 결과를 얻을 수 있다. 그래서 \(m\)과 \(l\) 대신 \(L\)을 저장한다.&lt;/p&gt; &lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt; &lt;p&gt;결과적으로 FlashAttention-2는 다음과 같은 방법으로 attention을 구현한다.&lt;/p&gt; \[m^{(1)}=\text{rowmax}(S^{(1)})\in\mathbb{R}^{B_r}\] \[l^{(1)}=\text{rowsum}(e^{S^{(1)}-m^{(1)}})\in\mathbb{R}^{B_r\times B_c}\] \[\tilde{O}^{(1)}=e^{S^{(1)}-m^{(1)}}V^{(1)}\in\mathbb{R}^{B_r\times d}\] \[m^{(2)}=\text{max}(m^{(1)},\text{rowmax}(S^{(2)}))=m\] \[l^{(2)}=e^{m^{(1)}-m^{(2)}}l^{(1)}+\text{rowsum}(e^{S^{(2)}-m})=\text{rowsum}(e^{S^{(1)}-m})+\text{rowsum}(e^{S^{(2)}-m})=l\] \[\tilde{P}^{(2)}=\text{diag}(l^{(2)})^{-1}e^{S^{(2)}-m^{(2)}}\] \[\tilde{O}^{(2)}=\text{diag}(e^{m^{(1)}-m^{(2)}})^{-1}\tilde{O}^{(1)}+e^{S^{(2)}-m^{(2)}}V^{(2)}=e^{S^{(1)}-m}V^{(1)}+e^{S^{(2)}-m}V^{(2)}\] \[O^{(2)}=\text{diag}(l^{(2)})^{-1}\tilde{O}^{(2)}=O\] &lt;p&gt;FlashAttention-2에서는 기존과 달리 term 자체가 줄어들었다. Forward pass 알고리즘을 정리하면 다음과 같다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/alg1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;backward&quot;&gt;Backward&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/alg2.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;Backward는 \(L\)을 사용한다는 것 말고는 다른 차이점은 없다.&lt;/p&gt; &lt;h2 id=&quot;parallelism&quot;&gt;Parallelism&lt;/h2&gt; &lt;p&gt;기본적으로 GPU는 병렬 처리가 가능하다. 각 GPU thread block마다 1개의 attention module이 들어가고, 보통 batch size와 self-attention head 수에 맞춰 thread block을 구성한다. 만약 sequence 길이가 길어지거나, batch size가 작거나, self-attention head가 적으면 병렬 처리가 잘 이루어지지 않는다. 그래서 저자는 sequence length dimension에 따라 병렬 처리를 하도록 했다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Forward pass&lt;/strong&gt; 저자는 sequence length dimension으로 병렬처리를 하도록 했으며, 이는 한 sequence 내에서 독립적으로 처리되게 구성했다. 물론 기존처럼 batch와 multi-head 간 병렬처리는 계속 유지된다.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Backward pass&lt;/strong&gt; Algorithm 2에 따르면, backward pass는 column block 간 병렬처리가 이루어진다. 또한 sequence length dimension을 병렬 처리할 수 있도록 추가적인 방법을 사용한다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig2.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;p&gt;결과적으로 worker마다 병렬 처리가 잘 되도록 구성되었다.&lt;/p&gt; &lt;h2 id=&quot;work-partitioning-between-warp&quot;&gt;Work Partitioning Between Warp&lt;/h2&gt; &lt;h3 id=&quot;forward&quot;&gt;Forward&lt;/h3&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig3.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;기존 FlashAttention에서는 \(K\)와 \(V\)를 각각의 warp에 나눠서 partition하고, \(Q\)는 모든 warp이 접근할 수 있도록 했다. 이를 “split-K”라고 부른다. 하지만 이 방식은 \(QK^T\)와 \(V\)의 연산이 partition된 후 중간 계산 결과를 저장하고, 읽고, 동기화를 자주 해야 해서 IO에서 병목이 생긴다. 그래서 \(Q\)를 partition하고, \(K\)와 \(Q\)는 공유하게 해서 IO를 줄여 속도를 높였다.&lt;/p&gt; &lt;h2 id=&quot;backward-1&quot;&gt;Backward&lt;/h2&gt; &lt;p&gt;“split-K”를 지양한다는 것 밖에 이해를 못했다.&lt;/p&gt; &lt;h3 id=&quot;tuning-block-sizes&quot;&gt;Tuning block sizes&lt;/h3&gt; &lt;p&gt;Block size를 늘리면 memory IO가 줄어든다. 하지만 block 수가 많아지면 registers 수가 늘어나고, total shared memory 크기가 커져 비효율적이 될 수 있다. 너무 많은 registers는 속도를 저하시킬 수 있고, shared memory가 너무 커지면 GPU 메모리가 부족해질 수 있다. 그래서 GPU마다 적절한 block size를 조정해야 한다.&lt;/p&gt; &lt;h1 id=&quot;empirical-validation&quot;&gt;Empirical Validation&lt;/h1&gt; &lt;p&gt;결과적으로 FlashAttention-2는 기존 FlashAttention, xFormer보다 2배 이상의 성능 향상을 보였고, 특히 A100 GPU에서 2.7배까지 성능 향상이 일어났다. 전체적으로 FlashAttention-2는 GPU의 이론적인 성능에 가까운 성능을 보여주었다.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/flashattention2/fig4.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;FlashAttention-2는 기존 FlashAttention을 개선한 방법으로, 다양한 최적화를 통해 성능을 2배 이상 향상시켰다. GPU의 이론적 성능에 근접한 성능을 내면서, low-occupancy와 memory IO를 줄였다. 이 논문은 attention을 최적화하고 성능을 향상시키기 위한 여러 기술적 접근을 다뤘다.&lt;/p&gt; </description> <pubDate>Sun, 06 Aug 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/flashattention-2/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/flashattention-2/</guid> <category>attention</category> <category>hardware-optimization</category> <category>paper</category> <category>attention</category> <category>hardware-optimization</category> <category>paper</category> </item> <item> <title>Fairness-aware Data Valuation for Supervised Learning</title> <description>&lt;h1 id=&quot;들어가기-앞서&quot;&gt;들어가기 앞서&lt;/h1&gt; &lt;p&gt;Active learning과 class imbalance를 찾던 도중 발견한 논문이자. 그래서 FairML 분야는 아는 것이 없고, 이 논문이 좋은지 나쁜지도 판단이 안 된다. 하지만 해당 논문의 개념도 간단하고, 이런 것을 고려하면서 sampling을 하는 것도 좋겠다는 생각에 논문을 정리하고자 한다.&lt;/p&gt; &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;기존의 data valueation 연구는 데이터를 특정한 performace related value로 embedding한다. Active learning에서 “해당 train instance를 추가하면 모델의 성능이 올라가겠다”를 목적으로 entropy를 사용하여 embedding하는 방법도 있다. 하지만 추천시스템에서 해당 value만 사용할 경우 특정 그룹이나 인종에 안 좋은 데이터를 추천해주는 등 여러 안 좋은 점들이 있다. 따라서 fairness도 고려되어야 하는데 기존까지는 performance와 fairness를 동시에 고려하는 연구는 많지 않았다. 저자는 entropy를 기반으로 하여 두 가지 factor를 모두 고려하여 data를 utility value에 embedding하는 framework를 제안하였고 이를 이용하여 더 좋은 data sampling, re-weighting을 할 수 있었다.&lt;/p&gt; &lt;h1 id=&quot;fairness-aware-data-valuation&quot;&gt;Fairness-aware Data Valuation&lt;/h1&gt; &lt;h2 id=&quot;framework&quot;&gt;Framework&lt;/h2&gt; &lt;p&gt;일단 저자는 utility개념을 빌려왔는데, 해당 논문에서 utility는 performance와 fairness를 종합하는 function을 의미한다. Single data instance \(i\) 에 대해 perforamce-related valuation은 \(v_{y_i}\) , protected attribute에 대한 fairness-related valuation은 \(v_{z_i}\) 로 표시한다. Utility function은 다음과 같다.&lt;/p&gt; \[U_i(v_{y_i},v_{z_i})=\alpha(v_{y_i})+(1-\alpha)v_{z_i}\] &lt;p&gt;이 때 \(\alpha \in [0,1]\) 이다. 만약 fairness를 subgroup으로 나눈다면 다음의 식으로 확장할 수 있다.&lt;/p&gt; \[U_i(v_{y_i},v_{z_i})=\alpha(v_{y_i})+\sum_{j=1}^{k}\beta v_{z_{j_i}}\] &lt;h2 id=&quot;entorpy-metric&quot;&gt;Entorpy metric&lt;/h2&gt; &lt;p&gt;먼저 저자는 performace related value를 instance \(i\) 의 prediction \(y_{i}\) 의 entropy로 정의했다.&lt;/p&gt; \[V_{y_i}=E_{y_i}=\hat{y}_i\cdot {log}_2\hat{y}_i+(1-\hat{y}_i)\cdot {log}_2(1-\hat{y}_i)\] &lt;p&gt;해당 수식은 active learning에서 영감을 받았다. Entropy가 높다는 것은 모델이 해당 instance를 잘 예측하지 못한다는 이야기이고, 추후에 이를 집어넣으면 성능이 높아진다는 것을 예상할 수 있다. 하지만 실제 상황에서는 애매한 instance뿐만 아니라 noise 또한 entropy가 높아져서 성능이 더 낮아질 가능성도 있다. 하지만 여러 task에서 해당 방법은 성능이 준수하다는 것으로 나와서 저자는 entropy를 사용했다.&lt;/p&gt; &lt;p&gt;저자는 fairness-related valueation또한 entropy로 정의했다.&lt;/p&gt; \[V_{z_i}=E_{z_i}=\hat{z}_i\cdot {log}_2\hat{z}_i+(1-\hat{z}_i)\cdot {log}_2(1-\hat{z}_i)\] &lt;p&gt;이 수식이 왜 되는지는 이해가 잘 안되지만 저자에 말은 이러하다.&lt;/p&gt; &lt;blockquote&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;In the second case, where the variable in question (Z) is not the target for the task at hand, and is seen by the model even at inference time, we prioritize observations where the model had more difficulty in establishing a relationship among X, Y, and Z, leveraging the fact the model has no explicit incentive to draw such relationships. This is directly related to mitigating the base bias condition of the taxonomy of Pombal et al. (2022a) ( P[X, Y] 6= P[X, Y&lt;/td&gt; &lt;td&gt;Z] ), and so related to promoting fairness.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;ul&gt; &lt;li&gt;Fairness-aware data valuation for supervised learning, Pombal et al, 2023 -&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;p&gt;내가 이해한 대로 적자면 여러 예측 모델이 있는데 이들 모두의 예측값의 entropy가 높다면 bias가 없는 데이터이다. 하지만 특정 모델에서 entropy가 높아 잘 예측한다면 그 데이터는 특정 모델에 적합한 bias가 있는 데이터일 것이다. 따라서 fairness 또한 entropy로 표현할 수 있게 되는 것이다.&lt;/p&gt; &lt;p&gt;이렇게 구한 perforamce related valuation과 fairness related valuation을 종합하여 utility function을 제작하게 된다.&lt;/p&gt; \[U_i=\alpha E_{y_i}+(1-\alpha)E_{z_i}\] &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt; &lt;p&gt;Dataset으로 bank account-opening fraud dataset을 사용했다. 해당 데이터에서 fraud rate는 1%이다. 해당 데이터는 사기계좌를 찾는 것으로 True Positive Rate (TPR)이 높아야한다. 반면에 False Positive Rate (FPR)가 높으면 사용자의 계좌사용이 불편해지기 떄문에 FPR을 낮추는 것을 목표로 하고 있다.&lt;/p&gt; &lt;h1 id=&quot;model&quot;&gt;Model&lt;/h1&gt; &lt;p&gt;Model은 tublar data에서 SOTA를 찍고있는 LightGBM을 사용한다.&lt;/p&gt; &lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt; &lt;p&gt;Data sampling, re-weighting은 다음의 과정을 거친다.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;각각의 예측모델 Y, Z에 대해 tranining instance의 value를 계산한다.&lt;/li&gt; &lt;li&gt;각각의 train instance에 대해 utility value를 계산한다.&lt;/li&gt; &lt;li&gt;Utility value를 기반으로 Utility-aware prevalence sampling (UASP) 또는 Utility-aware reweighting (UAR) 수행한다. 이 때 UASP는 under sampling을 이야기하는 것이다.&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/assets/post/image/fado/fig1.png&quot; width=&quot;100%&quot; /&gt; &lt;/p&gt; &lt;p&gt;결과는 위와 같다. 해당 표를 보는 방법은 모르나 저자말로는 자신이 제안한 framework가 balance를 잘 잡는다고 주장한다.&lt;/p&gt; </description> <pubDate>Wed, 12 Jul 2023 15:00:00 +0000</pubDate> <link>https://www.wonbeomjang.kr/blog/2023/fado/</link> <guid isPermaLink="true">https://www.wonbeomjang.kr/blog/2023/fado/</guid> <category>data-sampling</category> <category>fair-ml</category> <category>data-sampling</category> <category>fair-ml</category> </item> </channel> </rss>